{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "# 02. Data Layer Deep Dive\n",
    "\n",
    "Iceberg 테이블의 **Data Layer**를 구성하는 Parquet 파일의 내부 구조를 직접 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000002",
   "metadata": {},
   "source": [
    "## Data Layer 개요\n",
    "\n",
    "Iceberg의 Data Layer는 실제 레코드가 저장된 **데이터 파일**들로 구성됩니다.\n",
    "\n",
    "- 기본 포맷: **Apache Parquet** (ORC, Avro도 지원)\n",
    "- Parquet는 **Columnar Format** (열 기반 저장)\n",
    "  - 분석 쿼리에서 필요한 컬럼만 읽어 I/O를 줄임\n",
    "  - 같은 컬럼의 데이터를 모아 저장하므로 **압축 효율**이 높음\n",
    "\n",
    "### Parquet 파일 내부 구조\n",
    "\n",
    "```\n",
    "┌──────────────────────────┐\n",
    "│       Row Group 1        │\n",
    "│  ┌────┬────┬────┬────┐  │\n",
    "│  │Col1│Col2│Col3│... │  │  ← Column Chunks\n",
    "│  └────┴────┴────┴────┘  │\n",
    "├──────────────────────────┤\n",
    "│       Row Group 2        │\n",
    "│  ┌────┬────┬────┬────┐  │\n",
    "│  │Col1│Col2│Col3│... │  │\n",
    "│  └────┴────┴────┴────┘  │\n",
    "├──────────────────────────┤\n",
    "│        Footer            │\n",
    "│  (Schema, Row Group      │\n",
    "│   metadata, statistics)  │\n",
    "└──────────────────────────┘\n",
    "```\n",
    "\n",
    "- **Row Group**: 행의 묶음 (기본 128MB)\n",
    "- **Column Chunk**: Row Group 내 단일 컬럼의 데이터\n",
    "- **Footer**: 스키마, 통계 등 메타데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000003",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session(\"DataLayerDeepDive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000006",
   "metadata": {},
   "source": [
    "## 테이블 생성 및 데이터 삽입\n",
    "\n",
    "3번의 배치로 나눠 삽입하여 **파티션당 여러 개의 Parquet 파일**이 생기는 상황을 만듭니다.\n",
    "같은 파티션(월)에 속하는 데이터라도 별도 append이면 별도 파일로 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 1/3 완료 (300건 삽입)\n",
      "배치 2/3 완료 (300건 삽입)\n",
      "배치 3/3 완료 (300건 삽입)\n",
      "\n",
      "총 레코드: 900건\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo.lab\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS demo.lab.data_orders\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE demo.lab.data_orders (\n",
    "        order_id     BIGINT,\n",
    "        customer_id  BIGINT,\n",
    "        product_name STRING,\n",
    "        order_date   DATE,\n",
    "        amount       DOUBLE,\n",
    "        status       STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "# 3번의 배치 적재 → 파티션당 최대 3개 parquet 파일\n",
    "for i in range(3):\n",
    "    orders = generate_orders(num_records=300, seed=i, id_offset=i*300+1)\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(\"demo.lab.data_orders\").append()\n",
    "    print(f\"배치 {i+1}/3 완료 (300건 삽입)\")\n",
    "\n",
    "count = spark.sql(\"SELECT COUNT(*) AS cnt FROM demo.lab.data_orders\").collect()[0][\"cnt\"]\n",
    "print(f\"\\n총 레코드: {count}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000008",
   "metadata": {},
   "source": [
    "## 디렉토리 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1000009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00001.parquet  (3.1 KB)\n",
      "│   │   ├── 00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00001.parquet  (3.1 KB)\n",
      "│   │   └── 00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00001.parquet  (3.2 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00002.parquet  (3.1 KB)\n",
      "│   │   ├── 00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00002.parquet  (3.1 KB)\n",
      "│   │   └── 00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00002.parquet  (3.1 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00003.parquet  (3.0 KB)\n",
      "│       ├── 00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00003.parquet  (3.1 KB)\n",
      "│       └── 00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00003.parquet  (3.1 KB)\n",
      "└── metadata/\n",
      "    ├── 92035252-c4e6-48bd-a941-4d8c2847ef9c-m0.avro  (7.3 KB)\n",
      "    ├── d3e31f53-1f8b-47cd-b78e-d96f2eedca01-m0.avro  (7.3 KB)\n",
      "    ├── f8121a8d-4e3c-4df2-bdbb-329cfe92f19a-m0.avro  (7.3 KB)\n",
      "    ├── snap-7456894987477154910-1-92035252-c4e6-48bd-a941-4d8c2847ef9c.avro  (4.3 KB)\n",
      "    ├── snap-7698805410324450549-1-d3e31f53-1f8b-47cd-b78e-d96f2eedca01.avro  (4.2 KB)\n",
      "    ├── snap-7713966624943110306-1-f8121a8d-4e3c-4df2-bdbb-329cfe92f19a.avro  (4.3 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v2.metadata.json  (2.5 KB)\n",
      "    ├── v3.metadata.json  (3.5 KB)\n",
      "    ├── v4.metadata.json  (4.5 KB)\n",
      "    └── version-hint.text  (1 B)\n"
     ]
    }
   ],
   "source": [
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/data_orders\"\n",
    "\n",
    "show_tree(TABLE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100000a",
   "metadata": {},
   "source": [
    "## Parquet 파일 찾기\n",
    "\n",
    "`glob`을 사용하여 data/ 디렉토리 내 모든 Parquet 파일을 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b100000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet 파일 수: 9개\n",
      "\n",
      "  data/order_date_month=2024-01/00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00001.parquet  (3.1 KB)\n",
      "  data/order_date_month=2024-01/00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00001.parquet  (3.1 KB)\n",
      "  data/order_date_month=2024-01/00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00001.parquet  (3.2 KB)\n",
      "  data/order_date_month=2024-02/00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00002.parquet  (3.1 KB)\n",
      "  data/order_date_month=2024-02/00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00002.parquet  (3.1 KB)\n",
      "  data/order_date_month=2024-02/00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00002.parquet  (3.1 KB)\n",
      "  data/order_date_month=2024-03/00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00003.parquet  (3.0 KB)\n",
      "  data/order_date_month=2024-03/00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00003.parquet  (3.1 KB)\n",
      "  data/order_date_month=2024-03/00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00003.parquet  (3.1 KB)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "parquet_files = sorted(glob.glob(f\"{TABLE_PATH}/data/**/*.parquet\", recursive=True))\n",
    "\n",
    "print(f\"Parquet 파일 수: {len(parquet_files)}개\\n\")\n",
    "for f in parquet_files:\n",
    "    rel = os.path.relpath(f, TABLE_PATH)\n",
    "    size_kb = os.path.getsize(f) / 1024\n",
    "    print(f\"  {rel}  ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100000c",
   "metadata": {},
   "source": [
    "## 파티션 디렉토리 구조 관찰\n",
    "\n",
    "Iceberg의 Hidden Partitioning에 의해 `order_date_month=YYYY-MM` 형태의 디렉토리가 생성됩니다.\n",
    "\n",
    "3번의 append로 **같은 파티션에 여러 parquet 파일**이 쌓인 것을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b100000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파티션 수: 3개\n",
      "\n",
      "  order_date_month=2024-01: 3개 파일, 9.4 KB\n",
      "  order_date_month=2024-02: 3개 파일, 9.3 KB\n",
      "  order_date_month=2024-03: 3개 파일, 9.2 KB\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(TABLE_PATH, \"data\")\n",
    "partitions = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "print(f\"파티션 수: {len(partitions)}개\\n\")\n",
    "for p in partitions:\n",
    "    p_path = os.path.join(data_dir, p)\n",
    "    files = [f for f in os.listdir(p_path) if f.endswith(\".parquet\")]\n",
    "    p_size = sum(os.path.getsize(os.path.join(p_path, f)) for f in files)\n",
    "    print(f\"  {p}: {len(files)}개 파일, {p_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100000e",
   "metadata": {},
   "source": [
    "## Parquet 파일 내부 분석 (pyarrow)\n",
    "\n",
    "`pyarrow.parquet`를 사용하여 Parquet 파일의 내부 구조를 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b100000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 파일: data/order_date_month=2024-01/00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00001.parquet\n",
      "============================================================\n",
      "\n",
      "[ 기본 정보 ]\n",
      "  Parquet 포맷 버전: 1.0\n",
      "  생성 라이브러리:   parquet-mr version 1.13.1 (build db4183109d5b734ec5930d870cdae161e408ddba)\n",
      "  전체 행 수:        99\n",
      "  Row Group 수:      1\n",
      "  컬럼 수:           6\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 첫 번째 Parquet 파일 분석\n",
    "sample_file = parquet_files[0]\n",
    "print(f\"분석 파일: {os.path.relpath(sample_file, TABLE_PATH)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pf = pq.ParquetFile(sample_file)\n",
    "meta = pf.metadata\n",
    "\n",
    "print(f\"\\n[ 기본 정보 ]\")\n",
    "print(f\"  Parquet 포맷 버전: {meta.format_version}\")\n",
    "print(f\"  생성 라이브러리:   {meta.created_by}\")\n",
    "print(f\"  전체 행 수:        {meta.num_rows}\")\n",
    "print(f\"  Row Group 수:      {meta.num_row_groups}\")\n",
    "print(f\"  컬럼 수:           {meta.num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Schema ]\n",
      "order_id: int64\n",
      "  -- field metadata --\n",
      "  PARQUET:field_id: '1'\n",
      "customer_id: int64\n",
      "  -- field metadata --\n",
      "  PARQUET:field_id: '2'\n",
      "product_name: string\n",
      "  -- field metadata --\n",
      "  PARQUET:field_id: '3'\n",
      "order_date: date32[day]\n",
      "  -- field metadata --\n",
      "  PARQUET:field_id: '4'\n",
      "amount: double\n",
      "  -- field metadata --\n",
      "  PARQUET:field_id: '5'\n",
      "status: string\n",
      "  -- field metadata --\n",
      "  PARQUET:field_id: '6'\n",
      "-- schema metadata --\n",
      "iceberg.schema: '{\"type\":\"struct\",\"schema-id\":0,\"fields\":[{\"id\":1,\"name\":' + 345\n"
     ]
    }
   ],
   "source": [
    "# 스키마 확인\n",
    "print(\"[ Schema ]\")\n",
    "print(pf.schema_arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Row Group 0 ]\n",
      "  행 수:       99\n",
      "  총 크기:     3.2 KB\n",
      "  컬럼 수:     6\n",
      "\n",
      "  Column Chunks:\n",
      "    [0] order_id             | type=INT64      | encodings=('BIT_PACKED', 'RLE', 'PLAIN') | compressed=174 B | uncompressed=825 B\n",
      "    [1] customer_id          | type=INT64      | encodings=('BIT_PACKED', 'RLE', 'PLAIN') | compressed=268 B | uncompressed=824 B\n",
      "    [2] product_name         | type=BYTE_ARRAY | encodings=('PLAIN_DICTIONARY', 'BIT_PACKED', 'RLE') | compressed=307 B | uncompressed=395 B\n",
      "    [3] order_date           | type=INT32      | encodings=('PLAIN_DICTIONARY', 'BIT_PACKED', 'RLE') | compressed=203 B | uncompressed=245 B\n",
      "    [4] amount               | type=DOUBLE     | encodings=('BIT_PACKED', 'RLE', 'PLAIN') | compressed=525 B | uncompressed=825 B\n",
      "    [5] status               | type=BYTE_ARRAY | encodings=('PLAIN_DICTIONARY', 'BIT_PACKED', 'RLE') | compressed=172 B | uncompressed=154 B\n"
     ]
    }
   ],
   "source": [
    "# Row Group 분석\n",
    "for rg_idx in range(meta.num_row_groups):\n",
    "    rg = meta.row_group(rg_idx)\n",
    "    print(f\"\\n[ Row Group {rg_idx} ]\")\n",
    "    print(f\"  행 수:       {rg.num_rows}\")\n",
    "    print(f\"  총 크기:     {rg.total_byte_size / 1024:.1f} KB\")\n",
    "    print(f\"  컬럼 수:     {rg.num_columns}\")\n",
    "    \n",
    "    print(f\"\\n  Column Chunks:\")\n",
    "    for col_idx in range(rg.num_columns):\n",
    "        col = rg.column(col_idx)\n",
    "        print(f\"    [{col_idx}] {col.path_in_schema:20s} | \"\n",
    "              f\"type={str(col.physical_type):10s} | \"\n",
    "              f\"encodings={col.encodings} | \"\n",
    "              f\"compressed={col.total_compressed_size} B | \"\n",
    "              f\"uncompressed={col.total_uncompressed_size} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1000012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Column Statistics (Row Group 0) ]\n",
      "\n",
      "  order_id:\n",
      "    num_values=99, null_count=0\n",
      "    min=301, max=593\n",
      "\n",
      "  customer_id:\n",
      "    num_values=99, null_count=0\n",
      "    min=128, max=997\n",
      "\n",
      "  product_name:\n",
      "    num_values=99, null_count=0\n",
      "    min=AirPods, max=iPhone SE\n",
      "\n",
      "  order_date:\n",
      "    num_values=99, null_count=0\n",
      "    min=2024-01-01, max=2024-01-31\n",
      "\n",
      "  amount:\n",
      "    num_values=99, null_count=0\n",
      "    min=80.72, max=6981.31\n",
      "\n",
      "  status:\n",
      "    num_values=99, null_count=0\n",
      "    min=cancelled, max=shipped\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column 통계 확인\n",
    "rg = meta.row_group(0)\n",
    "print(\"[ Column Statistics (Row Group 0) ]\\n\")\n",
    "for col_idx in range(rg.num_columns):\n",
    "    col = rg.column(col_idx)\n",
    "    if col.statistics:\n",
    "        stats = col.statistics\n",
    "        print(f\"  {col.path_in_schema}:\")\n",
    "        print(f\"    num_values={stats.num_values}, null_count={stats.null_count}\")\n",
    "        if stats.has_min_max:\n",
    "            print(f\"    min={stats.min}, max={stats.max}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000013",
   "metadata": {},
   "source": [
    "## 파일 크기 및 레코드 수 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 전체 Parquet 파일 통계 ]\n",
      "\n",
      "  data/order_date_month=2024-01/00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00001.parquet: 99행, 3.1 KB\n",
      "  data/order_date_month=2024-01/00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00001.parquet: 101행, 3.1 KB\n",
      "  data/order_date_month=2024-01/00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00001.parquet: 109행, 3.2 KB\n",
      "  data/order_date_month=2024-02/00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00002.parquet: 107행, 3.1 KB\n",
      "  data/order_date_month=2024-02/00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00002.parquet: 95행, 3.1 KB\n",
      "  data/order_date_month=2024-02/00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00002.parquet: 99행, 3.1 KB\n",
      "  data/order_date_month=2024-03/00000-11-edb76034-f263-40b1-bd24-4fb8080e2f7e-00003.parquet: 94행, 3.0 KB\n",
      "  data/order_date_month=2024-03/00000-17-3dd389cf-d3bd-46c0-ad35-f9f62ed6d28b-00003.parquet: 104행, 3.1 KB\n",
      "  data/order_date_month=2024-03/00000-5-052ded78-4510-4ec0-ba2d-b983d5ef126b-00003.parquet: 92행, 3.1 KB\n",
      "\n",
      "  합계: 900행, 9개 파일, 27.9 KB\n",
      "  파일당 평균: 100행, 3.1 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"[ 전체 Parquet 파일 통계 ]\\n\")\n",
    "\n",
    "total_rows = 0\n",
    "total_bytes = 0\n",
    "\n",
    "for f in parquet_files:\n",
    "    pf_tmp = pq.ParquetFile(f)\n",
    "    rows = pf_tmp.metadata.num_rows\n",
    "    size = os.path.getsize(f)\n",
    "    total_rows += rows\n",
    "    total_bytes += size\n",
    "    rel = os.path.relpath(f, TABLE_PATH)\n",
    "    print(f\"  {rel}: {rows}행, {size / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\n  합계: {total_rows}행, {len(parquet_files)}개 파일, {total_bytes / 1024:.1f} KB\")\n",
    "if len(parquet_files) > 0:\n",
    "    print(f\"  파일당 평균: {total_rows / len(parquet_files):.0f}행, {total_bytes / len(parquet_files) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000015",
   "metadata": {},
   "source": [
    "## 관찰 포인트\n",
    "\n",
    "### 1. Parquet = Columnar Format\n",
    "- 스키마를 보면 각 컬럼이 독립적으로 저장됩니다\n",
    "- `SELECT amount FROM ...` 같은 쿼리는 amount 컬럼만 읽으면 됩니다\n",
    "- 압축 효율도 같은 타입의 데이터를 모아 저장하므로 높습니다\n",
    "\n",
    "### 2. Column Statistics\n",
    "- 각 Row Group의 각 컬럼에 `min`, `max`, `null_count` 통계가 저장됩니다\n",
    "- 쿼리 엔진이 이 통계를 활용하여 불필요한 Row Group을 건너뛸 수 있습니다 (**Predicate Pushdown**)\n",
    "\n",
    "### 3. 같은 파티션, 다른 파일\n",
    "- 3번의 append로 **같은 월(파티션) 안에 3개의 parquet 파일**이 생겼습니다\n",
    "- 각 파일은 서로 다른 `order_id` 범위, 다른 `min`/`max` 통계를 가집니다\n",
    "- Iceberg는 이 통계를 manifest file에 기록하여 **파일 수준 프루닝**에 활용합니다\n",
    "\n",
    "### 4. Small File Problem\n",
    "- 파티션당 3개의 작은 파일이 생겼는데, append를 100번 하면 **파티션당 100개**가 됩니다\n",
    "- 작은 파일이 많으면 메타데이터 오버헤드, 파일 열기/닫기 비용이 증가\n",
    "- **이 문제는 `4_optimization`에서 Compaction을 통해 해결합니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1000016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31eaf0-f385-4879-b77d-339198f3ff85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
