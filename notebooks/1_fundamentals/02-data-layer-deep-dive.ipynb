{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "# 02. Data Layer Deep Dive\n",
    "\n",
    "Iceberg 테이블의 **Data Layer**를 구성하는 Parquet 파일의 내부 구조를 직접 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000002",
   "metadata": {},
   "source": [
    "## Data Layer 개요\n",
    "\n",
    "Iceberg의 Data Layer는 실제 레코드가 저장된 **데이터 파일**들로 구성됩니다.\n",
    "\n",
    "- 기본 포맷: **Apache Parquet** (ORC, Avro도 지원)\n",
    "- Parquet는 **Columnar Format** (열 기반 저장)\n",
    "  - 분석 쿼리에서 필요한 컬럼만 읽어 I/O를 줄임\n",
    "  - 같은 컬럼의 데이터를 모아 저장하므로 **압축 효율**이 높음\n",
    "\n",
    "### Parquet 파일 내부 구조\n",
    "\n",
    "```\n",
    "┌──────────────────────────┐\n",
    "│       Row Group 1        │\n",
    "│  ┌────┬────┬────┬────┐  │\n",
    "│  │Col1│Col2│Col3│... │  │  ← Column Chunks\n",
    "│  └────┴────┴────┴────┘  │\n",
    "├──────────────────────────┤\n",
    "│       Row Group 2        │\n",
    "│  ┌────┬────┬────┬────┐  │\n",
    "│  │Col1│Col2│Col3│... │  │\n",
    "│  └────┴────┴────┴────┘  │\n",
    "├──────────────────────────┤\n",
    "│        Footer            │\n",
    "│  (Schema, Row Group      │\n",
    "│   metadata, statistics)  │\n",
    "└──────────────────────────┘\n",
    "```\n",
    "\n",
    "- **Row Group**: 행의 묶음 (기본 128MB)\n",
    "- **Column Chunk**: Row Group 내 단일 컬럼의 데이터\n",
    "- **Footer**: 스키마, 통계 등 메타데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000003",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session(\"DataLayerDeepDive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000006",
   "metadata": {},
   "source": [
    "## 테이블 생성 및 데이터 삽입\n",
    "\n",
    "200건의 데이터를 삽입하여 Data Layer를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo.lab\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS demo.lab.data_orders\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE demo.lab.data_orders (\n",
    "        order_id     BIGINT,\n",
    "        customer_id  BIGINT,\n",
    "        product_name STRING,\n",
    "        order_date   DATE,\n",
    "        amount       DOUBLE,\n",
    "        status       STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "orders = generate_orders(num_records=200, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(\"demo.lab.data_orders\").append()\n",
    "\n",
    "count = spark.sql(\"SELECT COUNT(*) AS cnt FROM demo.lab.data_orders\").collect()[0][\"cnt\"]\n",
    "print(f\"삽입 완료: {count}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000008",
   "metadata": {},
   "source": [
    "## 디렉토리 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/data_orders\"\n",
    "\n",
    "show_tree(TABLE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100000a",
   "metadata": {},
   "source": [
    "## Parquet 파일 찾기\n",
    "\n",
    "`glob`을 사용하여 data/ 디렉토리 내 모든 Parquet 파일을 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "parquet_files = sorted(glob.glob(f\"{TABLE_PATH}/data/**/*.parquet\", recursive=True))\n",
    "\n",
    "print(f\"Parquet 파일 수: {len(parquet_files)}개\\n\")\n",
    "for f in parquet_files:\n",
    "    rel = os.path.relpath(f, TABLE_PATH)\n",
    "    size_kb = os.path.getsize(f) / 1024\n",
    "    print(f\"  {rel}  ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100000c",
   "metadata": {},
   "source": [
    "## 파티션 디렉토리 구조 관찰\n",
    "\n",
    "Iceberg의 Hidden Partitioning에 의해 `order_date_month=YYYY-MM` 형태의 디렉토리가 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(TABLE_PATH, \"data\")\n",
    "partitions = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "print(f\"파티션 수: {len(partitions)}개\\n\")\n",
    "for p in partitions:\n",
    "    p_path = os.path.join(data_dir, p)\n",
    "    files = [f for f in os.listdir(p_path) if f.endswith(\".parquet\")]\n",
    "    p_size = sum(os.path.getsize(os.path.join(p_path, f)) for f in files)\n",
    "    print(f\"  {p}: {len(files)}개 파일, {p_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100000e",
   "metadata": {},
   "source": [
    "## Parquet 파일 내부 분석 (pyarrow)\n",
    "\n",
    "`pyarrow.parquet`를 사용하여 Parquet 파일의 내부 구조를 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 첫 번째 Parquet 파일 분석\n",
    "sample_file = parquet_files[0]\n",
    "print(f\"분석 파일: {os.path.relpath(sample_file, TABLE_PATH)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pf = pq.ParquetFile(sample_file)\n",
    "meta = pf.metadata\n",
    "\n",
    "print(f\"\\n[ 기본 정보 ]\")\n",
    "print(f\"  Parquet 포맷 버전: {meta.format_version}\")\n",
    "print(f\"  생성 라이브러리:   {meta.created_by}\")\n",
    "print(f\"  전체 행 수:        {meta.num_rows}\")\n",
    "print(f\"  Row Group 수:      {meta.num_row_groups}\")\n",
    "print(f\"  컬럼 수:           {meta.num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스키마 확인\n",
    "print(\"[ Schema ]\")\n",
    "print(pf.schema_arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row Group 분석\n",
    "for rg_idx in range(meta.num_row_groups):\n",
    "    rg = meta.row_group(rg_idx)\n",
    "    print(f\"\\n[ Row Group {rg_idx} ]\")\n",
    "    print(f\"  행 수:       {rg.num_rows}\")\n",
    "    print(f\"  총 크기:     {rg.total_byte_size / 1024:.1f} KB\")\n",
    "    print(f\"  컬럼 수:     {rg.num_columns}\")\n",
    "    \n",
    "    print(f\"\\n  Column Chunks:\")\n",
    "    for col_idx in range(rg.num_columns):\n",
    "        col = rg.column(col_idx)\n",
    "        print(f\"    [{col_idx}] {col.path_in_schema:20s} | \"\n",
    "              f\"type={str(col.physical_type):10s} | \"\n",
    "              f\"encodings={col.encodings} | \"\n",
    "              f\"compressed={col.total_compressed_size} B | \"\n",
    "              f\"uncompressed={col.total_uncompressed_size} B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 통계 확인\n",
    "rg = meta.row_group(0)\n",
    "print(\"[ Column Statistics (Row Group 0) ]\\n\")\n",
    "for col_idx in range(rg.num_columns):\n",
    "    col = rg.column(col_idx)\n",
    "    if col.statistics:\n",
    "        stats = col.statistics\n",
    "        print(f\"  {col.path_in_schema}:\")\n",
    "        print(f\"    num_values={stats.num_values}, null_count={stats.null_count}\")\n",
    "        if stats.has_min_max:\n",
    "            print(f\"    min={stats.min}, max={stats.max}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000013",
   "metadata": {},
   "source": [
    "## 파일 크기 및 레코드 수 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[ 전체 Parquet 파일 통계 ]\\n\")\n",
    "\n",
    "total_rows = 0\n",
    "total_bytes = 0\n",
    "\n",
    "for f in parquet_files:\n",
    "    pf_tmp = pq.ParquetFile(f)\n",
    "    rows = pf_tmp.metadata.num_rows\n",
    "    size = os.path.getsize(f)\n",
    "    total_rows += rows\n",
    "    total_bytes += size\n",
    "    rel = os.path.relpath(f, TABLE_PATH)\n",
    "    print(f\"  {rel}: {rows}행, {size / 1024:.1f} KB\")\n",
    "\n",
    "print(f\"\\n  합계: {total_rows}행, {len(parquet_files)}개 파일, {total_bytes / 1024:.1f} KB\")\n",
    "if len(parquet_files) > 0:\n",
    "    print(f\"  파일당 평균: {total_rows / len(parquet_files):.0f}행, {total_bytes / len(parquet_files) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000015",
   "metadata": {},
   "source": [
    "## 관찰 포인트\n",
    "\n",
    "### 1. Parquet = Columnar Format\n",
    "- 스키마를 보면 각 컬럼이 독립적으로 저장됩니다\n",
    "- `SELECT amount FROM ...` 같은 쿼리는 amount 컬럼만 읽으면 됩니다\n",
    "- 압축 효율도 같은 타입의 데이터를 모아 저장하므로 높습니다\n",
    "\n",
    "### 2. Column Statistics\n",
    "- 각 Row Group의 각 컬럼에 `min`, `max`, `null_count` 통계가 저장됩니다\n",
    "- 쿼리 엔진이 이 통계를 활용하여 불필요한 Row Group을 건너뛸 수 있습니다 (**Predicate Pushdown**)\n",
    "\n",
    "### 3. Small File Problem ⚠️\n",
    "- **파일 하나당 레코드가 매우 적습니다!**\n",
    "- 파티션별로 1개의 작은 파일이 생성되었는데, 이것이 누적되면 수천~수만 개의 작은 파일이 됩니다\n",
    "- 작은 파일이 많으면:\n",
    "  - 메타데이터 오버헤드가 증가\n",
    "  - 파일 열기/닫기 비용이 증가\n",
    "  - 쿼리 계획 시간이 길어짐\n",
    "- **이 문제는 `4_optimization`에서 Compaction을 통해 해결합니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000016",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}