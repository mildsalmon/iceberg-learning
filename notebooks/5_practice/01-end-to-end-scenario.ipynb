{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1000001",
   "metadata": {},
   "source": [
    "# End-to-End 시나리오 — 일별 배치부터 장애 복구까지\n",
    "\n",
    "이 노트북에서는 지금까지 배운 모든 내용을 **실전 시나리오**로 종합 실습합니다.\n",
    "\n",
    "### 시나리오\n",
    "\n",
    "온라인 쇼핑몰의 주문 데이터 파이프라인:\n",
    "\n",
    "1. **일별 배치 적재** — 매일 새로운 주문 데이터 INSERT\n",
    "2. **상태 업데이트** — 배송 완료된 주문 UPDATE\n",
    "3. **스키마 변경** — 새 컬럼 추가 (Schema Evolution)\n",
    "4. **컴팩션** — Small File Problem 해결\n",
    "5. **스냅샷 정리** — 유지보수 작업\n",
    "6. **장애 복구** — 실수로 데이터 삭제 후 Time Travel로 복구"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000002",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import time\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import (\n",
    "    show_tree,\n",
    "    snapshot_tree,\n",
    "    diff_tree,\n",
    "    count_files,\n",
    "    total_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "TABLE_NAME = \"demo.lab.e2e_orders\"\n",
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/e2e_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000005",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: 테이블 생성 + 일별 배치 적재\n",
    "\n",
    "월별 Hidden Partitioning으로 테이블을 생성하고, 5일치 배치 데이터를 순차적으로 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테이블 생성 완료: demo.lab.e2e_orders\n",
      "파티셔닝: months(order_date)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(f\"테이블 생성 완료: {TABLE_NAME}\")\n",
    "print(\"파티셔닝: months(order_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1: 100건 적재 (2024-01-01 ~ 2024-01-31)\n",
      "Day 2: 80건 적재 (2024-01-01 ~ 2024-01-31)\n",
      "Day 3: 120건 적재 (2024-02-01 ~ 2024-02-28)\n",
      "Day 4: 90건 적재 (2024-02-01 ~ 2024-02-28)\n",
      "Day 5: 110건 적재 (2024-03-01 ~ 2024-03-31)\n",
      "\n",
      "총 레코드: 500건\n",
      "파일 수: 5\n"
     ]
    }
   ],
   "source": [
    "# Day 1~5: 일별 배치 적재 시뮬레이션\n",
    "daily_batches = [\n",
    "    {\"day\": 1, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 100},\n",
    "    {\"day\": 2, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 80},\n",
    "    {\"day\": 3, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 120},\n",
    "    {\"day\": 4, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 90},\n",
    "    {\"day\": 5, \"start\": \"2024-03-01\", \"end\": \"2024-03-31\", \"records\": 110},\n",
    "]\n",
    "\n",
    "offset = 1\n",
    "for batch in daily_batches:\n",
    "    orders = generate_orders(\n",
    "        num_records=batch[\"records\"],\n",
    "        seed=batch[\"day\"],\n",
    "        start_date=batch[\"start\"],\n",
    "        end_date=batch[\"end\"],\n",
    "        id_offset=offset,\n",
    "    )\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(TABLE_NAME).append()\n",
    "    offset += batch[\"records\"]\n",
    "    print(\n",
    "        f\"Day {batch['day']}: {batch['records']}건 적재 ({batch['start']} ~ {batch['end']})\"\n",
    "    )\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n총 레코드: {total}건\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 완료 — 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-69-6b680a7c-560e-4038-8c12-bdb7ae56aed9-00001.parquet  (3.0 KB)\n",
      "│   │   └── 00000-75-4557e7d1-e2e2-4063-b7a9-bdc43bc7a0d7-00001.parquet  (2.8 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-81-28b87232-ee0b-49b7-9eb7-32eddef66fe2-00001.parquet  (3.1 KB)\n",
      "│   │   └── 00000-87-d0f8bf21-cded-49f6-8166-8d245926ea36-00001.parquet  (2.9 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       └── 00000-93-ece02ff2-1055-4c78-98ca-edba739262b1-00001.parquet  (3.0 KB)\n",
      "└── metadata/\n",
      "    ├── 2c685101-e974-4ec3-afef-11d108ac0dca-m0.avro  (7.1 KB)\n",
      "    ├── 9178965a-fd48-4b92-8038-16e764ea9871-m0.avro  (7.1 KB)\n",
      "    ├── 9f5ba179-81f1-4333-bec4-bdb29c164c8d-m0.avro  (7.1 KB)\n",
      "    ├── ab18da90-7ea9-46ea-b4e1-c98cd1b94214-m0.avro  (7.1 KB)\n",
      "    ├── c8f0742e-15a5-450f-b7cf-c742db199572-m0.avro  (7.1 KB)\n",
      "    ├── snap-3246290473534841970-1-9178965a-fd48-4b92-8038-16e764ea9871.avro  (4.4 KB)\n",
      "    ├── snap-6067149720729449558-1-9f5ba179-81f1-4333-bec4-bdb29c164c8d.avro  (4.3 KB)\n",
      "    ├── snap-70413296967110654-1-c8f0742e-15a5-450f-b7cf-c742db199572.avro  (4.3 KB)\n",
      "    ├── snap-7899258830142532014-1-ab18da90-7ea9-46ea-b4e1-c98cd1b94214.avro  (4.4 KB)\n",
      "    ├── snap-8523543611858364875-1-2c685101-e974-4ec3-afef-11d108ac0dca.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v2.metadata.json  (2.5 KB)\n",
      "    ├── v3.metadata.json  (3.5 KB)\n",
      "    ├── v4.metadata.json  (4.5 KB)\n",
      "    ├── v5.metadata.json  (5.5 KB)\n",
      "    ├── v6.metadata.json  (6.4 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "스냅샷 히스토리:\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "|snapshot_id        |committed_at           |operation|added_files|added_records|\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "|8523543611858364875|2026-02-16 05:51:26.146|append   |1          |100          |\n",
      "|6067149720729449558|2026-02-16 05:51:26.566|append   |1          |80           |\n",
      "|70413296967110654  |2026-02-16 05:51:26.877|append   |1          |120          |\n",
      "|7899258830142532014|2026-02-16 05:51:27.231|append   |1          |90           |\n",
      "|3246290473534841970|2026-02-16 05:51:27.801|append   |1          |110          |\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태 확인\n",
    "print(\"Phase 1 완료 — 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH, max_depth=3)\n",
    "\n",
    "print(\"\\n스냅샷 히스토리:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT snapshot_id, committed_at, operation,\n",
    "       summary['added-data-files'] as added_files,\n",
    "       summary['added-records'] as added_records\n",
    "FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f48bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1.5: 중복 데이터 유입과 정리\n",
    "\n",
    "운영에서는 동일 주문이 재전송되어 중복 키가 들어올 수 있습니다.\n",
    "업데이트 전에 중복을 탐지/정리하여 이후 단계의 정합성을 보장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf500c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 주입 후 중복 order_id 개수: 3\n",
      "중복 정리 전 총 레코드 수: 503\n",
      "중복 제거 대상 레코드 수: 3\n",
      "중복 정리 후 예상 레코드 수: 500\n",
      "중복 정리 후 중복 order_id 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 3건을 의도적으로 중복 유입\n",
    "dup_rows = spark.sql(f\"\"\"\n",
    "SELECT order_id, customer_id, product_name, order_date, amount, status\n",
    "FROM {TABLE_NAME}\n",
    "ORDER BY order_id\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "dup_rows.writeTo(TABLE_NAME).append()\n",
    "\n",
    "dup_keys = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    ") t\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"중복 주입 후 중복 order_id 개수: {dup_keys}\")\n",
    "\n",
    "# order_id 기준으로 1건만 남기도록 정리\n",
    "# cache()에 의존하지 않고, 임시 Iceberg 테이블에 먼저 물질화해 lineage를 완전히 분리\n",
    "TMP_DEDUP_TABLE = f\"{TABLE_NAME}_dedup_tmp\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TMP_DEDUP_TABLE}\")\n",
    "total_before_dedup = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME}\"\n",
    ").collect()[0][0]\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TMP_DEDUP_TABLE}\n",
    "USING ICEBERG\n",
    "AS\n",
    "SELECT order_id, customer_id, product_name, order_date, amount, status\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (\n",
    "               PARTITION BY order_id\n",
    "               ORDER BY order_date DESC, status DESC\n",
    "           ) AS rn\n",
    "    FROM {TABLE_NAME}\n",
    ") ranked\n",
    "WHERE rn = 1\n",
    "\"\"\")\n",
    "\n",
    "dedup_result_count = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TMP_DEDUP_TABLE}\"\n",
    ").collect()[0][0]\n",
    "dedup_removed_rows = total_before_dedup - dedup_result_count\n",
    "print(f\"중복 정리 전 총 레코드 수: {total_before_dedup}\")\n",
    "print(f\"중복 제거 대상 레코드 수: {dedup_removed_rows}\")\n",
    "print(f\"중복 정리 후 예상 레코드 수: {dedup_result_count}\")\n",
    "\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE true\")\n",
    "spark.sql(f\"INSERT INTO {TABLE_NAME} SELECT * FROM {TMP_DEDUP_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TMP_DEDUP_TABLE}\")\n",
    "\n",
    "dup_keys_after = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    ") t\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"중복 정리 후 중복 order_id 개수: {dup_keys_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e1311",
   "metadata": {},
   "source": [
    "\n",
    "### 관찰 포인트 — 중복 데이터 정리\n",
    "\n",
    "- 중복 유입을 조기에 정리하면 UPDATE/DELETE/집계 결과 왜곡을 예방할 수 있습니다.\n",
    "- 운영 파이프라인에는 배치 적재 직후 중복 탐지 쿼리를 자동화하는 것이 안전합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000009",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: 상태 업데이트 (COW)\n",
    "\n",
    "배송이 완료된 주문의 상태를 `shipped`로, 일부 주문을 `cancelled`로 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shipped 업데이트 대상: 32건\n",
      "shipped 상태 업데이트 완료 (현재 shipped: 129건)\n",
      "cancelled 업데이트 대상: 11건\n",
      "cancelled 상태 업데이트 완료 (현재 cancelled: 110건)\n",
      "\n",
      "파일 변경 사항 (COW 방식):\n",
      "\n",
      "[+] 추가된 파일 (10개):\n",
      "    + data/order_date_month=2024-01/00000-120-5afec7b2-bc83-41bf-a590-06dd94582164-00001.parquet  (3.6 KB)\n",
      "    + data/order_date_month=2024-03/00000-125-b3c638fc-a05f-42b0-941d-dc8f6f9b03e6-00001.parquet  (3.0 KB)\n",
      "    + metadata/7bc98a4b-d7b9-4646-9b0b-5970c1deca2e-m0.avro  (7.3 KB)\n",
      "    + metadata/7bc98a4b-d7b9-4646-9b0b-5970c1deca2e-m1.avro  (7.1 KB)\n",
      "    + metadata/af325f71-a8d3-453f-bcf0-5ce994623c9c-m0.avro  (7.2 KB)\n",
      "    + metadata/af325f71-a8d3-453f-bcf0-5ce994623c9c-m1.avro  (7.1 KB)\n",
      "    + metadata/snap-6696529076417919547-1-7bc98a4b-d7b9-4646-9b0b-5970c1deca2e.avro  (4.2 KB)\n",
      "    + metadata/snap-95883035658351778-1-af325f71-a8d3-453f-bcf0-5ce994623c9c.avro  (4.3 KB)\n",
      "    + metadata/v10.metadata.json  (10.5 KB)\n",
      "    + metadata/v11.metadata.json  (11.5 KB)\n",
      "\n",
      "[~] 크기 변경된 파일 (1개):\n",
      "    ~ metadata/version-hint.text  (1 B -> 2 B)\n",
      "\n",
      "요약: +10 추가, -0 삭제, ~1 변경\n"
     ]
    }
   ],
   "source": [
    "total_before_phase2 = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME}\"\n",
    ").collect()[0][0]\n",
    "if total_before_phase2 == 0:\n",
    "    raise RuntimeError(\n",
    "        \"테이블이 비어 있습니다. Phase 1의 일별 배치 적재 셀(Cell 6)을 먼저 실행하세요.\"\n",
    "    )\n",
    "\n",
    "before = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# pending → shipped\n",
    "pending_target = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM {TABLE_NAME}\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"shipped 업데이트 대상: {pending_target}건\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'shipped'\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\")\n",
    "shipped = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'shipped'\"\n",
    ").collect()[0][0]\n",
    "print(f\"shipped 상태 업데이트 완료 (현재 shipped: {shipped}건)\")\n",
    "\n",
    "# 일부 cancelled\n",
    "cancel_target = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM {TABLE_NAME}\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"cancelled 업데이트 대상: {cancel_target}건\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'cancelled'\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\")\n",
    "cancelled = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'cancelled'\"\n",
    ").collect()[0][0]\n",
    "print(f\"cancelled 상태 업데이트 완료 (현재 cancelled: {cancelled}건)\")\n",
    "\n",
    "after = snapshot_tree(TABLE_PATH)\n",
    "print(\"\\n파일 변경 사항 (COW 방식):\")\n",
    "diff_tree(before, after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주문 상태 분포:\n",
      "+----------+-----+\n",
      "|    status|count|\n",
      "+----------+-----+\n",
      "|   shipped|  127|\n",
      "|processing|  122|\n",
      "| cancelled|  110|\n",
      "| completed|   90|\n",
      "|   pending|   51|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 상태별 분포 확인\n",
    "total_for_distribution = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME}\"\n",
    ").collect()[0][0]\n",
    "\n",
    "print(\"주문 상태 분포:\")\n",
    "if total_for_distribution == 0:\n",
    "    print(\"현재 테이블에 데이터가 없습니다. Phase 1(셀 6)과 Phase 2(셀 9)를 순서대로 실행하세요.\")\n",
    "else:\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT status, COUNT(*) as count\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY status\n",
    "    ORDER BY count DESC\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000012",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: 스키마 변경 (Schema Evolution)\n",
    "\n",
    "비즈니스 요구사항 변경: `region` 컬럼을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1000013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region 컬럼 추가 완료\n",
      "+--------------+------------------+-------+\n",
      "|      col_name|         data_type|comment|\n",
      "+--------------+------------------+-------+\n",
      "|      order_id|            bigint|   null|\n",
      "|   customer_id|            bigint|   null|\n",
      "|  product_name|            string|   null|\n",
      "|    order_date|              date|   null|\n",
      "|        amount|     decimal(10,2)|   null|\n",
      "|        status|            string|   null|\n",
      "|        region|            string|   null|\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|        Part 0|months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# region 컬럼 추가\n",
    "spark.sql(f\"ALTER TABLE {TABLE_NAME} ADD COLUMN region STRING\")\n",
    "print(\"region 컬럼 추가 완료\")\n",
    "\n",
    "# 스키마 확인\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d1000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 데이터 region 업데이트 완료\n",
      "+------+-----+\n",
      "|region|count|\n",
      "+------+-----+\n",
      "| Seoul|  146|\n",
      "|  Jeju|  193|\n",
      "| Busan|  161|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 기존 데이터에 region 값 업데이트\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET region = CASE\n",
    "    WHEN customer_id % 3 = 0 THEN 'Seoul'\n",
    "    WHEN customer_id % 3 = 1 THEN 'Busan'\n",
    "    ELSE 'Jeju'\n",
    "END\n",
    "\"\"\")\n",
    "\n",
    "print(\"기존 데이터 region 업데이트 완료\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새 데이터 100건 추가 (region 포함). 총 레코드: 600건\n"
     ]
    }
   ],
   "source": [
    "# 새 데이터는 region 포함하여 삽입\n",
    "from pyspark.sql.functions import lit, when, col\n",
    "\n",
    "orders_new = generate_orders(\n",
    "    num_records=100,\n",
    "    seed=99,\n",
    "    start_date=\"2024-03-15\",\n",
    "    end_date=\"2024-03-31\",\n",
    "    id_offset=501,\n",
    ")\n",
    "df_new = to_spark_df(spark, orders_new)\n",
    "df_new = df_new.withColumn(\n",
    "    \"region\",\n",
    "    when(col(\"customer_id\") % 3 == 0, lit(\"Seoul\"))\n",
    "    .when(col(\"customer_id\") % 3 == 1, lit(\"Busan\"))\n",
    "    .otherwise(lit(\"Jeju\")),\n",
    ")\n",
    "df_new.writeTo(TABLE_NAME).append()\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"새 데이터 100건 추가 (region 포함). 총 레코드: {total}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000016",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: 컴팩션\n",
    "\n",
    "여러 번의 INSERT/UPDATE로 파일이 많아졌습니다. Sort 전략으로 컴팩션합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1000017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컴팩션 전: 파일 15개, 크기 377,788 bytes\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|0                         |0                     |0                    |0                      |\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n",
      "컴팩션 후: 파일 15개, 크기 377,788 bytes\n",
      "파일 수 변화: 15 → 15\n"
     ]
    }
   ],
   "source": [
    "files_before = count_files(TABLE_PATH)\n",
    "size_before = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 전: 파일 {files_before}개, 크기 {size_before:,} bytes\")\n",
    "\n",
    "# Sort 컴팩션 (order_date 기준)\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_data_files(\n",
    "    table => '{TABLE_NAME}',\n",
    "    strategy => 'sort',\n",
    "    sort_order => 'order_date ASC NULLS LAST'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "files_after = count_files(TABLE_PATH)\n",
    "size_after = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 후: 파일 {files_after}개, 크기 {size_after:,} bytes\")\n",
    "print(f\"파일 수 변화: {files_before} → {files_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000018",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: 스냅샷 정리 (유지보수)\n",
    "\n",
    "지금까지 생성된 스냅샷 중 최근 2개만 남기고 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1000019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 스냅샷 수: 12\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|deleted_data_files_count|deleted_position_delete_files_count|deleted_equality_delete_files_count|deleted_manifest_files_count|deleted_manifest_lists_count|deleted_statistics_files_count|\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|0                       |0                                  |0                                  |0                           |0                           |0                             |\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "\n",
      "정리 후 스냅샷 수: 12\n"
     ]
    }
   ],
   "source": [
    "# 현재 스냅샷 수 확인\n",
    "snapshots = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"현재 스냅샷 수: {len(snapshots)}\")\n",
    "\n",
    "# 최근 2개만 유지\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.expire_snapshots(\n",
    "    table => '{TABLE_NAME}',\n",
    "    retain_last => 2\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "snapshots_after = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"정리 후 스냅샷 수: {len(snapshots_after)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1000020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------+\n",
      "|rewritten_manifests_count|added_manifests_count|\n",
      "+-------------------------+---------------------+\n",
      "|2                        |1                    |\n",
      "+-------------------------+---------------------+\n",
      "\n",
      "유지보수 완료: Expire Snapshots + Rewrite Manifests\n"
     ]
    }
   ],
   "source": [
    "# Manifest 재작성\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_manifests(\n",
    "    table => '{TABLE_NAME}'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"유지보수 완료: Expire Snapshots + Rewrite Manifests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1000021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 5 완료 — 유지보수 후 테이블 상태:\n",
      "============================================================\n",
      "레코드 수: 600\n",
      "파일 수: 15\n",
      "스냅샷 수: 13\n",
      "\n",
      "파일 구조:\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-100-71e3fd79-15d4-4f60-bd25-c332d0c61d71-00001.parquet  (1.8 KB)\n",
      "│   │   ├── 00000-113-e944a71e-5121-4e8d-afb8-37abb4a9167b-00001.parquet  (3.6 KB)\n",
      "│   │   ├── 00000-120-5afec7b2-bc83-41bf-a590-06dd94582164-00001.parquet  (3.6 KB)\n",
      "│   │   ├── 00000-131-075482d4-9812-4903-9fb1-0375f5663e74-00001.parquet  (3.9 KB)\n",
      "│   │   ├── 00000-69-6b680a7c-560e-4038-8c12-bdb7ae56aed9-00001.parquet  (3.0 KB)\n",
      "│   │   └── 00000-75-4557e7d1-e2e2-4063-b7a9-bdc43bc7a0d7-00001.parquet  (2.8 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-113-e944a71e-5121-4e8d-afb8-37abb4a9167b-00002.parquet  (3.9 KB)\n",
      "│   │   ├── 00000-131-075482d4-9812-4903-9fb1-0375f5663e74-00002.parquet  (4.2 KB)\n",
      "│   │   ├── 00000-81-28b87232-ee0b-49b7-9eb7-32eddef66fe2-00001.parquet  (3.1 KB)\n",
      "│   │   └── 00000-87-d0f8bf21-cded-49f6-8166-8d245926ea36-00001.parquet  (2.9 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-113-e944a71e-5121-4e8d-afb8-37abb4a9167b-00003.parquet  (3.0 KB)\n",
      "│       ├── 00000-125-b3c638fc-a05f-42b0-941d-dc8f6f9b03e6-00001.parquet  (3.0 KB)\n",
      "│       ├── 00000-131-075482d4-9812-4903-9fb1-0375f5663e74-00003.parquet  (3.3 KB)\n",
      "│       ├── 00000-138-d4b7ff51-7a14-41fb-bd5c-9b577e9f2155-00001.parquet  (3.2 KB)\n",
      "│       └── 00000-93-ece02ff2-1055-4c78-98ca-edba739262b1-00001.parquet  (3.0 KB)\n",
      "└── metadata/\n",
      "    ├── 2c685101-e974-4ec3-afef-11d108ac0dca-m0.avro  (7.1 KB)\n",
      "    ├── 36a67aba-9289-4853-8ac0-2b74aba7b89e-m0.avro  (7.1 KB)\n",
      "    ├── 7bc98a4b-d7b9-4646-9b0b-5970c1deca2e-m0.avro  (7.3 KB)\n",
      "    ├── 7bc98a4b-d7b9-4646-9b0b-5970c1deca2e-m1.avro  (7.1 KB)\n",
      "    ├── 8a821792-c19e-41dc-8181-791579dc4545-m0.avro  (7.3 KB)\n",
      "    ├── 9178965a-fd48-4b92-8038-16e764ea9871-m0.avro  (7.1 KB)\n",
      "    ├── 9f5ba179-81f1-4333-bec4-bdb29c164c8d-m0.avro  (7.1 KB)\n",
      "    ├── a9cfbb55-5ec8-4acb-a504-181decb50988-m0.avro  (7.2 KB)\n",
      "    ├── aa663dea-a50d-4fa1-ab26-c5eaf75eed8b-m0.avro  (7.5 KB)\n",
      "    ├── ab18da90-7ea9-46ea-b4e1-c98cd1b94214-m0.avro  (7.1 KB)\n",
      "    ├── af325f71-a8d3-453f-bcf0-5ce994623c9c-m0.avro  (7.2 KB)\n",
      "    ├── af325f71-a8d3-453f-bcf0-5ce994623c9c-m1.avro  (7.1 KB)\n",
      "    ├── c8f0742e-15a5-450f-b7cf-c742db199572-m0.avro  (7.1 KB)\n",
      "    ├── e00a294a-f223-42e3-a9a4-dec18f97de85-m0.avro  (7.2 KB)\n",
      "    ├── e00a294a-f223-42e3-a9a4-dec18f97de85-m1.avro  (7.2 KB)\n",
      "    ├── e00a294a-f223-42e3-a9a4-dec18f97de85-m2.avro  (7.2 KB)\n",
      "    ├── e00a294a-f223-42e3-a9a4-dec18f97de85-m3.avro  (7.4 KB)\n",
      "    ├── fe08a641-43c6-403a-af8f-f9821cc2df49-m0.avro  (7.1 KB)\n",
      "    ├── fe08a641-43c6-403a-af8f-f9821cc2df49-m1.avro  (7.1 KB)\n",
      "    ├── fe08a641-43c6-403a-af8f-f9821cc2df49-m2.avro  (7.1 KB)\n",
      "    ├── fe08a641-43c6-403a-af8f-f9821cc2df49-m3.avro  (7.1 KB)\n",
      "    ├── fe08a641-43c6-403a-af8f-f9821cc2df49-m4.avro  (7.1 KB)\n",
      "    ├── fe08a641-43c6-403a-af8f-f9821cc2df49-m5.avro  (7.1 KB)\n",
      "    ├── snap-3246290473534841970-1-9178965a-fd48-4b92-8038-16e764ea9871.avro  (4.4 KB)\n",
      "    ├── snap-3441705049068171782-1-e00a294a-f223-42e3-a9a4-dec18f97de85.avro  (4.3 KB)\n",
      "    ├── snap-3455968571588914465-1-a9cfbb55-5ec8-4acb-a504-181decb50988.avro  (4.3 KB)\n",
      "    ├── snap-4494640826315992198-1-8a821792-c19e-41dc-8181-791579dc4545.avro  (4.2 KB)\n",
      "    ├── snap-5831915893391782548-1-aa663dea-a50d-4fa1-ab26-c5eaf75eed8b.avro  (4.2 KB)\n",
      "    ├── snap-6067149720729449558-1-9f5ba179-81f1-4333-bec4-bdb29c164c8d.avro  (4.3 KB)\n",
      "    ├── snap-6696529076417919547-1-7bc98a4b-d7b9-4646-9b0b-5970c1deca2e.avro  (4.2 KB)\n",
      "    ├── snap-6863172531723935657-1-fe08a641-43c6-403a-af8f-f9821cc2df49.avro  (4.3 KB)\n",
      "    ├── snap-6884703428675884281-1-36a67aba-9289-4853-8ac0-2b74aba7b89e.avro  (4.5 KB)\n",
      "    ├── snap-70413296967110654-1-c8f0742e-15a5-450f-b7cf-c742db199572.avro  (4.3 KB)\n",
      "    ├── snap-7899258830142532014-1-ab18da90-7ea9-46ea-b4e1-c98cd1b94214.avro  (4.4 KB)\n",
      "    ├── snap-8523543611858364875-1-2c685101-e974-4ec3-afef-11d108ac0dca.avro  (4.2 KB)\n",
      "    ├── snap-95883035658351778-1-af325f71-a8d3-453f-bcf0-5ce994623c9c.avro  (4.3 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v10.metadata.json  (10.5 KB)\n",
      "    ├── v11.metadata.json  (11.5 KB)\n",
      "    ├── v12.metadata.json  (12.4 KB)\n",
      "    ├── v13.metadata.json  (13.5 KB)\n",
      "    ├── v14.metadata.json  (14.5 KB)\n",
      "    ├── v15.metadata.json  (15.5 KB)\n",
      "    ├── v2.metadata.json  (2.5 KB)\n",
      "    ├── v3.metadata.json  (3.5 KB)\n",
      "    ├── v4.metadata.json  (4.5 KB)\n",
      "    ├── v5.metadata.json  (5.5 KB)\n",
      "    ├── v6.metadata.json  (6.4 KB)\n",
      "    ├── v7.metadata.json  (7.4 KB)\n",
      "    ├── v8.metadata.json  (8.4 KB)\n",
      "    ├── v9.metadata.json  (9.4 KB)\n",
      "    └── version-hint.text  (2 B)\n"
     ]
    }
   ],
   "source": [
    "# 유지보수 후 최종 상태\n",
    "print(\"Phase 5 완료 — 유지보수 후 테이블 상태:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n파일 구조:\")\n",
    "show_tree(TABLE_PATH, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000022",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6: 장애 복구 (Time Travel)\n",
    "\n",
    "### 시나리오: 실수로 데이터 대량 삭제\n",
    "\n",
    "운영자가 실수로 잘못된 DELETE 쿼리를 실행하여 데이터가 삭제되었습니다. Time Travel로 복구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1000023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제 전 레코드 수: 600\n",
      "안전한 스냅샷 ID: 5831915893391782548\n"
     ]
    }
   ],
   "source": [
    "# 삭제 전 레코드 수 기록\n",
    "count_before_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"삭제 전 레코드 수: {count_before_accident}\")\n",
    "\n",
    "# 현재 스냅샷 ID 기록 (복구 지점)\n",
    "safe_snapshot = spark.sql(f\"\"\"\n",
    "SELECT snapshot_id FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at DESC LIMIT 1\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"안전한 스냅샷 ID: {safe_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1000024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[사고 발생] 실수로 Seoul 지역 데이터 전체 삭제!\n",
      "삭제 후 레코드 수: 419 (181건 삭제됨!)\n",
      "\n",
      "현재 region 분포:\n",
      "+------+--------+\n",
      "|region|count(1)|\n",
      "+------+--------+\n",
      "|  Jeju|     228|\n",
      "| Busan|     191|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사고 발생: 실수로 Seoul 지역 데이터 전체 삭제!\n",
    "print(\"[사고 발생] 실수로 Seoul 지역 데이터 전체 삭제!\")\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE region = 'Seoul'\")\n",
    "\n",
    "count_after_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "deleted = count_before_accident - count_after_accident\n",
    "print(f\"삭제 후 레코드 수: {count_after_accident} ({deleted}건 삭제됨!)\")\n",
    "\n",
    "print(\"\\n현재 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1000025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[복구] 스냅샷 5831915893391782548의 데이터 확인:\n",
      "스냅샷 5831915893391782548 레코드 수: 600\n",
      "\n",
      "스냅샷의 region 분포:\n",
      "+------+-----+\n",
      "|region|count|\n",
      "+------+-----+\n",
      "| Seoul|  181|\n",
      "|  Jeju|  228|\n",
      "| Busan|  191|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 복구 방법 1: Time Travel로 삭제 전 데이터 확인\n",
    "print(f\"[복구] 스냅샷 {safe_snapshot}의 데이터 확인:\")\n",
    "safe_count = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*) FROM {TABLE_NAME}\n",
    "VERSION AS OF {safe_snapshot}\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"스냅샷 {safe_snapshot} 레코드 수: {safe_count}\")\n",
    "\n",
    "print(\"\\n스냅샷의 region 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME} VERSION AS OF {safe_snapshot}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1000026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[복구 실행] Rollback to safe snapshot...\n",
      "\n",
      "복구 완료!\n",
      "레코드 수: 600 → 419 (사고) → 600 (복구)\n",
      "\n",
      "복구 후 region 분포:\n",
      "+------+--------+\n",
      "|region|count(1)|\n",
      "+------+--------+\n",
      "| Seoul|     181|\n",
      "|  Jeju|     228|\n",
      "| Busan|     191|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 복구 방법 2: Rollback으로 테이블 상태를 이전 스냅샷으로 되돌리기\n",
    "print(\"[복구 실행] Rollback to safe snapshot...\")\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rollback_to_snapshot(\n",
    "    table => '{TABLE_NAME}',\n",
    "    snapshot_id => {safe_snapshot}\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "count_after_recovery = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n복구 완료!\")\n",
    "print(\n",
    "    f\"레코드 수: {count_before_accident} → {count_after_accident} (사고) → {count_after_recovery} (복구)\"\n",
    ")\n",
    "\n",
    "print(\"\\n복구 후 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000027",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 장애 복구\n",
    "\n",
    "- `VERSION AS OF`로 삭제 전 데이터를 **먼저 확인**한 후 복구할 수 있었습니다\n",
    "- `rollback_to_snapshot`으로 테이블 상태를 **원래대로 되돌렸습니다**\n",
    "- Rollback은 데이터를 복사하지 않고 **메타데이터 포인터만 변경**하므로 즉시 완료됩니다\n",
    "- 단, `expire_snapshots`로 삭제된 스냅샷으로는 복구할 수 없으므로 **보존 기간 설정이 중요**합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000028",
   "metadata": {},
   "source": [
    "---\n",
    "## 전체 파이프라인 요약\n",
    "\n",
    "```\n",
    "Phase 1: 일별 배치 적재        → INSERT (append)\n",
    "Phase 2: 상태 업데이트          → UPDATE (COW)\n",
    "Phase 3: 스키마 변경           → ALTER TABLE ADD COLUMN\n",
    "Phase 4: 컴팩션               → rewrite_data_files (sort)\n",
    "Phase 5: 스냅샷 정리           → expire_snapshots + rewrite_manifests\n",
    "Phase 6: 장애 복구             → VERSION AS OF + rollback_to_snapshot\n",
    "```\n",
    "\n",
    "### 운영 체크리스트\n",
    "\n",
    "| 항목 | 주기 | 명령 |\n",
    "|------|------|------|\n",
    "| 데이터 적재 | 매일 | `df.writeTo(table).append()` |\n",
    "| 컴팩션 | 매일 | `rewrite_data_files(strategy => 'sort')` |\n",
    "| 스냅샷 만료 | 매일~주간 | `expire_snapshots(retain_last => N)` |\n",
    "| 고아 파일 정리 | 주간~월간 | `remove_orphan_files(dry_run => true)` |\n",
    "| Manifest 재작성 | 주간 | `rewrite_manifests()` |\n",
    "| 파티션 모니터링 | 주간 | `SELECT * FROM table.partitions` |\n",
    "| 파일 크기 모니터링 | 주간 | `SELECT * FROM table.files` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1000029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "최종 테이블 상태\n",
      "============================================================\n",
      "레코드 수: 600\n",
      "파일 수: 18\n",
      "총 크기: 470,164 bytes\n",
      "스냅샷 수: 14\n",
      "\n",
      "스키마:\n",
      "+--------------+------------------+-------+\n",
      "|      col_name|         data_type|comment|\n",
      "+--------------+------------------+-------+\n",
      "|      order_id|            bigint|   null|\n",
      "|   customer_id|            bigint|   null|\n",
      "|  product_name|            string|   null|\n",
      "|    order_date|              date|   null|\n",
      "|        amount|     decimal(10,2)|   null|\n",
      "|        status|            string|   null|\n",
      "|        region|            string|   null|\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|        Part 0|months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n",
      "파티션:\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at        |last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|{648}    |0      |180         |1         |4012                         |0                           |0                         |0                           |0                         |2026-02-16 05:51:41.973|3441705049068171782     |\n",
      "|{649}    |0      |210         |1         |4326                         |0                           |0                         |0                           |0                         |2026-02-16 05:51:41.973|3441705049068171782     |\n",
      "|{650}    |0      |210         |2         |6709                         |0                           |0                         |0                           |0                         |2026-02-16 05:51:42.5  |3455968571588914465     |\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최종 테이블 상태\n",
    "print(\"=\" * 60)\n",
    "print(\"최종 테이블 상태\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n스키마:\")\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()\n",
    "\n",
    "print(\"파티션:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1000030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n",
      "\n",
      "End-to-End 시나리오 실습 완료!\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")\n",
    "print(\"\\nEnd-to-End 시나리오 실습 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
