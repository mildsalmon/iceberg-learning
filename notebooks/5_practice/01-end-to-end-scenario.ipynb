{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1000001",
   "metadata": {},
   "source": [
    "# End-to-End 시나리오 — 일별 배치부터 장애 복구까지\n",
    "\n",
    "이 노트북에서는 지금까지 배운 모든 내용을 **실전 시나리오**로 종합 실습합니다.\n",
    "\n",
    "### 시나리오\n",
    "\n",
    "온라인 쇼핑몰의 주문 데이터 파이프라인:\n",
    "\n",
    "1. **일별 배치 적재** — 매일 새로운 주문 데이터 INSERT\n",
    "2. **상태 업데이트** — 배송 완료된 주문 UPDATE\n",
    "3. **스키마 변경** — 새 컬럼 추가 (Schema Evolution)\n",
    "4. **컴팩션** — Small File Problem 해결\n",
    "5. **스냅샷 정리** — 유지보수 작업\n",
    "6. **장애 복구** — 실수로 데이터 삭제 후 Time Travel로 복구"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000002",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, snapshot_tree, diff_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "TABLE_NAME = \"demo.lab.e2e_orders\"\n",
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/e2e_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000005",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: 테이블 생성 + 일별 배치 적재\n",
    "\n",
    "월별 Hidden Partitioning으로 테이블을 생성하고, 5일치 배치 데이터를 순차적으로 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(f\"테이블 생성 완료: {TABLE_NAME}\")\n",
    "print(\"파티셔닝: months(order_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1~5: 일별 배치 적재 시뮬레이션\n",
    "daily_batches = [\n",
    "    {\"day\": 1, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 100},\n",
    "    {\"day\": 2, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 80},\n",
    "    {\"day\": 3, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 120},\n",
    "    {\"day\": 4, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 90},\n",
    "    {\"day\": 5, \"start\": \"2024-03-01\", \"end\": \"2024-03-31\", \"records\": 110},\n",
    "]\n",
    "\n",
    "offset = 1\n",
    "for batch in daily_batches:\n",
    "    orders = generate_orders(\n",
    "        num_records=batch[\"records\"],\n",
    "        seed=batch[\"day\"],\n",
    "        start_date=batch[\"start\"],\n",
    "        end_date=batch[\"end\"],\n",
    "        id_offset=offset\n",
    "    )\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(TABLE_NAME).append()\n",
    "    offset += batch[\"records\"]\n",
    "    print(f\"Day {batch['day']}: {batch['records']}건 적재 ({batch['start']} ~ {batch['end']})\")\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n총 레코드: {total}건\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 상태 확인\n",
    "print(\"Phase 1 완료 — 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH, max_depth=3)\n",
    "\n",
    "print(\"\\n스냅샷 히스토리:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT snapshot_id, committed_at, operation,\n",
    "       summary['added-data-files'] as added_files,\n",
    "       summary['added-records'] as added_records\n",
    "FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000009",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: 상태 업데이트 (COW)\n",
    "\n",
    "배송이 완료된 주문의 상태를 `shipped`로, 일부 주문을 `cancelled`로 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# pending → shipped\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'shipped'\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\")\n",
    "shipped = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'shipped'\").collect()[0][0]\n",
    "print(f\"shipped 상태 업데이트 완료 (현재 shipped: {shipped}건)\")\n",
    "\n",
    "# 일부 cancelled\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'cancelled'\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\")\n",
    "cancelled = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'cancelled'\").collect()[0][0]\n",
    "print(f\"cancelled 상태 업데이트 완료 (현재 cancelled: {cancelled}건)\")\n",
    "\n",
    "after = snapshot_tree(TABLE_PATH)\n",
    "print(\"\\n파일 변경 사항 (COW 방식):\")\n",
    "diff_tree(before, after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태별 분포 확인\n",
    "print(\"주문 상태 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT status, COUNT(*) as count\n",
    "FROM {TABLE_NAME}\n",
    "GROUP BY status\n",
    "ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000012",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: 스키마 변경 (Schema Evolution)\n",
    "\n",
    "비즈니스 요구사항 변경: `region` 컬럼을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 컬럼 추가\n",
    "spark.sql(f\"ALTER TABLE {TABLE_NAME} ADD COLUMN region STRING\")\n",
    "print(\"region 컬럼 추가 완료\")\n",
    "\n",
    "# 스키마 확인\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터에 region 값 업데이트\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET region = CASE\n",
    "    WHEN customer_id % 3 = 0 THEN 'Seoul'\n",
    "    WHEN customer_id % 3 = 1 THEN 'Busan'\n",
    "    ELSE 'Jeju'\n",
    "END\n",
    "\"\"\")\n",
    "\n",
    "print(\"기존 데이터 region 업데이트 완료\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 데이터는 region 포함하여 삽입\n",
    "from pyspark.sql.functions import lit, when, col\n",
    "\n",
    "orders_new = generate_orders(num_records=100, seed=99, start_date=\"2024-03-15\", end_date=\"2024-03-31\", id_offset=501)\n",
    "df_new = to_spark_df(spark, orders_new)\n",
    "df_new = df_new.withColumn(\"region\", \n",
    "    when(col(\"customer_id\") % 3 == 0, lit(\"Seoul\"))\n",
    "    .when(col(\"customer_id\") % 3 == 1, lit(\"Busan\"))\n",
    "    .otherwise(lit(\"Jeju\"))\n",
    ")\n",
    "df_new.writeTo(TABLE_NAME).append()\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"새 데이터 100건 추가 (region 포함). 총 레코드: {total}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000016",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: 컴팩션\n",
    "\n",
    "여러 번의 INSERT/UPDATE로 파일이 많아졌습니다. Sort 전략으로 컴팩션합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_before = count_files(TABLE_PATH)\n",
    "size_before = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 전: 파일 {files_before}개, 크기 {size_before:,} bytes\")\n",
    "\n",
    "# Sort 컴팩션 (order_date 기준)\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_data_files(\n",
    "    table => '{TABLE_NAME}',\n",
    "    strategy => 'sort',\n",
    "    sort_order => 'order_date ASC NULLS LAST'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "files_after = count_files(TABLE_PATH)\n",
    "size_after = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 후: 파일 {files_after}개, 크기 {size_after:,} bytes\")\n",
    "print(f\"파일 수 변화: {files_before} → {files_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000018",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: 스냅샷 정리 (유지보수)\n",
    "\n",
    "지금까지 생성된 스냅샷 중 최근 2개만 남기고 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 스냅샷 수 확인\n",
    "snapshots = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"현재 스냅샷 수: {len(snapshots)}\")\n",
    "\n",
    "# 최근 2개만 유지\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.expire_snapshots(\n",
    "    table => '{TABLE_NAME}',\n",
    "    retain_last => 2\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "snapshots_after = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"정리 후 스냅샷 수: {len(snapshots_after)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifest 재작성\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_manifests(\n",
    "    table => '{TABLE_NAME}'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"유지보수 완료: Expire Snapshots + Rewrite Manifests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유지보수 후 최종 상태\n",
    "print(\"Phase 5 완료 — 유지보수 후 테이블 상태:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n파일 구조:\")\n",
    "show_tree(TABLE_PATH, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000022",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6: 장애 복구 (Time Travel)\n",
    "\n",
    "### 시나리오: 실수로 데이터 대량 삭제\n",
    "\n",
    "운영자가 실수로 잘못된 DELETE 쿼리를 실행하여 데이터가 삭제되었습니다. Time Travel로 복구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제 전 레코드 수 기록\n",
    "count_before_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"삭제 전 레코드 수: {count_before_accident}\")\n",
    "\n",
    "# 현재 스냅샷 ID 기록 (복구 지점)\n",
    "safe_snapshot = spark.sql(f\"\"\"\n",
    "SELECT snapshot_id FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at DESC LIMIT 1\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"안전한 스냅샷 ID: {safe_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사고 발생: 실수로 Seoul 지역 데이터 전체 삭제!\n",
    "print(\"[사고 발생] 실수로 Seoul 지역 데이터 전체 삭제!\")\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE region = 'Seoul'\")\n",
    "\n",
    "count_after_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "deleted = count_before_accident - count_after_accident\n",
    "print(f\"삭제 후 레코드 수: {count_after_accident} ({deleted}건 삭제됨!)\")\n",
    "\n",
    "print(\"\\n현재 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복구 방법 1: Time Travel로 삭제 전 데이터 확인\n",
    "print(f\"[복구] 스냅샷 {safe_snapshot}의 데이터 확인:\")\n",
    "safe_count = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*) FROM {TABLE_NAME}\n",
    "VERSION AS OF {safe_snapshot}\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"스냅샷 {safe_snapshot} 레코드 수: {safe_count}\")\n",
    "\n",
    "print(\"\\n스냅샷의 region 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME} VERSION AS OF {safe_snapshot}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복구 방법 2: Rollback으로 테이블 상태를 이전 스냅샷으로 되돌리기\n",
    "print(\"[복구 실행] Rollback to safe snapshot...\")\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rollback_to_snapshot(\n",
    "    table => '{TABLE_NAME}',\n",
    "    snapshot_id => {safe_snapshot}\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "count_after_recovery = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n복구 완료!\")\n",
    "print(f\"레코드 수: {count_before_accident} → {count_after_accident} (사고) → {count_after_recovery} (복구)\")\n",
    "\n",
    "print(\"\\n복구 후 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000027",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 장애 복구\n",
    "\n",
    "- `VERSION AS OF`로 삭제 전 데이터를 **먼저 확인**한 후 복구할 수 있었습니다\n",
    "- `rollback_to_snapshot`으로 테이블 상태를 **원래대로 되돌렸습니다**\n",
    "- Rollback은 데이터를 복사하지 않고 **메타데이터 포인터만 변경**하므로 즉시 완료됩니다\n",
    "- 단, `expire_snapshots`로 삭제된 스냅샷으로는 복구할 수 없으므로 **보존 기간 설정이 중요**합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000028",
   "metadata": {},
   "source": [
    "---\n",
    "## 전체 파이프라인 요약\n",
    "\n",
    "```\n",
    "Phase 1: 일별 배치 적재        → INSERT (append)\n",
    "Phase 2: 상태 업데이트          → UPDATE (COW)\n",
    "Phase 3: 스키마 변경           → ALTER TABLE ADD COLUMN\n",
    "Phase 4: 컴팩션               → rewrite_data_files (sort)\n",
    "Phase 5: 스냅샷 정리           → expire_snapshots + rewrite_manifests\n",
    "Phase 6: 장애 복구             → VERSION AS OF + rollback_to_snapshot\n",
    "```\n",
    "\n",
    "### 운영 체크리스트\n",
    "\n",
    "| 항목 | 주기 | 명령 |\n",
    "|------|------|------|\n",
    "| 데이터 적재 | 매일 | `df.writeTo(table).append()` |\n",
    "| 컴팩션 | 매일 | `rewrite_data_files(strategy => 'sort')` |\n",
    "| 스냅샷 만료 | 매일~주간 | `expire_snapshots(retain_last => N)` |\n",
    "| 고아 파일 정리 | 주간~월간 | `remove_orphan_files(dry_run => true)` |\n",
    "| Manifest 재작성 | 주간 | `rewrite_manifests()` |\n",
    "| 파티션 모니터링 | 주간 | `SELECT * FROM table.partitions` |\n",
    "| 파일 크기 모니터링 | 주간 | `SELECT * FROM table.files` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 테이블 상태\n",
    "print(\"=\" * 60)\n",
    "print(\"최종 테이블 상태\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n스키마:\")\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()\n",
    "\n",
    "print(\"파티션:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000030",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")\n",
    "print(\"\\nEnd-to-End 시나리오 실습 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}