{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1000001",
   "metadata": {},
   "source": [
    "# End-to-End 시나리오 — 일별 배치부터 장애 복구까지\n",
    "\n",
    "이 노트북에서는 지금까지 배운 모든 내용을 **실전 시나리오**로 종합 실습합니다.\n",
    "\n",
    "### 시나리오\n",
    "\n",
    "온라인 쇼핑몰의 주문 데이터 파이프라인:\n",
    "\n",
    "1. **일별 배치 적재** — 매일 새로운 주문 데이터 INSERT\n",
    "2. **상태 업데이트** — 배송 완료된 주문 UPDATE\n",
    "3. **스키마 변경** — 새 컬럼 추가 (Schema Evolution)\n",
    "4. **컴팩션** — Small File Problem 해결\n",
    "5. **스냅샷 정리** — 유지보수 작업\n",
    "6. **장애 복구** — 실수로 데이터 삭제 후 Time Travel로 복구"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000002",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import time\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import (\n",
    "    show_tree,\n",
    "    snapshot_tree,\n",
    "    diff_tree,\n",
    "    count_files,\n",
    "    total_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "TABLE_NAME = \"demo.lab.e2e_orders\"\n",
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/e2e_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000005",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: 테이블 생성 + 일별 배치 적재\n",
    "\n",
    "월별 Hidden Partitioning으로 테이블을 생성하고, 5일치 배치 데이터를 순차적으로 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1000006",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35.sql.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'demo': org.apache.iceberg.spark.SparkCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:2079)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:53)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:53)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n\tat org.apache.spark.sql.catalyst.plans.logical.DropTable.mapChildren(v2Commands.scala:802)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.SparkCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 78 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDROP TABLE IF EXISTS \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mTABLE_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mCREATE TABLE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTABLE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    order_id BIGINT,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mPARTITIONED BY (months(order_date))\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m테이블 생성 완료: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTABLE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35.sql.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'demo': org.apache.iceberg.spark.SparkCatalog.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:2079)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:70)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:53)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:53)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1249)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1248)\n\tat org.apache.spark.sql.catalyst.plans.logical.DropTable.mapChildren(v2Commands.scala:802)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.apache.iceberg.spark.SparkCatalog\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:60)\n\t... 78 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(f\"테이블 생성 완료: {TABLE_NAME}\")\n",
    "print(\"파티셔닝: months(order_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1~5: 일별 배치 적재 시뮬레이션\n",
    "daily_batches = [\n",
    "    {\"day\": 1, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 100},\n",
    "    {\"day\": 2, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 80},\n",
    "    {\"day\": 3, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 120},\n",
    "    {\"day\": 4, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 90},\n",
    "    {\"day\": 5, \"start\": \"2024-03-01\", \"end\": \"2024-03-31\", \"records\": 110},\n",
    "]\n",
    "\n",
    "offset = 1\n",
    "for batch in daily_batches:\n",
    "    orders = generate_orders(\n",
    "        num_records=batch[\"records\"],\n",
    "        seed=batch[\"day\"],\n",
    "        start_date=batch[\"start\"],\n",
    "        end_date=batch[\"end\"],\n",
    "        id_offset=offset,\n",
    "    )\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(TABLE_NAME).append()\n",
    "    offset += batch[\"records\"]\n",
    "    print(\n",
    "        f\"Day {batch['day']}: {batch['records']}건 적재 ({batch['start']} ~ {batch['end']})\"\n",
    "    )\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n총 레코드: {total}건\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 상태 확인\n",
    "print(\"Phase 1 완료 — 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH, max_depth=3)\n",
    "\n",
    "print(\"\\n스냅샷 히스토리:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT snapshot_id, committed_at, operation,\n",
    "       summary['added-data-files'] as added_files,\n",
    "       summary['added-records'] as added_records\n",
    "FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f48bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1.5: 중복 데이터 유입과 정리\n",
    "\n",
    "운영에서는 동일 주문이 재전송되어 중복 키가 들어올 수 있습니다.\n",
    "업데이트 전에 중복을 탐지/정리하여 이후 단계의 정합성을 보장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf500c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 3건을 의도적으로 중복 유입\n",
    "dup_rows = spark.sql(f\"\"\"\n",
    "SELECT order_id, customer_id, product_name, order_date, amount, status\n",
    "FROM {TABLE_NAME}\n",
    "ORDER BY order_id\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "dup_rows.writeTo(TABLE_NAME).append()\n",
    "\n",
    "dup_keys = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    ") t\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"중복 주입 후 중복 order_id 개수: {dup_keys}\")\n",
    "\n",
    "# order_id 기준으로 1건만 남기도록 정리\n",
    "# 중요: 원본 테이블 삭제 전에 dedup 결과를 물질화해야 lazy evaluation으로 빈 테이블을 다시 읽지 않음\n",
    "w = Window.partitionBy(\"order_id\").orderBy(\n",
    "    F.col(\"order_date\").desc(), F.col(\"status\").desc()\n",
    ")\n",
    "dedup_df = (\n",
    "    spark.table(TABLE_NAME)\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    "    .cache()\n",
    ")\n",
    "dedup_rows_count = dedup_df.count()\n",
    "print(f\"중복 정리 대상 레코드 수: {dedup_rows_count}\")\n",
    "\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE true\")\n",
    "dedup_df.writeTo(TABLE_NAME).append()\n",
    "dedup_df.unpersist()\n",
    "\n",
    "dup_keys_after = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    ") t\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"중복 정리 후 중복 order_id 개수: {dup_keys_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e1311",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 중복 데이터 정리\n",
    "\n",
    "- 중복 유입을 조기에 정리하면 UPDATE/DELETE/집계 결과 왜곡을 예방할 수 있습니다.\n",
    "- 운영 파이프라인에는 배치 적재 직후 중복 탐지 쿼리를 자동화하는 것이 안전합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000009",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: 상태 업데이트 (COW)\n",
    "\n",
    "배송이 완료된 주문의 상태를 `shipped`로, 일부 주문을 `cancelled`로 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# pending → shipped\n",
    "pending_target = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM {TABLE_NAME}\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"shipped 업데이트 대상: {pending_target}건\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'shipped'\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\")\n",
    "shipped = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'shipped'\"\n",
    ").collect()[0][0]\n",
    "print(f\"shipped 상태 업데이트 완료 (현재 shipped: {shipped}건)\")\n",
    "\n",
    "# 일부 cancelled\n",
    "cancel_target = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM {TABLE_NAME}\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"cancelled 업데이트 대상: {cancel_target}건\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'cancelled'\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\")\n",
    "cancelled = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'cancelled'\"\n",
    ").collect()[0][0]\n",
    "print(f\"cancelled 상태 업데이트 완료 (현재 cancelled: {cancelled}건)\")\n",
    "\n",
    "after = snapshot_tree(TABLE_PATH)\n",
    "print(\"\\n파일 변경 사항 (COW 방식):\")\n",
    "diff_tree(before, after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태별 분포 확인\n",
    "print(\"주문 상태 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT status, COUNT(*) as count\n",
    "FROM {TABLE_NAME}\n",
    "GROUP BY status\n",
    "ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000012",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: 스키마 변경 (Schema Evolution)\n",
    "\n",
    "비즈니스 요구사항 변경: `region` 컬럼을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region 컬럼 추가\n",
    "spark.sql(f\"ALTER TABLE {TABLE_NAME} ADD COLUMN region STRING\")\n",
    "print(\"region 컬럼 추가 완료\")\n",
    "\n",
    "# 스키마 확인\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터에 region 값 업데이트\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET region = CASE\n",
    "    WHEN customer_id % 3 = 0 THEN 'Seoul'\n",
    "    WHEN customer_id % 3 = 1 THEN 'Busan'\n",
    "    ELSE 'Jeju'\n",
    "END\n",
    "\"\"\")\n",
    "\n",
    "print(\"기존 데이터 region 업데이트 완료\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 데이터는 region 포함하여 삽입\n",
    "from pyspark.sql.functions import lit, when, col\n",
    "\n",
    "orders_new = generate_orders(\n",
    "    num_records=100,\n",
    "    seed=99,\n",
    "    start_date=\"2024-03-15\",\n",
    "    end_date=\"2024-03-31\",\n",
    "    id_offset=501,\n",
    ")\n",
    "df_new = to_spark_df(spark, orders_new)\n",
    "df_new = df_new.withColumn(\n",
    "    \"region\",\n",
    "    when(col(\"customer_id\") % 3 == 0, lit(\"Seoul\"))\n",
    "    .when(col(\"customer_id\") % 3 == 1, lit(\"Busan\"))\n",
    "    .otherwise(lit(\"Jeju\")),\n",
    ")\n",
    "df_new.writeTo(TABLE_NAME).append()\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"새 데이터 100건 추가 (region 포함). 총 레코드: {total}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000016",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: 컴팩션\n",
    "\n",
    "여러 번의 INSERT/UPDATE로 파일이 많아졌습니다. Sort 전략으로 컴팩션합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_before = count_files(TABLE_PATH)\n",
    "size_before = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 전: 파일 {files_before}개, 크기 {size_before:,} bytes\")\n",
    "\n",
    "# Sort 컴팩션 (order_date 기준)\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_data_files(\n",
    "    table => '{TABLE_NAME}',\n",
    "    strategy => 'sort',\n",
    "    sort_order => 'order_date ASC NULLS LAST'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "files_after = count_files(TABLE_PATH)\n",
    "size_after = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 후: 파일 {files_after}개, 크기 {size_after:,} bytes\")\n",
    "print(f\"파일 수 변화: {files_before} → {files_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000018",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: 스냅샷 정리 (유지보수)\n",
    "\n",
    "지금까지 생성된 스냅샷 중 최근 2개만 남기고 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 스냅샷 수 확인\n",
    "snapshots = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"현재 스냅샷 수: {len(snapshots)}\")\n",
    "\n",
    "# 최근 2개만 유지\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.expire_snapshots(\n",
    "    table => '{TABLE_NAME}',\n",
    "    retain_last => 2\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "snapshots_after = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"정리 후 스냅샷 수: {len(snapshots_after)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifest 재작성\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_manifests(\n",
    "    table => '{TABLE_NAME}'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"유지보수 완료: Expire Snapshots + Rewrite Manifests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유지보수 후 최종 상태\n",
    "print(\"Phase 5 완료 — 유지보수 후 테이블 상태:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n파일 구조:\")\n",
    "show_tree(TABLE_PATH, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000022",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6: 장애 복구 (Time Travel)\n",
    "\n",
    "### 시나리오: 실수로 데이터 대량 삭제\n",
    "\n",
    "운영자가 실수로 잘못된 DELETE 쿼리를 실행하여 데이터가 삭제되었습니다. Time Travel로 복구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제 전 레코드 수 기록\n",
    "count_before_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"삭제 전 레코드 수: {count_before_accident}\")\n",
    "\n",
    "# 현재 스냅샷 ID 기록 (복구 지점)\n",
    "safe_snapshot = spark.sql(f\"\"\"\n",
    "SELECT snapshot_id FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at DESC LIMIT 1\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"안전한 스냅샷 ID: {safe_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사고 발생: 실수로 Seoul 지역 데이터 전체 삭제!\n",
    "print(\"[사고 발생] 실수로 Seoul 지역 데이터 전체 삭제!\")\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE region = 'Seoul'\")\n",
    "\n",
    "count_after_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "deleted = count_before_accident - count_after_accident\n",
    "print(f\"삭제 후 레코드 수: {count_after_accident} ({deleted}건 삭제됨!)\")\n",
    "\n",
    "print(\"\\n현재 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복구 방법 1: Time Travel로 삭제 전 데이터 확인\n",
    "print(f\"[복구] 스냅샷 {safe_snapshot}의 데이터 확인:\")\n",
    "safe_count = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*) FROM {TABLE_NAME}\n",
    "VERSION AS OF {safe_snapshot}\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"스냅샷 {safe_snapshot} 레코드 수: {safe_count}\")\n",
    "\n",
    "print(\"\\n스냅샷의 region 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME} VERSION AS OF {safe_snapshot}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복구 방법 2: Rollback으로 테이블 상태를 이전 스냅샷으로 되돌리기\n",
    "print(\"[복구 실행] Rollback to safe snapshot...\")\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rollback_to_snapshot(\n",
    "    table => '{TABLE_NAME}',\n",
    "    snapshot_id => {safe_snapshot}\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "count_after_recovery = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n복구 완료!\")\n",
    "print(\n",
    "    f\"레코드 수: {count_before_accident} → {count_after_accident} (사고) → {count_after_recovery} (복구)\"\n",
    ")\n",
    "\n",
    "print(\"\\n복구 후 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000027",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 장애 복구\n",
    "\n",
    "- `VERSION AS OF`로 삭제 전 데이터를 **먼저 확인**한 후 복구할 수 있었습니다\n",
    "- `rollback_to_snapshot`으로 테이블 상태를 **원래대로 되돌렸습니다**\n",
    "- Rollback은 데이터를 복사하지 않고 **메타데이터 포인터만 변경**하므로 즉시 완료됩니다\n",
    "- 단, `expire_snapshots`로 삭제된 스냅샷으로는 복구할 수 없으므로 **보존 기간 설정이 중요**합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000028",
   "metadata": {},
   "source": [
    "---\n",
    "## 전체 파이프라인 요약\n",
    "\n",
    "```\n",
    "Phase 1: 일별 배치 적재        → INSERT (append)\n",
    "Phase 2: 상태 업데이트          → UPDATE (COW)\n",
    "Phase 3: 스키마 변경           → ALTER TABLE ADD COLUMN\n",
    "Phase 4: 컴팩션               → rewrite_data_files (sort)\n",
    "Phase 5: 스냅샷 정리           → expire_snapshots + rewrite_manifests\n",
    "Phase 6: 장애 복구             → VERSION AS OF + rollback_to_snapshot\n",
    "```\n",
    "\n",
    "### 운영 체크리스트\n",
    "\n",
    "| 항목 | 주기 | 명령 |\n",
    "|------|------|------|\n",
    "| 데이터 적재 | 매일 | `df.writeTo(table).append()` |\n",
    "| 컴팩션 | 매일 | `rewrite_data_files(strategy => 'sort')` |\n",
    "| 스냅샷 만료 | 매일~주간 | `expire_snapshots(retain_last => N)` |\n",
    "| 고아 파일 정리 | 주간~월간 | `remove_orphan_files(dry_run => true)` |\n",
    "| Manifest 재작성 | 주간 | `rewrite_manifests()` |\n",
    "| 파티션 모니터링 | 주간 | `SELECT * FROM table.partitions` |\n",
    "| 파일 크기 모니터링 | 주간 | `SELECT * FROM table.files` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 테이블 상태\n",
    "print(\"=\" * 60)\n",
    "print(\"최종 테이블 상태\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n스키마:\")\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()\n",
    "\n",
    "print(\"파티션:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1000030",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")\n",
    "print(\"\\nEnd-to-End 시나리오 실습 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
