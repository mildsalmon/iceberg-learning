{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1000001",
   "metadata": {},
   "source": [
    "# End-to-End 시나리오 — 일별 배치부터 장애 복구까지\n",
    "\n",
    "이 노트북에서는 지금까지 배운 모든 내용을 **실전 시나리오**로 종합 실습합니다.\n",
    "\n",
    "### 시나리오\n",
    "\n",
    "온라인 쇼핑몰의 주문 데이터 파이프라인:\n",
    "\n",
    "1. **일별 배치 적재** — 매일 새로운 주문 데이터 INSERT\n",
    "2. **상태 업데이트** — 배송 완료된 주문 UPDATE\n",
    "3. **스키마 변경** — 새 컬럼 추가 (Schema Evolution)\n",
    "4. **컴팩션** — Small File Problem 해결\n",
    "5. **스냅샷 정리** — 유지보수 작업\n",
    "6. **장애 복구** — 실수로 데이터 삭제 후 Time Travel로 복구"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000002",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import time\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import (\n",
    "    show_tree,\n",
    "    snapshot_tree,\n",
    "    diff_tree,\n",
    "    count_files,\n",
    "    total_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "TABLE_NAME = \"demo.lab.e2e_orders\"\n",
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/e2e_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000005",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: 테이블 생성 + 일별 배치 적재\n",
    "\n",
    "월별 Hidden Partitioning으로 테이블을 생성하고, 5일치 배치 데이터를 순차적으로 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테이블 생성 완료: demo.lab.e2e_orders\n",
      "파티셔닝: months(order_date)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(f\"테이블 생성 완료: {TABLE_NAME}\")\n",
    "print(\"파티셔닝: months(order_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1: 100건 적재 (2024-01-01 ~ 2024-01-31)\n",
      "Day 2: 80건 적재 (2024-01-01 ~ 2024-01-31)\n",
      "Day 3: 120건 적재 (2024-02-01 ~ 2024-02-28)\n",
      "Day 4: 90건 적재 (2024-02-01 ~ 2024-02-28)\n",
      "Day 5: 110건 적재 (2024-03-01 ~ 2024-03-31)\n",
      "\n",
      "총 레코드: 500건\n",
      "파일 수: 5\n"
     ]
    }
   ],
   "source": [
    "# Day 1~5: 일별 배치 적재 시뮬레이션\n",
    "daily_batches = [\n",
    "    {\"day\": 1, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 100},\n",
    "    {\"day\": 2, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 80},\n",
    "    {\"day\": 3, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 120},\n",
    "    {\"day\": 4, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 90},\n",
    "    {\"day\": 5, \"start\": \"2024-03-01\", \"end\": \"2024-03-31\", \"records\": 110},\n",
    "]\n",
    "\n",
    "offset = 1\n",
    "for batch in daily_batches:\n",
    "    orders = generate_orders(\n",
    "        num_records=batch[\"records\"],\n",
    "        seed=batch[\"day\"],\n",
    "        start_date=batch[\"start\"],\n",
    "        end_date=batch[\"end\"],\n",
    "        id_offset=offset,\n",
    "    )\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(TABLE_NAME).append()\n",
    "    offset += batch[\"records\"]\n",
    "    print(\n",
    "        f\"Day {batch['day']}: {batch['records']}건 적재 ({batch['start']} ~ {batch['end']})\"\n",
    "    )\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n총 레코드: {total}건\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 완료 — 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-909-2bae8660-2a17-42df-97a9-29af7561946b-00001.parquet  (3.0 KB)\n",
      "│   │   └── 00000-915-b36fd36b-b991-42a3-9e99-947fdf5ed9ff-00001.parquet  (2.8 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-921-f1531eac-1d7e-4418-bfae-8390be0e6105-00001.parquet  (3.1 KB)\n",
      "│   │   └── 00000-927-51b251c5-074d-45de-b859-679d8d45f3c3-00001.parquet  (2.9 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       └── 00000-933-a8396472-707d-4126-beb1-7ea8407d69e1-00001.parquet  (3.0 KB)\n",
      "└── metadata/\n",
      "    ├── 24cf3d69-37c1-498f-bfd9-47776769a4d1-m0.avro  (7.1 KB)\n",
      "    ├── 40874836-6de6-40ab-9d67-c68b5b7b8c8c-m0.avro  (7.1 KB)\n",
      "    ├── 563b1b3d-0525-421b-b275-209feea0cc1e-m0.avro  (7.1 KB)\n",
      "    ├── 74565c83-bb86-4fc1-bf5a-db8c7d44596f-m0.avro  (7.1 KB)\n",
      "    ├── cb23e275-cffb-4084-a421-46796f13448f-m0.avro  (7.1 KB)\n",
      "    ├── snap-4404655999747093507-1-24cf3d69-37c1-498f-bfd9-47776769a4d1.avro  (4.4 KB)\n",
      "    ├── snap-5740802469932150748-1-563b1b3d-0525-421b-b275-209feea0cc1e.avro  (4.3 KB)\n",
      "    ├── snap-7464881929905122237-1-cb23e275-cffb-4084-a421-46796f13448f.avro  (4.3 KB)\n",
      "    ├── snap-7827338589424491138-1-74565c83-bb86-4fc1-bf5a-db8c7d44596f.avro  (4.4 KB)\n",
      "    ├── snap-8865779315272265733-1-40874836-6de6-40ab-9d67-c68b5b7b8c8c.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v2.metadata.json  (2.5 KB)\n",
      "    ├── v3.metadata.json  (3.5 KB)\n",
      "    ├── v4.metadata.json  (4.5 KB)\n",
      "    ├── v5.metadata.json  (5.5 KB)\n",
      "    ├── v6.metadata.json  (6.4 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "스냅샷 히스토리:\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "|snapshot_id        |committed_at           |operation|added_files|added_records|\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "|8865779315272265733|2026-02-16 05:43:12.011|append   |1          |100          |\n",
      "|5740802469932150748|2026-02-16 05:43:12.472|append   |1          |80           |\n",
      "|7464881929905122237|2026-02-16 05:43:12.775|append   |1          |120          |\n",
      "|7827338589424491138|2026-02-16 05:43:13.082|append   |1          |90           |\n",
      "|4404655999747093507|2026-02-16 05:43:13.425|append   |1          |110          |\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태 확인\n",
    "print(\"Phase 1 완료 — 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH, max_depth=3)\n",
    "\n",
    "print(\"\\n스냅샷 히스토리:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT snapshot_id, committed_at, operation,\n",
    "       summary['added-data-files'] as added_files,\n",
    "       summary['added-records'] as added_records\n",
    "FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f48bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1.5: 중복 데이터 유입과 정리\n",
    "\n",
    "운영에서는 동일 주문이 재전송되어 중복 키가 들어올 수 있습니다.\n",
    "업데이트 전에 중복을 탐지/정리하여 이후 단계의 정합성을 보장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf500c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 주입 후 중복 order_id 개수: 3\n",
      "중복 정리 대상 레코드 수: 500\n",
      "중복 정리 후 중복 order_id 개수: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 3건을 의도적으로 중복 유입\n",
    "dup_rows = spark.sql(f\"\"\"\n",
    "SELECT order_id, customer_id, product_name, order_date, amount, status\n",
    "FROM {TABLE_NAME}\n",
    "ORDER BY order_id\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "dup_rows.writeTo(TABLE_NAME).append()\n",
    "\n",
    "dup_keys = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    ") t\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"중복 주입 후 중복 order_id 개수: {dup_keys}\")\n",
    "\n",
    "# order_id 기준으로 1건만 남기도록 정리\n",
    "# 중요: 원본 테이블 삭제 전에 dedup 결과를 물질화해야 lazy evaluation으로 빈 테이블을 다시 읽지 않음\n",
    "w = Window.partitionBy(\"order_id\").orderBy(\n",
    "    F.col(\"order_date\").desc(), F.col(\"status\").desc()\n",
    ")\n",
    "dedup_df = (\n",
    "    spark.table(TABLE_NAME)\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    "    .cache()\n",
    ")\n",
    "dedup_rows_count = dedup_df.count()\n",
    "print(f\"중복 정리 대상 레코드 수: {dedup_rows_count}\")\n",
    "\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE true\")\n",
    "dedup_df.writeTo(TABLE_NAME).append()\n",
    "dedup_df.unpersist()\n",
    "\n",
    "dup_keys_after = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    ") t\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"중복 정리 후 중복 order_id 개수: {dup_keys_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e1311",
   "metadata": {},
   "source": [
    "\n",
    "### 관찰 포인트 — 중복 데이터 정리\n",
    "\n",
    "- 중복 유입을 조기에 정리하면 UPDATE/DELETE/집계 결과 왜곡을 예방할 수 있습니다.\n",
    "- 운영 파이프라인에는 배치 적재 직후 중복 탐지 쿼리를 자동화하는 것이 안전합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000009",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: 상태 업데이트 (COW)\n",
    "\n",
    "배송이 완료된 주문의 상태를 `shipped`로, 일부 주문을 `cancelled`로 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shipped 업데이트 대상: 32건\n",
      "shipped 상태 업데이트 완료 (현재 shipped: 129건)\n",
      "cancelled 업데이트 대상: 11건\n",
      "cancelled 상태 업데이트 완료 (현재 cancelled: 110건)\n",
      "\n",
      "파일 변경 사항 (COW 방식):\n",
      "\n",
      "[+] 추가된 파일 (11개):\n",
      "    + data/order_date_month=2024-01/00000-942-8b5a3787-ae97-4ccd-bf39-eb574761d44e-00001.parquet  (3.6 KB)\n",
      "    + data/order_date_month=2024-03/00000-947-9a74f6d5-9fd5-4db5-bfa3-71278e534f24-00001.parquet  (3.0 KB)\n",
      "    + metadata/c2423331-d5a5-46f7-bd8e-45ff738f47bb-m0.avro  (7.1 KB)\n",
      "    + metadata/c2423331-d5a5-46f7-bd8e-45ff738f47bb-m1.avro  (7.1 KB)\n",
      "    + metadata/c2423331-d5a5-46f7-bd8e-45ff738f47bb-m2.avro  (7.1 KB)\n",
      "    + metadata/d8b3e567-f5d5-454e-a922-427c48a2fd27-m0.avro  (7.1 KB)\n",
      "    + metadata/d8b3e567-f5d5-454e-a922-427c48a2fd27-m1.avro  (7.1 KB)\n",
      "    + metadata/snap-1851445945964956442-1-c2423331-d5a5-46f7-bd8e-45ff738f47bb.avro  (4.4 KB)\n",
      "    + metadata/snap-339597220726284806-1-d8b3e567-f5d5-454e-a922-427c48a2fd27.avro  (4.4 KB)\n",
      "    + metadata/v7.metadata.json  (7.5 KB)\n",
      "    + metadata/v8.metadata.json  (8.6 KB)\n",
      "\n",
      "요약: +11 추가, -0 삭제, ~0 변경\n"
     ]
    }
   ],
   "source": [
    "total_before_phase2 = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME}\"\n",
    ").collect()[0][0]\n",
    "if total_before_phase2 == 0:\n",
    "    raise RuntimeError(\n",
    "        \"테이블이 비어 있습니다. Phase 1의 일별 배치 적재 셀(Cell 6)을 먼저 실행하세요.\"\n",
    "    )\n",
    "\n",
    "before = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# pending → shipped\n",
    "pending_target = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM {TABLE_NAME}\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"shipped 업데이트 대상: {pending_target}건\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'shipped'\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\")\n",
    "shipped = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'shipped'\"\n",
    ").collect()[0][0]\n",
    "print(f\"shipped 상태 업데이트 완료 (현재 shipped: {shipped}건)\")\n",
    "\n",
    "# 일부 cancelled\n",
    "cancel_target = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM {TABLE_NAME}\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"cancelled 업데이트 대상: {cancel_target}건\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'cancelled'\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\")\n",
    "cancelled = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'cancelled'\"\n",
    ").collect()[0][0]\n",
    "print(f\"cancelled 상태 업데이트 완료 (현재 cancelled: {cancelled}건)\")\n",
    "\n",
    "after = snapshot_tree(TABLE_PATH)\n",
    "print(\"\\n파일 변경 사항 (COW 방식):\")\n",
    "diff_tree(before, after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주문 상태 분포:\n",
      "+----------+-----+\n",
      "|    status|count|\n",
      "+----------+-----+\n",
      "|   shipped|  127|\n",
      "|processing|  122|\n",
      "| cancelled|  110|\n",
      "| completed|   90|\n",
      "|   pending|   51|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 상태별 분포 확인\n",
    "total_for_distribution = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME}\"\n",
    ").collect()[0][0]\n",
    "\n",
    "print(\"주문 상태 분포:\")\n",
    "if total_for_distribution == 0:\n",
    "    print(\"현재 테이블에 데이터가 없습니다. Phase 1(셀 6)과 Phase 2(셀 9)를 순서대로 실행하세요.\")\n",
    "else:\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT status, COUNT(*) as count\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY status\n",
    "    ORDER BY count DESC\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000012",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: 스키마 변경 (Schema Evolution)\n",
    "\n",
    "비즈니스 요구사항 변경: `region` 컬럼을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1000013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region 컬럼 추가 완료\n",
      "+--------------+------------------+-------+\n",
      "|      col_name|         data_type|comment|\n",
      "+--------------+------------------+-------+\n",
      "|      order_id|            bigint|   null|\n",
      "|   customer_id|            bigint|   null|\n",
      "|  product_name|            string|   null|\n",
      "|    order_date|              date|   null|\n",
      "|        amount|     decimal(10,2)|   null|\n",
      "|        status|            string|   null|\n",
      "|        region|            string|   null|\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|        Part 0|months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# region 컬럼 추가\n",
    "spark.sql(f\"ALTER TABLE {TABLE_NAME} ADD COLUMN region STRING\")\n",
    "print(\"region 컬럼 추가 완료\")\n",
    "\n",
    "# 스키마 확인\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 데이터 region 업데이트 완료\n",
      "+------+-----+\n",
      "|region|count|\n",
      "+------+-----+\n",
      "| Seoul|  146|\n",
      "|  Jeju|  193|\n",
      "| Busan|  161|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 기존 데이터에 region 값 업데이트\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET region = CASE\n",
    "    WHEN customer_id % 3 = 0 THEN 'Seoul'\n",
    "    WHEN customer_id % 3 = 1 THEN 'Busan'\n",
    "    ELSE 'Jeju'\n",
    "END\n",
    "\"\"\")\n",
    "\n",
    "print(\"기존 데이터 region 업데이트 완료\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새 데이터 100건 추가 (region 포함). 총 레코드: 600건\n"
     ]
    }
   ],
   "source": [
    "# 새 데이터는 region 포함하여 삽입\n",
    "from pyspark.sql.functions import lit, when, col\n",
    "\n",
    "orders_new = generate_orders(\n",
    "    num_records=100,\n",
    "    seed=99,\n",
    "    start_date=\"2024-03-15\",\n",
    "    end_date=\"2024-03-31\",\n",
    "    id_offset=501,\n",
    ")\n",
    "df_new = to_spark_df(spark, orders_new)\n",
    "df_new = df_new.withColumn(\n",
    "    \"region\",\n",
    "    when(col(\"customer_id\") % 3 == 0, lit(\"Seoul\"))\n",
    "    .when(col(\"customer_id\") % 3 == 1, lit(\"Busan\"))\n",
    "    .otherwise(lit(\"Jeju\")),\n",
    ")\n",
    "df_new.writeTo(TABLE_NAME).append()\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"새 데이터 100건 추가 (region 포함). 총 레코드: {total}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000016",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: 컴팩션\n",
    "\n",
    "여러 번의 INSERT/UPDATE로 파일이 많아졌습니다. Sort 전략으로 컴팩션합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1000017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컴팩션 전: 파일 11개, 크기 267,258 bytes\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|0                         |0                     |0                    |0                      |\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n",
      "컴팩션 후: 파일 11개, 크기 267,258 bytes\n",
      "파일 수 변화: 11 → 11\n"
     ]
    }
   ],
   "source": [
    "files_before = count_files(TABLE_PATH)\n",
    "size_before = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 전: 파일 {files_before}개, 크기 {size_before:,} bytes\")\n",
    "\n",
    "# Sort 컴팩션 (order_date 기준)\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_data_files(\n",
    "    table => '{TABLE_NAME}',\n",
    "    strategy => 'sort',\n",
    "    sort_order => 'order_date ASC NULLS LAST'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "files_after = count_files(TABLE_PATH)\n",
    "size_after = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 후: 파일 {files_after}개, 크기 {size_after:,} bytes\")\n",
    "print(f\"파일 수 변화: {files_before} → {files_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000018",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: 스냅샷 정리 (유지보수)\n",
    "\n",
    "지금까지 생성된 스냅샷 중 최근 2개만 남기고 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1000019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 스냅샷 수: 9\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|deleted_data_files_count|deleted_position_delete_files_count|deleted_equality_delete_files_count|deleted_manifest_files_count|deleted_manifest_lists_count|deleted_statistics_files_count|\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|0                       |0                                  |0                                  |0                           |0                           |0                             |\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "\n",
      "정리 후 스냅샷 수: 9\n"
     ]
    }
   ],
   "source": [
    "# 현재 스냅샷 수 확인\n",
    "snapshots = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"현재 스냅샷 수: {len(snapshots)}\")\n",
    "\n",
    "# 최근 2개만 유지\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.expire_snapshots(\n",
    "    table => '{TABLE_NAME}',\n",
    "    retain_last => 2\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "snapshots_after = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"정리 후 스냅샷 수: {len(snapshots_after)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1000020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------+\n",
      "|rewritten_manifests_count|added_manifests_count|\n",
      "+-------------------------+---------------------+\n",
      "|2                        |1                    |\n",
      "+-------------------------+---------------------+\n",
      "\n",
      "유지보수 완료: Expire Snapshots + Rewrite Manifests\n"
     ]
    }
   ],
   "source": [
    "# Manifest 재작성\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_manifests(\n",
    "    table => '{TABLE_NAME}'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"유지보수 완료: Expire Snapshots + Rewrite Manifests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1000021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 5 완료 — 유지보수 후 테이블 상태:\n",
      "============================================================\n",
      "레코드 수: 600\n",
      "파일 수: 11\n",
      "스냅샷 수: 10\n",
      "\n",
      "파일 구조:\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-909-2bae8660-2a17-42df-97a9-29af7561946b-00001.parquet  (3.0 KB)\n",
      "│   │   ├── 00000-915-b36fd36b-b991-42a3-9e99-947fdf5ed9ff-00001.parquet  (2.8 KB)\n",
      "│   │   ├── 00000-942-8b5a3787-ae97-4ccd-bf39-eb574761d44e-00001.parquet  (3.6 KB)\n",
      "│   │   └── 00000-953-e737dbd7-1dfa-4989-b349-4d949f294d5f-00001.parquet  (3.9 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-921-f1531eac-1d7e-4418-bfae-8390be0e6105-00001.parquet  (3.1 KB)\n",
      "│   │   ├── 00000-927-51b251c5-074d-45de-b859-679d8d45f3c3-00001.parquet  (2.9 KB)\n",
      "│   │   └── 00000-953-e737dbd7-1dfa-4989-b349-4d949f294d5f-00002.parquet  (4.1 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-933-a8396472-707d-4126-beb1-7ea8407d69e1-00001.parquet  (3.0 KB)\n",
      "│       ├── 00000-947-9a74f6d5-9fd5-4db5-bfa3-71278e534f24-00001.parquet  (3.0 KB)\n",
      "│       ├── 00000-953-e737dbd7-1dfa-4989-b349-4d949f294d5f-00003.parquet  (3.3 KB)\n",
      "│       └── 00000-960-16e806a5-57dd-465c-b13f-dc3d859206cf-00001.parquet  (3.2 KB)\n",
      "└── metadata/\n",
      "    ├── 04f4eb05-c01c-4e40-8e24-6f41aa6427cb-m0.avro  (7.2 KB)\n",
      "    ├── 24cf3d69-37c1-498f-bfd9-47776769a4d1-m0.avro  (7.1 KB)\n",
      "    ├── 40874836-6de6-40ab-9d67-c68b5b7b8c8c-m0.avro  (7.1 KB)\n",
      "    ├── 563b1b3d-0525-421b-b275-209feea0cc1e-m0.avro  (7.1 KB)\n",
      "    ├── 74565c83-bb86-4fc1-bf5a-db8c7d44596f-m0.avro  (7.1 KB)\n",
      "    ├── 84c399de-87e0-412b-b629-a17cce5c9111-m0.avro  (7.5 KB)\n",
      "    ├── 9942bbce-9978-4163-9c17-db6d455372f7-m0.avro  (7.2 KB)\n",
      "    ├── 9942bbce-9978-4163-9c17-db6d455372f7-m1.avro  (7.2 KB)\n",
      "    ├── 9942bbce-9978-4163-9c17-db6d455372f7-m2.avro  (7.2 KB)\n",
      "    ├── 9942bbce-9978-4163-9c17-db6d455372f7-m3.avro  (7.2 KB)\n",
      "    ├── 9942bbce-9978-4163-9c17-db6d455372f7-m4.avro  (7.4 KB)\n",
      "    ├── c2423331-d5a5-46f7-bd8e-45ff738f47bb-m0.avro  (7.1 KB)\n",
      "    ├── c2423331-d5a5-46f7-bd8e-45ff738f47bb-m1.avro  (7.1 KB)\n",
      "    ├── c2423331-d5a5-46f7-bd8e-45ff738f47bb-m2.avro  (7.1 KB)\n",
      "    ├── cb23e275-cffb-4084-a421-46796f13448f-m0.avro  (7.1 KB)\n",
      "    ├── d8b3e567-f5d5-454e-a922-427c48a2fd27-m0.avro  (7.1 KB)\n",
      "    ├── d8b3e567-f5d5-454e-a922-427c48a2fd27-m1.avro  (7.1 KB)\n",
      "    ├── snap-1495744749485585034-1-9942bbce-9978-4163-9c17-db6d455372f7.avro  (4.3 KB)\n",
      "    ├── snap-1851445945964956442-1-c2423331-d5a5-46f7-bd8e-45ff738f47bb.avro  (4.4 KB)\n",
      "    ├── snap-3105680197605447200-1-84c399de-87e0-412b-b629-a17cce5c9111.avro  (4.2 KB)\n",
      "    ├── snap-339597220726284806-1-d8b3e567-f5d5-454e-a922-427c48a2fd27.avro  (4.4 KB)\n",
      "    ├── snap-4404655999747093507-1-24cf3d69-37c1-498f-bfd9-47776769a4d1.avro  (4.4 KB)\n",
      "    ├── snap-5740802469932150748-1-563b1b3d-0525-421b-b275-209feea0cc1e.avro  (4.3 KB)\n",
      "    ├── snap-7255557774689941860-1-04f4eb05-c01c-4e40-8e24-6f41aa6427cb.avro  (4.3 KB)\n",
      "    ├── snap-7464881929905122237-1-cb23e275-cffb-4084-a421-46796f13448f.avro  (4.3 KB)\n",
      "    ├── snap-7827338589424491138-1-74565c83-bb86-4fc1-bf5a-db8c7d44596f.avro  (4.4 KB)\n",
      "    ├── snap-8865779315272265733-1-40874836-6de6-40ab-9d67-c68b5b7b8c8c.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v10.metadata.json  (10.6 KB)\n",
      "    ├── v11.metadata.json  (11.6 KB)\n",
      "    ├── v12.metadata.json  (12.5 KB)\n",
      "    ├── v2.metadata.json  (2.5 KB)\n",
      "    ├── v3.metadata.json  (3.5 KB)\n",
      "    ├── v4.metadata.json  (4.5 KB)\n",
      "    ├── v5.metadata.json  (5.5 KB)\n",
      "    ├── v6.metadata.json  (6.4 KB)\n",
      "    ├── v7.metadata.json  (7.5 KB)\n",
      "    ├── v8.metadata.json  (8.6 KB)\n",
      "    ├── v9.metadata.json  (9.5 KB)\n",
      "    └── version-hint.text  (2 B)\n"
     ]
    }
   ],
   "source": [
    "# 유지보수 후 최종 상태\n",
    "print(\"Phase 5 완료 — 유지보수 후 테이블 상태:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n파일 구조:\")\n",
    "show_tree(TABLE_PATH, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000022",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6: 장애 복구 (Time Travel)\n",
    "\n",
    "### 시나리오: 실수로 데이터 대량 삭제\n",
    "\n",
    "운영자가 실수로 잘못된 DELETE 쿼리를 실행하여 데이터가 삭제되었습니다. Time Travel로 복구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1000023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제 전 레코드 수: 600\n",
      "안전한 스냅샷 ID: 3105680197605447200\n"
     ]
    }
   ],
   "source": [
    "# 삭제 전 레코드 수 기록\n",
    "count_before_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"삭제 전 레코드 수: {count_before_accident}\")\n",
    "\n",
    "# 현재 스냅샷 ID 기록 (복구 지점)\n",
    "safe_snapshot = spark.sql(f\"\"\"\n",
    "SELECT snapshot_id FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at DESC LIMIT 1\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"안전한 스냅샷 ID: {safe_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1000024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[사고 발생] 실수로 Seoul 지역 데이터 전체 삭제!\n",
      "삭제 후 레코드 수: 419 (181건 삭제됨!)\n",
      "\n",
      "현재 region 분포:\n",
      "+------+--------+\n",
      "|region|count(1)|\n",
      "+------+--------+\n",
      "|  Jeju|     228|\n",
      "| Busan|     191|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사고 발생: 실수로 Seoul 지역 데이터 전체 삭제!\n",
    "print(\"[사고 발생] 실수로 Seoul 지역 데이터 전체 삭제!\")\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE region = 'Seoul'\")\n",
    "\n",
    "count_after_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "deleted = count_before_accident - count_after_accident\n",
    "print(f\"삭제 후 레코드 수: {count_after_accident} ({deleted}건 삭제됨!)\")\n",
    "\n",
    "print(\"\\n현재 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1000025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[복구] 스냅샷 3105680197605447200의 데이터 확인:\n",
      "스냅샷 3105680197605447200 레코드 수: 600\n",
      "\n",
      "스냅샷의 region 분포:\n",
      "+------+-----+\n",
      "|region|count|\n",
      "+------+-----+\n",
      "| Seoul|  181|\n",
      "|  Jeju|  228|\n",
      "| Busan|  191|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 복구 방법 1: Time Travel로 삭제 전 데이터 확인\n",
    "print(f\"[복구] 스냅샷 {safe_snapshot}의 데이터 확인:\")\n",
    "safe_count = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*) FROM {TABLE_NAME}\n",
    "VERSION AS OF {safe_snapshot}\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"스냅샷 {safe_snapshot} 레코드 수: {safe_count}\")\n",
    "\n",
    "print(\"\\n스냅샷의 region 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME} VERSION AS OF {safe_snapshot}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1000026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[복구 실행] Rollback to safe snapshot...\n",
      "\n",
      "복구 완료!\n",
      "레코드 수: 600 → 419 (사고) → 600 (복구)\n",
      "\n",
      "복구 후 region 분포:\n",
      "+------+--------+\n",
      "|region|count(1)|\n",
      "+------+--------+\n",
      "| Seoul|     181|\n",
      "|  Jeju|     228|\n",
      "| Busan|     191|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 복구 방법 2: Rollback으로 테이블 상태를 이전 스냅샷으로 되돌리기\n",
    "print(\"[복구 실행] Rollback to safe snapshot...\")\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rollback_to_snapshot(\n",
    "    table => '{TABLE_NAME}',\n",
    "    snapshot_id => {safe_snapshot}\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "count_after_recovery = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n복구 완료!\")\n",
    "print(\n",
    "    f\"레코드 수: {count_before_accident} → {count_after_accident} (사고) → {count_after_recovery} (복구)\"\n",
    ")\n",
    "\n",
    "print(\"\\n복구 후 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000027",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 장애 복구\n",
    "\n",
    "- `VERSION AS OF`로 삭제 전 데이터를 **먼저 확인**한 후 복구할 수 있었습니다\n",
    "- `rollback_to_snapshot`으로 테이블 상태를 **원래대로 되돌렸습니다**\n",
    "- Rollback은 데이터를 복사하지 않고 **메타데이터 포인터만 변경**하므로 즉시 완료됩니다\n",
    "- 단, `expire_snapshots`로 삭제된 스냅샷으로는 복구할 수 없으므로 **보존 기간 설정이 중요**합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000028",
   "metadata": {},
   "source": [
    "---\n",
    "## 전체 파이프라인 요약\n",
    "\n",
    "```\n",
    "Phase 1: 일별 배치 적재        → INSERT (append)\n",
    "Phase 2: 상태 업데이트          → UPDATE (COW)\n",
    "Phase 3: 스키마 변경           → ALTER TABLE ADD COLUMN\n",
    "Phase 4: 컴팩션               → rewrite_data_files (sort)\n",
    "Phase 5: 스냅샷 정리           → expire_snapshots + rewrite_manifests\n",
    "Phase 6: 장애 복구             → VERSION AS OF + rollback_to_snapshot\n",
    "```\n",
    "\n",
    "### 운영 체크리스트\n",
    "\n",
    "| 항목 | 주기 | 명령 |\n",
    "|------|------|------|\n",
    "| 데이터 적재 | 매일 | `df.writeTo(table).append()` |\n",
    "| 컴팩션 | 매일 | `rewrite_data_files(strategy => 'sort')` |\n",
    "| 스냅샷 만료 | 매일~주간 | `expire_snapshots(retain_last => N)` |\n",
    "| 고아 파일 정리 | 주간~월간 | `remove_orphan_files(dry_run => true)` |\n",
    "| Manifest 재작성 | 주간 | `rewrite_manifests()` |\n",
    "| 파티션 모니터링 | 주간 | `SELECT * FROM table.partitions` |\n",
    "| 파일 크기 모니터링 | 주간 | `SELECT * FROM table.files` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1000029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "최종 테이블 상태\n",
      "============================================================\n",
      "레코드 수: 600\n",
      "파일 수: 14\n",
      "총 크기: 350,584 bytes\n",
      "스냅샷 수: 11\n",
      "\n",
      "스키마:\n",
      "+--------------+------------------+-------+\n",
      "|      col_name|         data_type|comment|\n",
      "+--------------+------------------+-------+\n",
      "|      order_id|            bigint|   null|\n",
      "|   customer_id|            bigint|   null|\n",
      "|  product_name|            string|   null|\n",
      "|    order_date|              date|   null|\n",
      "|        amount|     decimal(10,2)|   null|\n",
      "|        status|            string|   null|\n",
      "|        region|            string|   null|\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|        Part 0|months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n",
      "파티션:\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at        |last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|{648}    |0      |180         |1         |4012                         |0                           |0                         |0                           |0                         |2026-02-16 05:43:26.674|1495744749485585034     |\n",
      "|{649}    |0      |210         |1         |4220                         |0                           |0                         |0                           |0                         |2026-02-16 05:43:26.674|1495744749485585034     |\n",
      "|{650}    |0      |210         |2         |6709                         |0                           |0                         |0                           |0                         |2026-02-16 05:43:28.552|7255557774689941860     |\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최종 테이블 상태\n",
    "print(\"=\" * 60)\n",
    "print(\"최종 테이블 상태\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n스키마:\")\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()\n",
    "\n",
    "print(\"파티션:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1000030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n",
      "\n",
      "End-to-End 시나리오 실습 완료!\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")\n",
    "print(\"\\nEnd-to-End 시나리오 실습 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
