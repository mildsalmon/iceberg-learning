{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1000001",
   "metadata": {},
   "source": [
    "# End-to-End 시나리오 — 일별 배치부터 장애 복구까지\n",
    "\n",
    "이 노트북에서는 지금까지 배운 모든 내용을 **실전 시나리오**로 종합 실습합니다.\n",
    "\n",
    "### 시나리오\n",
    "\n",
    "온라인 쇼핑몰의 주문 데이터 파이프라인:\n",
    "\n",
    "1. **일별 배치 적재** — 매일 새로운 주문 데이터 INSERT\n",
    "2. **상태 업데이트** — 배송 완료된 주문 UPDATE\n",
    "3. **스키마 변경** — 새 컬럼 추가 (Schema Evolution)\n",
    "4. **컴팩션** — Small File Problem 해결\n",
    "5. **스냅샷 정리** — 유지보수 작업\n",
    "6. **장애 복구** — 실수로 데이터 삭제 후 Time Travel로 복구"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000002",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import time\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import (\n",
    "    show_tree,\n",
    "    snapshot_tree,\n",
    "    diff_tree,\n",
    "    count_files,\n",
    "    total_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "TABLE_NAME = \"demo.lab.e2e_orders\"\n",
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/e2e_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000005",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1: 테이블 생성 + 일별 배치 적재\n",
    "\n",
    "월별 Hidden Partitioning으로 테이블을 생성하고, 5일치 배치 데이터를 순차적으로 적재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테이블 생성 완료: demo.lab.e2e_orders\n",
      "파티셔닝: months(order_date)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(f\"테이블 생성 완료: {TABLE_NAME}\")\n",
    "print(\"파티셔닝: months(order_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1: 100건 적재 (2024-01-01 ~ 2024-01-31)\n",
      "Day 2: 80건 적재 (2024-01-01 ~ 2024-01-31)\n",
      "Day 3: 120건 적재 (2024-02-01 ~ 2024-02-28)\n",
      "Day 4: 90건 적재 (2024-02-01 ~ 2024-02-28)\n",
      "Day 5: 110건 적재 (2024-03-01 ~ 2024-03-31)\n",
      "\n",
      "총 레코드: 500건\n",
      "파일 수: 5\n"
     ]
    }
   ],
   "source": [
    "# Day 1~5: 일별 배치 적재 시뮬레이션\n",
    "daily_batches = [\n",
    "    {\"day\": 1, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 100},\n",
    "    {\"day\": 2, \"start\": \"2024-01-01\", \"end\": \"2024-01-31\", \"records\": 80},\n",
    "    {\"day\": 3, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 120},\n",
    "    {\"day\": 4, \"start\": \"2024-02-01\", \"end\": \"2024-02-28\", \"records\": 90},\n",
    "    {\"day\": 5, \"start\": \"2024-03-01\", \"end\": \"2024-03-31\", \"records\": 110},\n",
    "]\n",
    "\n",
    "offset = 1\n",
    "for batch in daily_batches:\n",
    "    orders = generate_orders(\n",
    "        num_records=batch[\"records\"],\n",
    "        seed=batch[\"day\"],\n",
    "        start_date=batch[\"start\"],\n",
    "        end_date=batch[\"end\"],\n",
    "        id_offset=offset,\n",
    "    )\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(TABLE_NAME).append()\n",
    "    offset += batch[\"records\"]\n",
    "    print(\n",
    "        f\"Day {batch['day']}: {batch['records']}건 적재 ({batch['start']} ~ {batch['end']})\"\n",
    "    )\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n총 레코드: {total}건\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 완료 — 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-11-3ef72516-6977-421d-b2f3-e630e8b1ff37-00001.parquet  (2.8 KB)\n",
      "│   │   └── 00000-5-4d4f7563-bede-4428-9b42-883119f3c7d0-00001.parquet  (3.0 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-17-59c27e89-9dc8-46e5-a826-27bf631bdb5b-00001.parquet  (3.1 KB)\n",
      "│   │   └── 00000-23-f48c03eb-5201-4e42-9fbb-9243ea676ad1-00001.parquet  (2.9 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       └── 00000-29-a656bcb3-018f-4399-9212-e7774187cc7c-00001.parquet  (3.0 KB)\n",
      "└── metadata/\n",
      "    ├── 206e6691-df4b-480d-997f-9193b4cc7e3d-m0.avro  (7.1 KB)\n",
      "    ├── 4049c62d-e1e1-4688-85f0-a90fd4758361-m0.avro  (7.1 KB)\n",
      "    ├── 5e463f73-0362-48bf-97a5-a8b6af1b4635-m0.avro  (7.1 KB)\n",
      "    ├── 857a3fa1-7e8b-4ed1-90f6-c44e8a68762b-m0.avro  (7.1 KB)\n",
      "    ├── f956ba5b-1ced-4538-8052-d81b67ff273d-m0.avro  (7.1 KB)\n",
      "    ├── snap-14983475507778106-1-206e6691-df4b-480d-997f-9193b4cc7e3d.avro  (4.2 KB)\n",
      "    ├── snap-547469803087228542-1-857a3fa1-7e8b-4ed1-90f6-c44e8a68762b.avro  (4.4 KB)\n",
      "    ├── snap-5773236967909098679-1-f956ba5b-1ced-4538-8052-d81b67ff273d.avro  (4.2 KB)\n",
      "    ├── snap-8039030728845755918-1-4049c62d-e1e1-4688-85f0-a90fd4758361.avro  (4.3 KB)\n",
      "    ├── snap-951120180675203487-1-5e463f73-0362-48bf-97a5-a8b6af1b4635.avro  (4.4 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v2.metadata.json  (2.5 KB)\n",
      "    ├── v3.metadata.json  (3.5 KB)\n",
      "    ├── v4.metadata.json  (4.5 KB)\n",
      "    ├── v5.metadata.json  (5.5 KB)\n",
      "    ├── v6.metadata.json  (6.4 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "스냅샷 히스토리:\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "|snapshot_id        |committed_at           |operation|added_files|added_records|\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "|5773236967909098679|2026-02-16 05:32:53.257|append   |1          |100          |\n",
      "|14983475507778106  |2026-02-16 05:32:53.995|append   |1          |80           |\n",
      "|8039030728845755918|2026-02-16 05:32:54.659|append   |1          |120          |\n",
      "|547469803087228542 |2026-02-16 05:32:55.157|append   |1          |90           |\n",
      "|951120180675203487 |2026-02-16 05:32:55.587|append   |1          |110          |\n",
      "+-------------------+-----------------------+---------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태 확인\n",
    "print(\"Phase 1 완료 — 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH, max_depth=3)\n",
    "\n",
    "print(\"\\n스냅샷 히스토리:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT snapshot_id, committed_at, operation,\n",
    "       summary['added-data-files'] as added_files,\n",
    "       summary['added-records'] as added_records\n",
    "FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f48bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 1.5: 중복 데이터 유입과 정리\n",
    "\n",
    "운영에서는 동일 주문이 재전송되어 중복 키가 들어올 수 있습니다.\n",
    "업데이트 전에 중복을 탐지/정리하여 이후 단계의 정합성을 보장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf500c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 주입 후 중복 order_id 개수: 3\n",
      "중복 정리 대상 레코드 수: 500\n",
      "중복 정리 후 중복 order_id 개수: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 3건을 의도적으로 중복 유입\n",
    "dup_rows = spark.sql(f\"\"\"\n",
    "SELECT order_id, customer_id, product_name, order_date, amount, status\n",
    "FROM {TABLE_NAME}\n",
    "ORDER BY order_id\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "dup_rows.writeTo(TABLE_NAME).append()\n",
    "\n",
    "dup_keys = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    ") t\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"중복 주입 후 중복 order_id 개수: {dup_keys}\")\n",
    "\n",
    "# order_id 기준으로 1건만 남기도록 정리\n",
    "# 중요: 원본 테이블 삭제 전에 dedup 결과를 물질화해야 lazy evaluation으로 빈 테이블을 다시 읽지 않음\n",
    "w = Window.partitionBy(\"order_id\").orderBy(\n",
    "    F.col(\"order_date\").desc(), F.col(\"status\").desc()\n",
    ")\n",
    "dedup_df = (\n",
    "    spark.table(TABLE_NAME)\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    "    .cache()\n",
    ")\n",
    "dedup_rows_count = dedup_df.count()\n",
    "print(f\"중복 정리 대상 레코드 수: {dedup_rows_count}\")\n",
    "\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE true\")\n",
    "dedup_df.writeTo(TABLE_NAME).append()\n",
    "dedup_df.unpersist()\n",
    "\n",
    "dup_keys_after = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM (\n",
    "    SELECT order_id\n",
    "    FROM {TABLE_NAME}\n",
    "    GROUP BY order_id\n",
    "    HAVING COUNT(*) > 1\n",
    ") t\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"중복 정리 후 중복 order_id 개수: {dup_keys_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e1311",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 중복 데이터 정리\n",
    "\n",
    "- 중복 유입을 조기에 정리하면 UPDATE/DELETE/집계 결과 왜곡을 예방할 수 있습니다.\n",
    "- 운영 파이프라인에는 배치 적재 직후 중복 탐지 쿼리를 자동화하는 것이 안전합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000009",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: 상태 업데이트 (COW)\n",
    "\n",
    "배송이 완료된 주문의 상태를 `shipped`로, 일부 주문을 `cancelled`로 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shipped 업데이트 대상: 0건\n",
      "shipped 상태 업데이트 완료 (현재 shipped: 0건)\n",
      "cancelled 업데이트 대상: 0건\n",
      "cancelled 상태 업데이트 완료 (현재 cancelled: 0건)\n",
      "\n",
      "파일 변경 사항 (COW 방식):\n",
      "\n",
      "[+] 추가된 파일 (4개):\n",
      "    + metadata/snap-4426612073051357008-1-afb41d3f-f750-4b74-a32f-8612bc68040a.avro  (4.0 KB)\n",
      "    + metadata/snap-7486135810243869740-1-c4ec5833-5760-43f5-9f94-f3932bc30e2e.avro  (4.0 KB)\n",
      "    + metadata/v10.metadata.json  (10.1 KB)\n",
      "    + metadata/v11.metadata.json  (11.0 KB)\n",
      "\n",
      "[~] 크기 변경된 파일 (1개):\n",
      "    ~ metadata/version-hint.text  (1 B -> 2 B)\n",
      "\n",
      "요약: +4 추가, -0 삭제, ~1 변경\n"
     ]
    }
   ],
   "source": [
    "before = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# pending → shipped\n",
    "pending_target = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM {TABLE_NAME}\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"shipped 업데이트 대상: {pending_target}건\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'shipped'\n",
    "WHERE status = 'pending' AND order_date < '2024-02-01'\n",
    "\"\"\")\n",
    "shipped = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'shipped'\"\n",
    ").collect()[0][0]\n",
    "print(f\"shipped 상태 업데이트 완료 (현재 shipped: {shipped}건)\")\n",
    "\n",
    "# 일부 cancelled\n",
    "cancel_target = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM {TABLE_NAME}\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"cancelled 업데이트 대상: {cancel_target}건\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET status = 'cancelled'\n",
    "WHERE amount < 150 AND order_date >= '2024-03-01'\n",
    "\"\"\")\n",
    "cancelled = spark.sql(\n",
    "    f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'cancelled'\"\n",
    ").collect()[0][0]\n",
    "print(f\"cancelled 상태 업데이트 완료 (현재 cancelled: {cancelled}건)\")\n",
    "\n",
    "after = snapshot_tree(TABLE_PATH)\n",
    "print(\"\\n파일 변경 사항 (COW 방식):\")\n",
    "diff_tree(before, after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주문 상태 분포:\n",
      "+------+-----+\n",
      "|status|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 상태별 분포 확인\n",
    "print(\"주문 상태 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT status, COUNT(*) as count\n",
    "FROM {TABLE_NAME}\n",
    "GROUP BY status\n",
    "ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000012",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: 스키마 변경 (Schema Evolution)\n",
    "\n",
    "비즈니스 요구사항 변경: `region` 컬럼을 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1000013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region 컬럼 추가 완료\n",
      "+--------------+------------------+-------+\n",
      "|      col_name|         data_type|comment|\n",
      "+--------------+------------------+-------+\n",
      "|      order_id|            bigint|   null|\n",
      "|   customer_id|            bigint|   null|\n",
      "|  product_name|            string|   null|\n",
      "|    order_date|              date|   null|\n",
      "|        amount|     decimal(10,2)|   null|\n",
      "|        status|            string|   null|\n",
      "|        region|            string|   null|\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|        Part 0|months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# region 컬럼 추가\n",
    "spark.sql(f\"ALTER TABLE {TABLE_NAME} ADD COLUMN region STRING\")\n",
    "print(\"region 컬럼 추가 완료\")\n",
    "\n",
    "# 스키마 확인\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 데이터 region 업데이트 완료\n",
      "+------+-----+\n",
      "|region|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 기존 데이터에 region 값 업데이트\n",
    "spark.sql(f\"\"\"\n",
    "UPDATE {TABLE_NAME}\n",
    "SET region = CASE\n",
    "    WHEN customer_id % 3 = 0 THEN 'Seoul'\n",
    "    WHEN customer_id % 3 = 1 THEN 'Busan'\n",
    "    ELSE 'Jeju'\n",
    "END\n",
    "\"\"\")\n",
    "\n",
    "print(\"기존 데이터 region 업데이트 완료\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새 데이터 100건 추가 (region 포함). 총 레코드: 100건\n"
     ]
    }
   ],
   "source": [
    "# 새 데이터는 region 포함하여 삽입\n",
    "from pyspark.sql.functions import lit, when, col\n",
    "\n",
    "orders_new = generate_orders(\n",
    "    num_records=100,\n",
    "    seed=99,\n",
    "    start_date=\"2024-03-15\",\n",
    "    end_date=\"2024-03-31\",\n",
    "    id_offset=501,\n",
    ")\n",
    "df_new = to_spark_df(spark, orders_new)\n",
    "df_new = df_new.withColumn(\n",
    "    \"region\",\n",
    "    when(col(\"customer_id\") % 3 == 0, lit(\"Seoul\"))\n",
    "    .when(col(\"customer_id\") % 3 == 1, lit(\"Busan\"))\n",
    "    .otherwise(lit(\"Jeju\")),\n",
    ")\n",
    "df_new.writeTo(TABLE_NAME).append()\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"새 데이터 100건 추가 (region 포함). 총 레코드: {total}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000016",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 4: 컴팩션\n",
    "\n",
    "여러 번의 INSERT/UPDATE로 파일이 많아졌습니다. Sort 전략으로 컴팩션합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1000017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컴팩션 전: 파일 7개, 크기 278,183 bytes\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|0                         |0                     |0                    |0                      |\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n",
      "컴팩션 후: 파일 7개, 크기 278,183 bytes\n",
      "파일 수 변화: 7 → 7\n"
     ]
    }
   ],
   "source": [
    "files_before = count_files(TABLE_PATH)\n",
    "size_before = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 전: 파일 {files_before}개, 크기 {size_before:,} bytes\")\n",
    "\n",
    "# Sort 컴팩션 (order_date 기준)\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_data_files(\n",
    "    table => '{TABLE_NAME}',\n",
    "    strategy => 'sort',\n",
    "    sort_order => 'order_date ASC NULLS LAST'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "files_after = count_files(TABLE_PATH)\n",
    "size_after = total_size(TABLE_PATH)\n",
    "print(f\"컴팩션 후: 파일 {files_after}개, 크기 {size_after:,} bytes\")\n",
    "print(f\"파일 수 변화: {files_before} → {files_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000018",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 5: 스냅샷 정리 (유지보수)\n",
    "\n",
    "지금까지 생성된 스냅샷 중 최근 2개만 남기고 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1000019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 스냅샷 수: 12\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|deleted_data_files_count|deleted_position_delete_files_count|deleted_equality_delete_files_count|deleted_manifest_files_count|deleted_manifest_lists_count|deleted_statistics_files_count|\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|0                       |0                                  |0                                  |0                           |0                           |0                             |\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "\n",
      "정리 후 스냅샷 수: 12\n"
     ]
    }
   ],
   "source": [
    "# 현재 스냅샷 수 확인\n",
    "snapshots = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"현재 스냅샷 수: {len(snapshots)}\")\n",
    "\n",
    "# 최근 2개만 유지\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.expire_snapshots(\n",
    "    table => '{TABLE_NAME}',\n",
    "    retain_last => 2\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "snapshots_after = spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").collect()\n",
    "print(f\"정리 후 스냅샷 수: {len(snapshots_after)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1000020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------+\n",
      "|rewritten_manifests_count|added_manifests_count|\n",
      "+-------------------------+---------------------+\n",
      "|0                        |0                    |\n",
      "+-------------------------+---------------------+\n",
      "\n",
      "유지보수 완료: Expire Snapshots + Rewrite Manifests\n"
     ]
    }
   ],
   "source": [
    "# Manifest 재작성\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_manifests(\n",
    "    table => '{TABLE_NAME}'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"유지보수 완료: Expire Snapshots + Rewrite Manifests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1000021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 5 완료 — 유지보수 후 테이블 상태:\n",
      "============================================================\n",
      "레코드 수: 100\n",
      "파일 수: 7\n",
      "스냅샷 수: 12\n",
      "\n",
      "파일 구조:\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-11-3ef72516-6977-421d-b2f3-e630e8b1ff37-00001.parquet  (2.8 KB)\n",
      "│   │   ├── 00000-36-0bb3e0c0-36c5-45f2-9723-551e1a663a23-00001.parquet  (1.8 KB)\n",
      "│   │   └── 00000-5-4d4f7563-bede-4428-9b42-883119f3c7d0-00001.parquet  (3.0 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-17-59c27e89-9dc8-46e5-a826-27bf631bdb5b-00001.parquet  (3.1 KB)\n",
      "│   │   └── 00000-23-f48c03eb-5201-4e42-9fbb-9243ea676ad1-00001.parquet  (2.9 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-29-a656bcb3-018f-4399-9212-e7774187cc7c-00001.parquet  (3.0 KB)\n",
      "│       └── 00000-458-64e040a2-cba6-4fdc-a84b-bce29e6dd88b-00001.parquet  (3.2 KB)\n",
      "└── metadata/\n",
      "    ├── 02697e2a-ec64-4965-9e4e-a7d019731979-m0.avro  (7.1 KB)\n",
      "    ├── 206e6691-df4b-480d-997f-9193b4cc7e3d-m0.avro  (7.1 KB)\n",
      "    ├── 4049c62d-e1e1-4688-85f0-a90fd4758361-m0.avro  (7.1 KB)\n",
      "    ├── 5e463f73-0362-48bf-97a5-a8b6af1b4635-m0.avro  (7.1 KB)\n",
      "    ├── 857a3fa1-7e8b-4ed1-90f6-c44e8a68762b-m0.avro  (7.1 KB)\n",
      "    ├── 98c6c4ff-d856-4dd5-927f-baa8ab94871a-m0.avro  (7.1 KB)\n",
      "    ├── 98c6c4ff-d856-4dd5-927f-baa8ab94871a-m1.avro  (7.1 KB)\n",
      "    ├── 98c6c4ff-d856-4dd5-927f-baa8ab94871a-m2.avro  (7.1 KB)\n",
      "    ├── 98c6c4ff-d856-4dd5-927f-baa8ab94871a-m3.avro  (7.1 KB)\n",
      "    ├── 98c6c4ff-d856-4dd5-927f-baa8ab94871a-m4.avro  (7.1 KB)\n",
      "    ├── 98c6c4ff-d856-4dd5-927f-baa8ab94871a-m5.avro  (7.1 KB)\n",
      "    ├── c86b2f4c-e01f-4a0a-8faa-720e3779cad3-m0.avro  (7.2 KB)\n",
      "    ├── f956ba5b-1ced-4538-8052-d81b67ff273d-m0.avro  (7.1 KB)\n",
      "    ├── snap-14983475507778106-1-206e6691-df4b-480d-997f-9193b4cc7e3d.avro  (4.2 KB)\n",
      "    ├── snap-2101146279281052299-1-356a1a85-8803-4a1a-b8ab-1a36f6aa804b.avro  (4.0 KB)\n",
      "    ├── snap-2453853547296178657-1-c86b2f4c-e01f-4a0a-8faa-720e3779cad3.avro  (4.2 KB)\n",
      "    ├── snap-4067430786983894962-1-02697e2a-ec64-4965-9e4e-a7d019731979.avro  (4.5 KB)\n",
      "    ├── snap-4426612073051357008-1-afb41d3f-f750-4b74-a32f-8612bc68040a.avro  (4.0 KB)\n",
      "    ├── snap-4965640619568745628-1-1f564e11-274f-4561-8dff-abc029a5c605.avro  (4.0 KB)\n",
      "    ├── snap-547469803087228542-1-857a3fa1-7e8b-4ed1-90f6-c44e8a68762b.avro  (4.4 KB)\n",
      "    ├── snap-5773236967909098679-1-f956ba5b-1ced-4538-8052-d81b67ff273d.avro  (4.2 KB)\n",
      "    ├── snap-7486135810243869740-1-c4ec5833-5760-43f5-9f94-f3932bc30e2e.avro  (4.0 KB)\n",
      "    ├── snap-8039030728845755918-1-4049c62d-e1e1-4688-85f0-a90fd4758361.avro  (4.3 KB)\n",
      "    ├── snap-9220464416887492269-1-98c6c4ff-d856-4dd5-927f-baa8ab94871a.avro  (4.3 KB)\n",
      "    ├── snap-951120180675203487-1-5e463f73-0362-48bf-97a5-a8b6af1b4635.avro  (4.4 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v10.metadata.json  (10.1 KB)\n",
      "    ├── v11.metadata.json  (11.0 KB)\n",
      "    ├── v12.metadata.json  (11.9 KB)\n",
      "    ├── v13.metadata.json  (12.8 KB)\n",
      "    ├── v14.metadata.json  (13.8 KB)\n",
      "    ├── v2.metadata.json  (2.5 KB)\n",
      "    ├── v3.metadata.json  (3.5 KB)\n",
      "    ├── v4.metadata.json  (4.5 KB)\n",
      "    ├── v5.metadata.json  (5.5 KB)\n",
      "    ├── v6.metadata.json  (6.4 KB)\n",
      "    ├── v7.metadata.json  (7.4 KB)\n",
      "    ├── v8.metadata.json  (8.4 KB)\n",
      "    ├── v9.metadata.json  (9.3 KB)\n",
      "    └── version-hint.text  (2 B)\n"
     ]
    }
   ],
   "source": [
    "# 유지보수 후 최종 상태\n",
    "print(\"Phase 5 완료 — 유지보수 후 테이블 상태:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n파일 구조:\")\n",
    "show_tree(TABLE_PATH, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000022",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 6: 장애 복구 (Time Travel)\n",
    "\n",
    "### 시나리오: 실수로 데이터 대량 삭제\n",
    "\n",
    "운영자가 실수로 잘못된 DELETE 쿼리를 실행하여 데이터가 삭제되었습니다. Time Travel로 복구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1000023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제 전 레코드 수: 100\n",
      "안전한 스냅샷 ID: 2453853547296178657\n"
     ]
    }
   ],
   "source": [
    "# 삭제 전 레코드 수 기록\n",
    "count_before_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"삭제 전 레코드 수: {count_before_accident}\")\n",
    "\n",
    "# 현재 스냅샷 ID 기록 (복구 지점)\n",
    "safe_snapshot = spark.sql(f\"\"\"\n",
    "SELECT snapshot_id FROM {TABLE_NAME}.snapshots\n",
    "ORDER BY committed_at DESC LIMIT 1\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"안전한 스냅샷 ID: {safe_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1000024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[사고 발생] 실수로 Seoul 지역 데이터 전체 삭제!\n",
      "삭제 후 레코드 수: 65 (35건 삭제됨!)\n",
      "\n",
      "현재 region 분포:\n",
      "+------+--------+\n",
      "|region|count(1)|\n",
      "+------+--------+\n",
      "|  Jeju|      35|\n",
      "| Busan|      30|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 사고 발생: 실수로 Seoul 지역 데이터 전체 삭제!\n",
    "print(\"[사고 발생] 실수로 Seoul 지역 데이터 전체 삭제!\")\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE region = 'Seoul'\")\n",
    "\n",
    "count_after_accident = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "deleted = count_before_accident - count_after_accident\n",
    "print(f\"삭제 후 레코드 수: {count_after_accident} ({deleted}건 삭제됨!)\")\n",
    "\n",
    "print(\"\\n현재 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1000025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[복구] 스냅샷 2453853547296178657의 데이터 확인:\n",
      "스냅샷 2453853547296178657 레코드 수: 100\n",
      "\n",
      "스냅샷의 region 분포:\n",
      "+------+-----+\n",
      "|region|count|\n",
      "+------+-----+\n",
      "|  Jeju|   35|\n",
      "| Seoul|   35|\n",
      "| Busan|   30|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 복구 방법 1: Time Travel로 삭제 전 데이터 확인\n",
    "print(f\"[복구] 스냅샷 {safe_snapshot}의 데이터 확인:\")\n",
    "safe_count = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*) FROM {TABLE_NAME}\n",
    "VERSION AS OF {safe_snapshot}\n",
    "\"\"\").collect()[0][0]\n",
    "print(f\"스냅샷 {safe_snapshot} 레코드 수: {safe_count}\")\n",
    "\n",
    "print(\"\\n스냅샷의 region 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT region, COUNT(*) as count\n",
    "FROM {TABLE_NAME} VERSION AS OF {safe_snapshot}\n",
    "GROUP BY region\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1000026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[복구 실행] Rollback to safe snapshot...\n",
      "\n",
      "복구 완료!\n",
      "레코드 수: 100 → 65 (사고) → 100 (복구)\n",
      "\n",
      "복구 후 region 분포:\n",
      "+------+--------+\n",
      "|region|count(1)|\n",
      "+------+--------+\n",
      "|  Jeju|      35|\n",
      "| Seoul|      35|\n",
      "| Busan|      30|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 복구 방법 2: Rollback으로 테이블 상태를 이전 스냅샷으로 되돌리기\n",
    "print(\"[복구 실행] Rollback to safe snapshot...\")\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rollback_to_snapshot(\n",
    "    table => '{TABLE_NAME}',\n",
    "    snapshot_id => {safe_snapshot}\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "count_after_recovery = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME}\").collect()[0][0]\n",
    "print(f\"\\n복구 완료!\")\n",
    "print(\n",
    "    f\"레코드 수: {count_before_accident} → {count_after_accident} (사고) → {count_after_recovery} (복구)\"\n",
    ")\n",
    "\n",
    "print(\"\\n복구 후 region 분포:\")\n",
    "spark.sql(f\"SELECT region, COUNT(*) FROM {TABLE_NAME} GROUP BY region\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000027",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 장애 복구\n",
    "\n",
    "- `VERSION AS OF`로 삭제 전 데이터를 **먼저 확인**한 후 복구할 수 있었습니다\n",
    "- `rollback_to_snapshot`으로 테이블 상태를 **원래대로 되돌렸습니다**\n",
    "- Rollback은 데이터를 복사하지 않고 **메타데이터 포인터만 변경**하므로 즉시 완료됩니다\n",
    "- 단, `expire_snapshots`로 삭제된 스냅샷으로는 복구할 수 없으므로 **보존 기간 설정이 중요**합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1000028",
   "metadata": {},
   "source": [
    "---\n",
    "## 전체 파이프라인 요약\n",
    "\n",
    "```\n",
    "Phase 1: 일별 배치 적재        → INSERT (append)\n",
    "Phase 2: 상태 업데이트          → UPDATE (COW)\n",
    "Phase 3: 스키마 변경           → ALTER TABLE ADD COLUMN\n",
    "Phase 4: 컴팩션               → rewrite_data_files (sort)\n",
    "Phase 5: 스냅샷 정리           → expire_snapshots + rewrite_manifests\n",
    "Phase 6: 장애 복구             → VERSION AS OF + rollback_to_snapshot\n",
    "```\n",
    "\n",
    "### 운영 체크리스트\n",
    "\n",
    "| 항목 | 주기 | 명령 |\n",
    "|------|------|------|\n",
    "| 데이터 적재 | 매일 | `df.writeTo(table).append()` |\n",
    "| 컴팩션 | 매일 | `rewrite_data_files(strategy => 'sort')` |\n",
    "| 스냅샷 만료 | 매일~주간 | `expire_snapshots(retain_last => N)` |\n",
    "| 고아 파일 정리 | 주간~월간 | `remove_orphan_files(dry_run => true)` |\n",
    "| Manifest 재작성 | 주간 | `rewrite_manifests()` |\n",
    "| 파티션 모니터링 | 주간 | `SELECT * FROM table.partitions` |\n",
    "| 파일 크기 모니터링 | 주간 | `SELECT * FROM table.files` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1000029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "최종 테이블 상태\n",
      "============================================================\n",
      "레코드 수: 100\n",
      "파일 수: 8\n",
      "총 크기: 330,905 bytes\n",
      "스냅샷 수: 13\n",
      "\n",
      "스키마:\n",
      "+--------------+------------------+-------+\n",
      "|      col_name|         data_type|comment|\n",
      "+--------------+------------------+-------+\n",
      "|      order_id|            bigint|   null|\n",
      "|   customer_id|            bigint|   null|\n",
      "|  product_name|            string|   null|\n",
      "|    order_date|              date|   null|\n",
      "|        amount|     decimal(10,2)|   null|\n",
      "|        status|            string|   null|\n",
      "|        region|            string|   null|\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|        Part 0|months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n",
      "파티션:\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+----------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at       |last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+----------------------+------------------------+\n",
      "|{650}    |0      |100         |1         |3311                         |0                           |0                         |0                           |0                         |2026-02-16 05:33:02.77|2453853547296178657     |\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최종 테이블 상태\n",
    "print(\"=\" * 60)\n",
    "print(\"최종 테이블 상태\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(f\"파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")\n",
    "print(f\"스냅샷 수: {len(spark.sql(f'SELECT * FROM {TABLE_NAME}.snapshots').collect())}\")\n",
    "\n",
    "print(\"\\n스키마:\")\n",
    "spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()\n",
    "\n",
    "print(\"파티션:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1000030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n",
      "\n",
      "End-to-End 시나리오 실습 완료!\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")\n",
    "print(\"\\nEnd-to-End 시나리오 실습 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
