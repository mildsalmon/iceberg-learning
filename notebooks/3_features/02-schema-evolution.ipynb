{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1000001",
   "metadata": {},
   "source": [
    "# 02. Schema Evolution\n",
    "\n",
    "Iceberg의 **Schema Evolution** 기능을 통해 테이블 스키마를 안전하게 변경하고, 기존 데이터에 미치는 영향을 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000002",
   "metadata": {},
   "source": [
    "## Schema Evolution 개념\n",
    "\n",
    "전통적인 테이블 포맷에서 스키마 변경은 위험한 작업입니다.  \n",
    "컬럼 이름이나 순서에 의존하기 때문에, 변경 시 기존 데이터와 호환성 문제가 발생할 수 있습니다.\n",
    "\n",
    "### Iceberg의 접근 방식: Field ID 기반 매핑\n",
    "\n",
    "Iceberg는 **이름(name)**이 아닌 **고유 ID(field ID)**로 컬럼을 추적합니다.\n",
    "\n",
    "```\n",
    "Schema v0:                    Schema v1:\n",
    "┌─────────────────────┐      ┌─────────────────────────────┐\n",
    "│ id=1  order_id      │      │ id=1  order_id              │\n",
    "│ id=2  customer_id   │      │ id=2  customer_id           │\n",
    "│ id=3  product_name  │      │ id=3  item_name (renamed!)  │\n",
    "│ id=4  order_date    │      │ id=4  order_date            │\n",
    "│ id=5  amount        │      │ id=5  amount                │\n",
    "│ id=6  status        │      │ id=6  status                │\n",
    "└─────────────────────┘      │ id=7  discount_rate (new!)  │\n",
    "                              └─────────────────────────────┘\n",
    "```\n",
    "\n",
    "- 컬럼 이름을 변경해도 `field ID`가 같으면 동일 컬럼으로 인식\n",
    "- 새 컬럼이 추가되면 기존 데이터에서는 `NULL`로 반환\n",
    "- 스키마 변경 자체(ADD/RENAME/DROP)만으로는 기존 Parquet 파일을 다시 쓰지 않아도 file id로 맵핑 가능 (read-time projection)\n",
    "  - 단, OPTIMIZE/compaction/rewrite_data_files나 일부 UPDATE/DELETE/MERGE에서는 새 파일로 재작성될 수 있음 (in-place 수정은 없음)\n",
    "\n",
    "### 지원되는 스키마 변경\n",
    "\n",
    "| 변경 유형 | SQL | 설명 |\n",
    "|-----------|-----|------|\n",
    "| 컬럼 추가 | `ADD COLUMNS` | 새 컬럼 추가 (기존 데이터는 NULL) |\n",
    "| 컬럼 이름 변경 | `RENAME COLUMN` | field ID는 유지 |\n",
    "| 컬럼 타입 변경 | `ALTER COLUMN ... TYPE` | 호환 가능한 타입만 (int→bigint 등) |\n",
    "| 컬럼 삭제 | `DROP COLUMN` | 메타데이터에서만 제거 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000003",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session(\"SchemaEvolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000006",
   "metadata": {},
   "source": [
    "## 테이블 생성 및 초기 데이터 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테이블 생성 완료\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo.lab\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS demo.lab.schema_orders\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE demo.lab.schema_orders (\n",
    "        order_id     BIGINT,\n",
    "        customer_id  BIGINT,\n",
    "        product_name STRING,\n",
    "        order_date   DATE,\n",
    "        amount       DOUBLE,\n",
    "        status       STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(\"테이블 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 데이터 삽입: 100건\n"
     ]
    }
   ],
   "source": [
    "# 100건 삽입 (Schema v0)\n",
    "orders = generate_orders(num_records=100, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(\"demo.lab.schema_orders\").append()\n",
    "\n",
    "count = spark.sql(\"SELECT COUNT(*) AS cnt FROM demo.lab.schema_orders\").collect()[0][\"cnt\"]\n",
    "print(f\"초기 데이터 삽입: {count}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1000009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema v0 스냅샷 ID: 3746370947229820768\n",
      "\n",
      "[ 현재 스키마 ]\n",
      "+--------------+------------------+-------+\n",
      "|col_name      |data_type         |comment|\n",
      "+--------------+------------------+-------+\n",
      "|order_id      |bigint            |null   |\n",
      "|customer_id   |bigint            |null   |\n",
      "|product_name  |string            |null   |\n",
      "|order_date    |date              |null   |\n",
      "|amount        |double            |null   |\n",
      "|status        |string            |null   |\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|Part 0        |months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 초기 스냅샷 ID 저장\n",
    "snap_v0 = spark.sql(\"SELECT snapshot_id FROM demo.lab.schema_orders.snapshots ORDER BY committed_at\").collect()\n",
    "snapshot_v0_id = snap_v0[-1][\"snapshot_id\"]\n",
    "print(f\"Schema v0 스냅샷 ID: {snapshot_v0_id}\")\n",
    "\n",
    "# 현재 스키마 확인\n",
    "print(\"\\n[ 현재 스키마 ]\")\n",
    "spark.sql(\"DESCRIBE demo.lab.schema_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100000a",
   "metadata": {},
   "source": [
    "## ADD COLUMNS: 새 컬럼 추가\n",
    "\n",
    "`discount_rate` 컬럼을 추가합니다. 기존 데이터에서 이 컬럼은 `NULL`이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e100000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount_rate 컬럼 추가 완료\n",
      "\n",
      "[ 변경된 스키마 ]\n",
      "+--------------+------------------+-------+\n",
      "|col_name      |data_type         |comment|\n",
      "+--------------+------------------+-------+\n",
      "|order_id      |bigint            |null   |\n",
      "|customer_id   |bigint            |null   |\n",
      "|product_name  |string            |null   |\n",
      "|order_date    |date              |null   |\n",
      "|amount        |double            |null   |\n",
      "|status        |string            |null   |\n",
      "|discount_rate |double            |null   |\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|Part 0        |months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.lab.schema_orders\n",
    "    ADD COLUMNS (discount_rate DOUBLE)\n",
    "\"\"\")\n",
    "\n",
    "print(\"discount_rate 컬럼 추가 완료\")\n",
    "\n",
    "# 스키마 확인\n",
    "print(\"\\n[ 변경된 스키마 ]\")\n",
    "spark.sql(\"DESCRIBE demo.lab.schema_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e100000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 기존 데이터의 discount_rate 값 ]\n",
      "\n",
      "+--------+-------------+-------+-------------+\n",
      "|order_id|product_name |amount |discount_rate|\n",
      "+--------+-------------+-------+-------------+\n",
      "|1       |MacBook Air  |1053.0 |null         |\n",
      "|2       |iPad Air     |711.47 |null         |\n",
      "|3       |MacBook Pro  |2072.38|null         |\n",
      "|4       |AirPods      |178.6  |null         |\n",
      "|5       |AirPods      |217.27 |null         |\n",
      "|6       |AirPods Pro  |293.9  |null         |\n",
      "|7       |iPad Pro     |971.32 |null         |\n",
      "|8       |AirPods      |236.57 |null         |\n",
      "|9       |Mac Studio   |1845.1 |null         |\n",
      "|10      |iPhone 14 Pro|1310.26|null         |\n",
      "+--------+-------------+-------+-------------+\n",
      "\n",
      "discount_rate가 NULL인 레코드: 100건 (= 기존 전체 데이터)\n"
     ]
    }
   ],
   "source": [
    "# 기존 데이터에서 discount_rate 값 확인\n",
    "print(\"[ 기존 데이터의 discount_rate 값 ]\\n\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, product_name, amount, discount_rate\n",
    "    FROM demo.lab.schema_orders\n",
    "    ORDER BY order_id\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "null_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS cnt\n",
    "    FROM demo.lab.schema_orders\n",
    "    WHERE discount_rate IS NULL\n",
    "\"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(f\"discount_rate가 NULL인 레코드: {null_count}건 (= 기존 전체 데이터)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100000d",
   "metadata": {},
   "source": [
    "## 새 스키마로 데이터 삽입\n",
    "\n",
    "`discount_rate`를 포함한 새 데이터 50건을 삽입합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e100000e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삽입 후 총 레코드: 150건\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(77)\n",
    "\n",
    "# discount_rate를 포함하는 데이터 생성\n",
    "new_orders = generate_orders(num_records=50, id_offset=101, seed=77)\n",
    "for order in new_orders:\n",
    "    order['discount_rate'] = round(random.uniform(0.0, 0.3), 2)\n",
    "\n",
    "# Spark DataFrame으로 변환 (discount_rate 포함)\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pdf = pd.DataFrame(new_orders)\n",
    "new_df = spark.createDataFrame(pdf)\n",
    "new_df = new_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    "\n",
    "new_df.writeTo(\"demo.lab.schema_orders\").append()\n",
    "\n",
    "count = spark.sql(\"SELECT COUNT(*) AS cnt FROM demo.lab.schema_orders\").collect()[0][\"cnt\"]\n",
    "print(f\"삽입 후 총 레코드: {count}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e100000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 새로 삽입된 데이터 (discount_rate 포함) ]\n",
      "\n",
      "+--------+--------------+-------+-------------+\n",
      "|order_id|product_name  |amount |discount_rate|\n",
      "+--------+--------------+-------+-------------+\n",
      "|101     |Apple Watch   |372.14 |0.16         |\n",
      "|102     |Magic Mouse   |99.04  |0.2          |\n",
      "|103     |iPad Air      |664.52 |0.02         |\n",
      "|104     |Apple Watch   |324.53 |0.17         |\n",
      "|105     |iPhone SE     |539.58 |0.3          |\n",
      "|106     |Magic Keyboard|169.38 |0.12         |\n",
      "|107     |Mac Mini      |790.28 |0.21         |\n",
      "|108     |MacBook Pro   |2276.3 |0.05         |\n",
      "|109     |Pro Display   |6490.22|0.14         |\n",
      "|110     |Magic Keyboard|227.97 |0.16         |\n",
      "+--------+--------------+-------+-------------+\n",
      "\n",
      "전체: 150건, discount_rate 있음: 50건, NULL: 100건\n"
     ]
    }
   ],
   "source": [
    "# 새 데이터의 discount_rate 확인\n",
    "print(\"[ 새로 삽입된 데이터 (discount_rate 포함) ]\\n\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, product_name, amount, discount_rate\n",
    "    FROM demo.lab.schema_orders\n",
    "    WHERE order_id >= 101\n",
    "    ORDER BY order_id\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# NULL vs non-NULL 통계\n",
    "stats = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS total,\n",
    "        COUNT(discount_rate) AS has_discount,\n",
    "        COUNT(*) - COUNT(discount_rate) AS null_discount\n",
    "    FROM demo.lab.schema_orders\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"전체: {stats['total']}건, discount_rate 있음: {stats['has_discount']}건, NULL: {stats['null_discount']}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000010",
   "metadata": {},
   "source": [
    "## RENAME COLUMN: 컬럼 이름 변경\n",
    "\n",
    "`product_name`을 `item_name`으로 변경합니다. Field ID는 그대로 유지됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_name → item_name 변경 완료\n",
      "\n",
      "[ 변경된 스키마 ]\n",
      "+--------------+------------------+-------+\n",
      "|col_name      |data_type         |comment|\n",
      "+--------------+------------------+-------+\n",
      "|order_id      |bigint            |null   |\n",
      "|customer_id   |bigint            |null   |\n",
      "|item_name     |string            |null   |\n",
      "|order_date    |date              |null   |\n",
      "|amount        |double            |null   |\n",
      "|status        |string            |null   |\n",
      "|discount_rate |double            |null   |\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|Part 0        |months(order_date)|       |\n",
      "+--------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.lab.schema_orders\n",
    "    RENAME COLUMN product_name TO item_name\n",
    "\"\"\")\n",
    "\n",
    "print(\"product_name → item_name 변경 완료\")\n",
    "\n",
    "# 변경된 스키마 확인\n",
    "print(\"\\n[ 변경된 스키마 ]\")\n",
    "spark.sql(\"DESCRIBE demo.lab.schema_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1000012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ RENAME 후 데이터 조회 ]\n",
      "\n",
      "+--------+-----------+-------+\n",
      "|order_id|item_name  |amount |\n",
      "+--------+-----------+-------+\n",
      "|1       |MacBook Air|1053.0 |\n",
      "|2       |iPad Air   |711.47 |\n",
      "|3       |MacBook Pro|2072.38|\n",
      "|4       |AirPods    |178.6  |\n",
      "|5       |AirPods    |217.27 |\n",
      "+--------+-----------+-------+\n",
      "\n",
      "기존 데이터도 새 이름(item_name)으로 정상 조회됩니다.\n",
      "→ Iceberg가 field ID로 매핑하기 때문입니다.\n"
     ]
    }
   ],
   "source": [
    "# 이름 변경 후 데이터 조회 — 기존 데이터도 item_name으로 조회 가능\n",
    "print(\"[ RENAME 후 데이터 조회 ]\\n\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, item_name, amount\n",
    "    FROM demo.lab.schema_orders\n",
    "    ORDER BY order_id\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"기존 데이터도 새 이름(item_name)으로 정상 조회됩니다.\")\n",
    "print(\"→ Iceberg가 field ID로 매핑하기 때문입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000013",
   "metadata": {},
   "source": [
    "## metadata.json에서 스키마 히스토리 확인\n",
    "\n",
    "metadata.json의 `schemas` 배열에 모든 스키마 버전이 기록됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최신 metadata: v5.metadata.json\n",
      "전체 metadata 파일 수: 5개\n",
      "\n",
      "현재 schema ID: 2\n",
      "스키마 버전 수: 3\n",
      "\n",
      "--- Schema ID: 0 ---\n",
      "  id= 1  order_id              type=long\n",
      "  id= 2  customer_id           type=long\n",
      "  id= 3  product_name          type=string\n",
      "  id= 4  order_date            type=date\n",
      "  id= 5  amount                type=double\n",
      "  id= 6  status                type=string\n",
      "\n",
      "--- Schema ID: 1 ---\n",
      "  id= 1  order_id              type=long\n",
      "  id= 2  customer_id           type=long\n",
      "  id= 3  product_name          type=string\n",
      "  id= 4  order_date            type=date\n",
      "  id= 5  amount                type=double\n",
      "  id= 6  status                type=string\n",
      "  id= 7  discount_rate         type=double\n",
      "\n",
      "--- Schema ID: 2 ---\n",
      "  id= 1  order_id              type=long\n",
      "  id= 2  customer_id           type=long\n",
      "  id= 3  item_name             type=string\n",
      "  id= 4  order_date            type=date\n",
      "  id= 5  amount                type=double\n",
      "  id= 6  status                type=string\n",
      "  id= 7  discount_rate         type=double\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "METADATA_PATH = \"/home/jovyan/data/warehouse/lab/schema_orders/metadata\"\n",
    "\n",
    "# 최신 metadata.json\n",
    "metadata_files = sorted(glob.glob(f\"{METADATA_PATH}/v*.metadata.json\"))\n",
    "latest = metadata_files[-1]\n",
    "print(f\"최신 metadata: {os.path.basename(latest)}\")\n",
    "print(f\"전체 metadata 파일 수: {len(metadata_files)}개\\n\")\n",
    "\n",
    "with open(latest) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "print(f\"현재 schema ID: {meta.get('current-schema-id')}\")\n",
    "print(f\"스키마 버전 수: {len(meta.get('schemas', []))}\\n\")\n",
    "\n",
    "for schema in meta.get('schemas', []):\n",
    "    print(f\"--- Schema ID: {schema['schema-id']} ---\")\n",
    "    for field in schema['fields']:\n",
    "        print(f\"  id={field['id']:2d}  {field['name']:20s}  type={field['type']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000015",
   "metadata": {},
   "source": [
    "## Time Travel로 이전 스키마 데이터 조회\n",
    "\n",
    "스키마가 변경되기 전 시점의 데이터를 Time Travel로 조회합니다.  \n",
    "`VERSION AS OF` 조회 시 컬럼 이름/개수가 어떤 스키마를 따르는지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1000016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Schema v0 시점 (스냅샷 3746370947229820768) ]\n",
      "\n",
      "+--------+-----------+------------+----------+-------+----------+\n",
      "|order_id|customer_id|product_name|order_date|amount |status    |\n",
      "+--------+-----------+------------+----------+-------+----------+\n",
      "|1       |350        |MacBook Air |2024-02-05|1053.0 |pending   |\n",
      "|2       |858        |iPad Air    |2024-03-27|711.47 |processing|\n",
      "|3       |130        |MacBook Pro |2024-01-05|2072.38|completed |\n",
      "|4       |127        |AirPods     |2024-03-18|178.6  |processing|\n",
      "|5       |658        |AirPods     |2024-03-30|217.27 |cancelled |\n",
      "+--------+-----------+------------+----------+-------+----------+\n",
      "\n",
      "v0 시점 레코드 수: 100건\n",
      "\n",
      "→ VERSION AS OF는 해당 스냅샷의 스키마를 기준으로 조회됩니다.\n",
      "→ 이 스냅샷에서는 product_name이 보이고 discount_rate 컬럼은 나타나지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# Schema v0 시점의 데이터 조회 (ADD COLUMNS 이전)\n",
    "print(f\"[ Schema v0 시점 (스냅샷 {snapshot_v0_id}) ]\\n\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM demo.lab.schema_orders\n",
    "    VERSION AS OF {snapshot_v0_id}\n",
    "    ORDER BY order_id\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "v0_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS cnt\n",
    "    FROM demo.lab.schema_orders\n",
    "    VERSION AS OF {snapshot_v0_id}\n",
    "\"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(f\"v0 시점 레코드 수: {v0_count}건\")\n",
    "print(\"\\n→ VERSION AS OF는 해당 스냅샷의 스키마를 기준으로 조회됩니다.\")\n",
    "print(\"→ 이 스냅샷에서는 product_name이 보이고 discount_rate 컬럼은 나타나지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000017",
   "metadata": {},
   "source": [
    "## 관찰 포인트\n",
    "\n",
    "### Schema Evolution의 핵심\n",
    "\n",
    "1. **컬럼이 추가되어도 기존 데이터는 그대로입니다**\n",
    "   - 스키마 변경(ADD/RENAME/DROP) 자체 때문에 기존 Parquet 파일을 다시 쓰지는 않습니다\n",
    "   - 최신 스냅샷 조회에서는 read-time projection으로 새 컬럼이 기존 데이터에서 `NULL`로 반환됩니다\n",
    "\n",
    "2. **컬럼 이름을 변경해도 데이터가 깨지지 않습니다**\n",
    "   - Iceberg는 `field ID`로 컬럼을 추적합니다\n",
    "   - `product_name` → `item_name`으로 변경해도, ID가 같으면 동일 컬럼\n",
    "\n",
    "3. **Iceberg가 스키마 버전을 추적합니다**\n",
    "   - `metadata.json`의 `schemas` 배열에 모든 버전이 기록\n",
    "   - `current-schema-id`로 현재 활성 스키마를 식별\n",
    "   - 스냅샷마다 `schema-id`가 연결되며, `VERSION AS OF`는 해당 스냅샷 스키마로 조회됩니다\n",
    "\n",
    "4. **스키마 변경 시 파일 rewrite가 없는 장점**\n",
    "   - 대규모 테이블에서 스키마 변경이 **즉시(O(1))** 완료\n",
    "   - ALTER TABLE은 metadata.json만 갱신하므로 매우 빠름\n",
    "   - 데이터 마이그레이션이 필요 없음\n",
    "   - 단, OPTIMIZE/compaction/rewrite_data_files 및 일부 UPDATE/DELETE/MERGE는 새 파일을 생성해 교체할 수 있음 (기존 파일 in-place 수정은 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1000018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
