{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1000001",
   "metadata": {},
   "source": [
    "# 02. Schema Evolution\n",
    "\n",
    "Iceberg의 **Schema Evolution** 기능을 통해 테이블 스키마를 안전하게 변경하고, 기존 데이터에 미치는 영향을 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000002",
   "metadata": {},
   "source": [
    "## Schema Evolution 개념\n",
    "\n",
    "전통적인 테이블 포맷에서 스키마 변경은 위험한 작업입니다.  \n",
    "컬럼 이름이나 순서에 의존하기 때문에, 변경 시 기존 데이터와 호환성 문제가 발생할 수 있습니다.\n",
    "\n",
    "### Iceberg의 접근 방식: Field ID 기반 매핑\n",
    "\n",
    "Iceberg는 **이름(name)**이 아닌 **고유 ID(field ID)**로 컬럼을 추적합니다.\n",
    "\n",
    "```\n",
    "Schema v0:                    Schema v1:\n",
    "┌─────────────────────┐      ┌─────────────────────────────┐\n",
    "│ id=1  order_id      │      │ id=1  order_id              │\n",
    "│ id=2  customer_id   │      │ id=2  customer_id           │\n",
    "│ id=3  product_name  │      │ id=3  item_name (renamed!)  │\n",
    "│ id=4  order_date    │      │ id=4  order_date            │\n",
    "│ id=5  amount        │      │ id=5  amount                │\n",
    "│ id=6  status        │      │ id=6  status                │\n",
    "└─────────────────────┘      │ id=7  discount_rate (new!)  │\n",
    "                              └─────────────────────────────┘\n",
    "```\n",
    "\n",
    "- 컬럼 이름을 변경해도 `field ID`가 같으면 동일 컬럼으로 인식\n",
    "- 새 컬럼이 추가되면 기존 데이터에서는 `NULL`로 반환\n",
    "- 기존 Parquet 파일은 **절대 다시 쓰지 않음** (read-time projection)\n",
    "\n",
    "### 지원되는 스키마 변경\n",
    "\n",
    "| 변경 유형 | SQL | 설명 |\n",
    "|-----------|-----|------|\n",
    "| 컬럼 추가 | `ADD COLUMNS` | 새 컬럼 추가 (기존 데이터는 NULL) |\n",
    "| 컬럼 이름 변경 | `RENAME COLUMN` | field ID는 유지 |\n",
    "| 컬럼 타입 변경 | `ALTER COLUMN ... TYPE` | 호환 가능한 타입만 (int→bigint 등) |\n",
    "| 컬럼 삭제 | `DROP COLUMN` | 메타데이터에서만 제거 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000003",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session(\"SchemaEvolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000006",
   "metadata": {},
   "source": [
    "## 테이블 생성 및 초기 데이터 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo.lab\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS demo.lab.schema_orders\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE demo.lab.schema_orders (\n",
    "        order_id     BIGINT,\n",
    "        customer_id  BIGINT,\n",
    "        product_name STRING,\n",
    "        order_date   DATE,\n",
    "        amount       DOUBLE,\n",
    "        status       STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(\"테이블 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100건 삽입 (Schema v0)\n",
    "orders = generate_orders(num_records=100, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(\"demo.lab.schema_orders\").append()\n",
    "\n",
    "count = spark.sql(\"SELECT COUNT(*) AS cnt FROM demo.lab.schema_orders\").collect()[0][\"cnt\"]\n",
    "print(f\"초기 데이터 삽입: {count}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 스냅샷 ID 저장\n",
    "snap_v0 = spark.sql(\"SELECT snapshot_id FROM demo.lab.schema_orders.snapshots ORDER BY committed_at\").collect()\n",
    "snapshot_v0_id = snap_v0[-1][\"snapshot_id\"]\n",
    "print(f\"Schema v0 스냅샷 ID: {snapshot_v0_id}\")\n",
    "\n",
    "# 현재 스키마 확인\n",
    "print(\"\\n[ 현재 스키마 ]\")\n",
    "spark.sql(\"DESCRIBE demo.lab.schema_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100000a",
   "metadata": {},
   "source": [
    "## ADD COLUMNS: 새 컬럼 추가\n",
    "\n",
    "`discount_rate` 컬럼을 추가합니다. 기존 데이터에서 이 컬럼은 `NULL`이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.lab.schema_orders\n",
    "    ADD COLUMNS (discount_rate DOUBLE)\n",
    "\"\"\")\n",
    "\n",
    "print(\"discount_rate 컬럼 추가 완료\")\n",
    "\n",
    "# 스키마 확인\n",
    "print(\"\\n[ 변경된 스키마 ]\")\n",
    "spark.sql(\"DESCRIBE demo.lab.schema_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터에서 discount_rate 값 확인\n",
    "print(\"[ 기존 데이터의 discount_rate 값 ]\\n\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, product_name, amount, discount_rate\n",
    "    FROM demo.lab.schema_orders\n",
    "    ORDER BY order_id\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "null_count = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS cnt\n",
    "    FROM demo.lab.schema_orders\n",
    "    WHERE discount_rate IS NULL\n",
    "\"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(f\"discount_rate가 NULL인 레코드: {null_count}건 (= 기존 전체 데이터)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100000d",
   "metadata": {},
   "source": [
    "## 새 스키마로 데이터 삽입\n",
    "\n",
    "`discount_rate`를 포함한 새 데이터 50건을 삽입합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(77)\n",
    "\n",
    "# discount_rate를 포함하는 데이터 생성\n",
    "new_orders = generate_orders(num_records=50, id_offset=101, seed=77)\n",
    "for order in new_orders:\n",
    "    order['discount_rate'] = round(random.uniform(0.0, 0.3), 2)\n",
    "\n",
    "# Spark DataFrame으로 변환 (discount_rate 포함)\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pdf = pd.DataFrame(new_orders)\n",
    "new_df = spark.createDataFrame(pdf)\n",
    "new_df = new_df.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    "\n",
    "new_df.writeTo(\"demo.lab.schema_orders\").append()\n",
    "\n",
    "count = spark.sql(\"SELECT COUNT(*) AS cnt FROM demo.lab.schema_orders\").collect()[0][\"cnt\"]\n",
    "print(f\"삽입 후 총 레코드: {count}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 데이터의 discount_rate 확인\n",
    "print(\"[ 새로 삽입된 데이터 (discount_rate 포함) ]\\n\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, product_name, amount, discount_rate\n",
    "    FROM demo.lab.schema_orders\n",
    "    WHERE order_id >= 101\n",
    "    ORDER BY order_id\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# NULL vs non-NULL 통계\n",
    "stats = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS total,\n",
    "        COUNT(discount_rate) AS has_discount,\n",
    "        COUNT(*) - COUNT(discount_rate) AS null_discount\n",
    "    FROM demo.lab.schema_orders\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"전체: {stats['total']}건, discount_rate 있음: {stats['has_discount']}건, NULL: {stats['null_discount']}건\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000010",
   "metadata": {},
   "source": [
    "## RENAME COLUMN: 컬럼 이름 변경\n",
    "\n",
    "`product_name`을 `item_name`으로 변경합니다. Field ID는 그대로 유지됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.lab.schema_orders\n",
    "    RENAME COLUMN product_name TO item_name\n",
    "\"\"\")\n",
    "\n",
    "print(\"product_name → item_name 변경 완료\")\n",
    "\n",
    "# 변경된 스키마 확인\n",
    "print(\"\\n[ 변경된 스키마 ]\")\n",
    "spark.sql(\"DESCRIBE demo.lab.schema_orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이름 변경 후 데이터 조회 — 기존 데이터도 item_name으로 조회 가능\n",
    "print(\"[ RENAME 후 데이터 조회 ]\\n\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, item_name, amount\n",
    "    FROM demo.lab.schema_orders\n",
    "    ORDER BY order_id\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"기존 데이터도 새 이름(item_name)으로 정상 조회됩니다.\")\n",
    "print(\"→ Iceberg가 field ID로 매핑하기 때문입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000013",
   "metadata": {},
   "source": [
    "## metadata.json에서 스키마 히스토리 확인\n",
    "\n",
    "metadata.json의 `schemas` 배열에 모든 스키마 버전이 기록됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "METADATA_PATH = \"/home/jovyan/data/warehouse/lab/schema_orders/metadata\"\n",
    "\n",
    "# 최신 metadata.json\n",
    "metadata_files = sorted(glob.glob(f\"{METADATA_PATH}/v*.metadata.json\"))\n",
    "latest = metadata_files[-1]\n",
    "print(f\"최신 metadata: {os.path.basename(latest)}\")\n",
    "print(f\"전체 metadata 파일 수: {len(metadata_files)}개\\n\")\n",
    "\n",
    "with open(latest) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "print(f\"현재 schema ID: {meta.get('current-schema-id')}\")\n",
    "print(f\"스키마 버전 수: {len(meta.get('schemas', []))}\\n\")\n",
    "\n",
    "for schema in meta.get('schemas', []):\n",
    "    print(f\"--- Schema ID: {schema['schema-id']} ---\")\n",
    "    for field in schema['fields']:\n",
    "        print(f\"  id={field['id']:2d}  {field['name']:20s}  type={field['type']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000015",
   "metadata": {},
   "source": [
    "## Time Travel로 이전 스키마 데이터 조회\n",
    "\n",
    "스키마가 변경되기 전 시점의 데이터를 Time Travel로 조회합니다.  \n",
    "이전 스키마에 없는 `discount_rate`는 어떻게 처리될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema v0 시점의 데이터 조회 (ADD COLUMNS 이전)\n",
    "print(f\"[ Schema v0 시점 (스냅샷 {snapshot_v0_id}) ]\\n\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM demo.lab.schema_orders\n",
    "    VERSION AS OF {snapshot_v0_id}\n",
    "    ORDER BY order_id\n",
    "    LIMIT 5\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "v0_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) AS cnt\n",
    "    FROM demo.lab.schema_orders\n",
    "    VERSION AS OF {snapshot_v0_id}\n",
    "\"\"\").collect()[0][\"cnt\"]\n",
    "\n",
    "print(f\"v0 시점 레코드 수: {v0_count}건\")\n",
    "print(\"\\n→ 현재 스키마(item_name, discount_rate)가 이전 데이터에도 적용됩니다.\")\n",
    "print(\"→ discount_rate는 NULL, product_name은 item_name으로 표시됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1000017",
   "metadata": {},
   "source": [
    "## 관찰 포인트\n",
    "\n",
    "### Schema Evolution의 핵심\n",
    "\n",
    "1. **컬럼이 추가되어도 기존 데이터는 그대로입니다**\n",
    "   - 기존 Parquet 파일은 다시 쓰지 않습니다\n",
    "   - 새 컬럼은 기존 데이터에서 `NULL`로 반환됩니다 (read-time projection)\n",
    "\n",
    "2. **컬럼 이름을 변경해도 데이터가 깨지지 않습니다**\n",
    "   - Iceberg는 `field ID`로 컬럼을 추적합니다\n",
    "   - `product_name` → `item_name`으로 변경해도, ID가 같으면 동일 컬럼\n",
    "\n",
    "3. **Iceberg가 스키마 버전을 추적합니다**\n",
    "   - `metadata.json`의 `schemas` 배열에 모든 버전이 기록\n",
    "   - `current-schema-id`로 현재 활성 스키마를 식별\n",
    "   - Time Travel 시에도 현재 스키마 기준으로 데이터를 읽습니다\n",
    "\n",
    "4. **기존 파일을 다시 쓰지 않는 장점**\n",
    "   - 대규모 테이블에서 스키마 변경이 **즉시(O(1))** 완료\n",
    "   - ALTER TABLE은 metadata.json만 갱신하므로 매우 빠름\n",
    "   - 데이터 마이그레이션이 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}