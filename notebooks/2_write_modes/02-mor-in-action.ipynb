{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "# MOR(Merge-on-Read) 동작 원리 실습\n",
    "\n",
    "이 노트북에서는 Iceberg의 **Merge-on-Read(MOR)** 전략이 실제로 어떻게 동작하는지 파일 수준에서 관찰합니다.  \n",
    "특히 **Delete File**의 생성과 역할에 집중합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "## MOR(Merge-on-Read)란?\n",
    "\n",
    "MOR은 COW의 \"파일 전체 재작성\" 문제를 해결하기 위한 전략입니다.\n",
    "\n",
    "### 핵심 원리\n",
    "전체 데이터 파일을 다시 쓰는 대신, **삭제 파일(Delete File)**에 어떤 레코드를 무시할지 기록합니다.\n",
    "\n",
    "### 삭제(DELETE) 시 동작\n",
    "- 기존 데이터 파일은 **그대로 유지**\n",
    "- 삭제할 레코드의 위치가 **Delete File**에 기록됨\n",
    "\n",
    "### 업데이트(UPDATE) 시 동작\n",
    "1. 이전 레코드의 위치가 **Delete File**에 추가\n",
    "2. 업데이트된 레코드만 **새 데이터 파일**에 작성\n",
    "\n",
    "### 읽기(READ) 시 동작\n",
    "- 데이터 파일과 Delete File을 **조정(merge)**하여 최종 결과 반환\n",
    "- 즉, 삭제 표시된 레코드를 제외하고 읽음\n",
    "\n",
    "### 장점 vs 단점\n",
    "- **장점**: 쓰기가 빠름 (파일 전체를 재작성하지 않으므로)\n",
    "- **단점**: 읽기가 느림 (Delete File과의 병합 작업 필요)\n",
    "\n",
    "```\n",
    "DELETE 시:\n",
    "┌─────────────────┐    ┌─────────────────┐\n",
    "│  데이터 파일       │    │  Delete File     │\n",
    "│  (1000행, 그대로)  │ +  │  (삭제할 행 위치)   │\n",
    "│  ✅ 파일 유지      │    │  ✅ 새로 생성      │\n",
    "└─────────────────┘    └─────────────────┘\n",
    "\n",
    "UPDATE 시:\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│  기존 데이터 파일   │    │  Delete File     │    │  새 데이터 파일    │\n",
    "│  (그대로 유지)     │ +  │  (기존 행 위치)    │ +  │  (변경된 행만)     │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e4f5",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, snapshot_tree, diff_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "TABLE_NAME = \"demo.lab.mor_orders\"\n",
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/mor_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6b7c8",
   "metadata": {},
   "source": [
    "## MOR 테이블 생성\n",
    "\n",
    "모든 쓰기 모드(delete, update, merge)를 `merge-on-read`로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.delete.mode'='merge-on-read',\n",
    "    'write.update.mode'='merge-on-read',\n",
    "    'write.merge.mode'='merge-on-read'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(f\"테이블 생성 완료: {TABLE_NAME}\")\n",
    "\n",
    "# 설정 확인\n",
    "props = spark.sql(f\"SHOW TBLPROPERTIES {TABLE_NAME}\").collect()\n",
    "for row in props:\n",
    "    if 'write' in row['key']:\n",
    "        print(f\"  {row['key']} = {row['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8d9e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 1: INSERT — 초기 데이터 적재\n",
    "\n",
    "100건의 주문 데이터를 삽입합니다. INSERT는 COW와 동일하게 동작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_insert = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# 100건 INSERT\n",
    "orders = generate_orders(num_records=100, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(TABLE_NAME).append()\n",
    "\n",
    "after_insert = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INSERT 후 변경 사항:\")\n",
    "print(\"=\" * 60)\n",
    "diff_tree(before_insert, after_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INSERT 후 테이블 트리 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH)\n",
    "\n",
    "print(f\"\\n파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")\n",
    "print(f\"총 레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1a2b3",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — INSERT\n",
    "\n",
    "- INSERT는 COW/MOR 구분 없이 동일하게 동작합니다\n",
    "- 데이터 파일만 생성되었고, Delete File은 없습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2b3c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 2: DELETE — Delete File 생성 관찰\n",
    "\n",
    "`status='cancelled'`인 주문을 삭제합니다.  \n",
    "MOR에서는 데이터 파일을 재작성하지 않고 **Positional Delete File**을 생성합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE 대상 확인\n",
    "cancelled_count = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'cancelled'\").collect()[0][0]\n",
    "print(f\"DELETE 대상 (status='cancelled'): {cancelled_count}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_delete = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# DELETE 실행\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE status = 'cancelled'\")\n",
    "\n",
    "after_delete = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DELETE 후 변경 사항 (MOR):\")\n",
    "print(\"=\" * 60)\n",
    "diff_tree(before_delete, after_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DELETE 후 테이블 트리 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH)\n",
    "\n",
    "print(f\"\\n파일 수 (parquet): {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")\n",
    "print(f\"총 레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f7a8",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — DELETE (MOR)\n",
    "\n",
    "- **기존 데이터 파일은 그대로** 유지되었습니다!\n",
    "- 대신 **Positional Delete File**이 새로 생성되었습니다\n",
    "- COW였다면 데이터 파일이 통째로 재작성되었을 텐데, MOR은 작은 Delete File만 추가합니다\n",
    "- 이것이 MOR의 쓰기 성능이 빠른 이유입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7a8b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 3: UPDATE — Delete File + 새 데이터 파일\n",
    "\n",
    "`status='pending'`인 주문을 `status='refunded'`로 변경합니다.  \n",
    "MOR의 UPDATE는 두 가지를 동시에 수행합니다:\n",
    "1. 기존 행의 위치를 **Delete File**에 기록\n",
    "2. 변경된 행을 **새 데이터 파일**에 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE 대상 확인\n",
    "pending_count = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'pending'\").collect()[0][0]\n",
    "print(f\"UPDATE 대상 (status='pending'): {pending_count}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_update = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# UPDATE 실행\n",
    "spark.sql(f\"UPDATE {TABLE_NAME} SET status = 'refunded' WHERE status = 'pending'\")\n",
    "\n",
    "after_update = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UPDATE 후 변경 사항 (MOR):\")\n",
    "print(\"=\" * 60)\n",
    "diff_tree(before_update, after_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UPDATE 후 테이블 트리 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH)\n",
    "\n",
    "print(f\"\\n파일 수 (parquet): {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1e2f3",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — UPDATE (MOR)\n",
    "\n",
    "- **Delete File**이 추가 생성되었습니다 (기존 행의 위치 기록)\n",
    "- **새 데이터 파일**도 생성되었습니다 (변경된 행만 포함)\n",
    "- 기존 데이터 파일은 **역시 그대로** 유지됩니다\n",
    "- 즉, UPDATE = Delete File + 새 데이터 파일 조합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4",
   "metadata": {},
   "source": [
    "---\n",
    "## Delete File 상세 분석\n",
    "\n",
    "MOR의 핵심인 Delete File을 자세히 들여다봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files 메타데이터 테이블에서 Delete File 확인\n",
    "# content 컬럼: 0=데이터, 1=positional deletes, 2=equality deletes\n",
    "print(\"전체 파일 목록 (데이터 + Delete):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        content,\n",
    "        CASE content \n",
    "            WHEN 0 THEN 'DATA'\n",
    "            WHEN 1 THEN 'POSITIONAL DELETE'\n",
    "            WHEN 2 THEN 'EQUALITY DELETE'\n",
    "        END as file_type,\n",
    "        file_path,\n",
    "        record_count,\n",
    "        file_size_in_bytes\n",
    "    FROM {TABLE_NAME}.files\n",
    "    ORDER BY content, file_path\n",
    "\"\"\").show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete File만 필터링\n",
    "print(\"Delete File 상세:\")\n",
    "delete_files_df = spark.sql(f\"\"\"\n",
    "    SELECT file_path, record_count, file_size_in_bytes\n",
    "    FROM {TABLE_NAME}.files\n",
    "    WHERE content > 0\n",
    "\"\"\")\n",
    "delete_files_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete File을 pyarrow로 직접 읽어보기\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "delete_files = delete_files_df.collect()\n",
    "if delete_files:\n",
    "    first_delete_file = delete_files[0]['file_path']\n",
    "    print(f\"Delete File 경로: {first_delete_file}\\n\")\n",
    "    \n",
    "    try:\n",
    "        table = pq.read_table(first_delete_file)\n",
    "        print(f\"스키마: {table.schema}\")\n",
    "        print(f\"레코드 수: {len(table)}\")\n",
    "        print(f\"\\n내용 (처음 10행):\")\n",
    "        print(table.to_pandas().head(10))\n",
    "    except Exception as e:\n",
    "        print(f\"읽기 실패: {e}\")\n",
    "        print(\"(Delete File 형식이 표준 Parquet과 다를 수 있습니다)\")\n",
    "else:\n",
    "    print(\"Delete File이 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d7e8",
   "metadata": {},
   "source": [
    "---\n",
    "## Delete File 유형 설명\n",
    "\n",
    "Iceberg에는 두 가지 유형의 Delete File이 있습니다:\n",
    "\n",
    "### 1. Positional Delete (위치 기반 삭제)\n",
    "- **컬럼**: `file_path` + `pos` (행 번호)\n",
    "- **원리**: \"이 파일의 N번째 행을 무시하라\"\n",
    "- **비유**: 영화관 좌석번호로 찾기 — \"3열 5번 좌석의 사람\"\n",
    "- Spark에서 MOR 사용 시 기본적으로 생성되는 유형\n",
    "\n",
    "```\n",
    "Positional Delete File 내용:\n",
    "┌───────────────────────────┬─────┐\n",
    "│ file_path                 │ pos │\n",
    "├───────────────────────────┼─────┤\n",
    "│ data/part-00001.parquet   │  3  │\n",
    "│ data/part-00001.parquet   │  7  │\n",
    "│ data/part-00001.parquet   │ 15  │\n",
    "└───────────────────────────┴─────┘\n",
    "```\n",
    "\n",
    "### 2. Equality Delete (값 기반 삭제)\n",
    "- **컬럼**: 삭제 조건에 해당하는 컬럼값\n",
    "- **원리**: \"이 값과 일치하는 모든 행을 무시하라\"\n",
    "- **비유**: 군중에서 빨간 모자를 쓴 사람 모두 찾기 — \"빨간 모자인 사람\"\n",
    "- Spark에서는 거의 사용되지 않음 (주로 Flink 등에서 활용)\n",
    "\n",
    "```\n",
    "Equality Delete File 내용:\n",
    "┌──────────┐\n",
    "│ order_id │\n",
    "├──────────┤\n",
    "│    42    │\n",
    "│    57    │\n",
    "│    89    │\n",
    "└──────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스냅샷 히스토리 확인\n",
    "print(\"전체 스냅샷 히스토리:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8f9a0",
   "metadata": {},
   "source": [
    "---\n",
    "## 정리: MOR(Merge-on-Read) 핵심 요약\n",
    "\n",
    "| 항목 | 설명 |\n",
    "|------|------|\n",
    "| **쓰기 방식** | Delete File에 삭제할 행 위치를 기록, 기존 파일 유지 |\n",
    "| **Delete File** | Positional Delete (file_path + pos) 생성 |\n",
    "| **읽기 성능** | 느림 — 데이터 파일 + Delete File 병합 필요 |\n",
    "| **쓰기 성능** | 빠름 — 파일 전체 재작성 없음 |\n",
    "| **적합한 워크로드** | 쓰기 중심, 업데이트/삭제가 빈번한 경우 |\n",
    "\n",
    "> **핵심**: MOR에서는 **Delete File만 추가하고 기존 데이터 파일은 그대로** 유지됩니다.  \n",
    "> 읽을 때 데이터 파일과 Delete File을 **병합(merge)**하여 최종 결과를 만듭니다.  \n",
    "> 다음 노트북에서 COW와 MOR을 직접 비교해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}