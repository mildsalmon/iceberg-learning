{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "# COW vs MOR 직접 비교 실험\n",
    "\n",
    "동일한 데이터와 동일한 작업을 COW/MOR 테이블에서 각각 수행하고,  \n",
    "**실행 시간, 파일 구조, 메타데이터 계층**을 정량적으로 비교합니다.\n",
    "\n",
    "데이터 볼륨을 **50만 건**으로 설정하여,  \n",
    "COW의 **파일 재작성 비용**과 MOR의 **Delete File 전략** 차이가 실행 시간에 명확히 드러나도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "## 실험 설계\n",
    "\n",
    "| 단계 | 작업 | 데이터 규모 | 설명 |\n",
    "|------|------|------------|------|\n",
    "| 0 | Warmup | — | JVM/Iceberg 초기화 비용 제거 |\n",
    "| 1 | INSERT | 500,000건 | 초기 데이터 적재 (COW/MOR 동일) |\n",
    "| 2 | UPDATE | 50,000건 (10%) | 상태 변경 → COW는 파일 재작성, MOR는 Delete File + 새 데이터 |\n",
    "| 3 | DELETE | 20,000건 (4%) | 삭제 → COW는 파일 재작성, MOR는 Delete File만 추가 |\n",
    "| 4 | SELECT | 전체 | 읽기 → COW는 데이터만, MOR는 Delete File 병합 필요 |\n",
    "\n",
    "> **Warmup**: Spark/JVM은 첫 작업에서 클래스 로딩, JIT 컴파일 등의 초기화 비용이 발생합니다.  \n",
    "> 먼저 실행되는 쪽이 불리해지는 것을 방지하기 위해, 별도의 테이블에 dummy 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e4f5",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e4f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, count_files, total_size, show_metadata_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f5a6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "COW_TABLE = \"demo.lab.cmp_cow_orders\"\n",
    "MOR_TABLE = \"demo.lab.cmp_mor_orders\"\n",
    "COW_PATH = \"/home/jovyan/data/warehouse/lab/cmp_cow_orders\"\n",
    "MOR_PATH = \"/home/jovyan/data/warehouse/lab/cmp_mor_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6b7c8",
   "metadata": {},
   "source": [
    "## 테이블 생성\n",
    "\n",
    "동일한 스키마, 동일한 파티셔닝 — **TBLPROPERTIES만 다릅니다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b7c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW 테이블 생성 완료: demo.lab.cmp_cow_orders\n",
      "MOR 테이블 생성 완료: demo.lab.cmp_mor_orders\n",
      "JVM Warmup 완료 — 이후 측정은 초기화 비용이 제거된 상태\n"
     ]
    }
   ],
   "source": [
    "# 기존 테이블 정리\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {COW_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {MOR_TABLE}\")\n",
    "\n",
    "# COW 테이블\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {COW_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.delete.mode'='copy-on-write',\n",
    "    'write.update.mode'='copy-on-write',\n",
    "    'write.merge.mode'='copy-on-write'\n",
    ")\n",
    "\"\"\")\n",
    "print(f\"COW 테이블 생성 완료: {COW_TABLE}\")\n",
    "\n",
    "# MOR 테이블\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {MOR_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.delete.mode'='merge-on-read',\n",
    "    'write.update.mode'='merge-on-read',\n",
    "    'write.merge.mode'='merge-on-read'\n",
    ")\n",
    "\"\"\")\n",
    "print(f\"MOR 테이블 생성 완료: {MOR_TABLE}\")\n",
    "\n",
    "# ── JVM Warmup ──\n",
    "# 별도 dummy 테이블에 INSERT → UPDATE → DELETE → DROP 수행하여\n",
    "# Spark/JVM/Iceberg 초기화 비용을 미리 소모합니다.\n",
    "WARMUP_TABLE = \"demo.lab.cmp_warmup\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {WARMUP_TABLE}\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {WARMUP_TABLE} (id BIGINT, val STRING)\n",
    "USING ICEBERG\n",
    "\"\"\")\n",
    "warmup_df = spark.createDataFrame([(i, \"x\") for i in range(100)], [\"id\", \"val\"])\n",
    "warmup_df.writeTo(WARMUP_TABLE).append()\n",
    "spark.sql(f\"UPDATE {WARMUP_TABLE} SET val = 'y' WHERE id < 10\")\n",
    "spark.sql(f\"DELETE FROM {WARMUP_TABLE} WHERE id >= 90\")\n",
    "spark.sql(f\"SELECT COUNT(*) FROM {WARMUP_TABLE}\").collect()\n",
    "spark.sql(f\"DROP TABLE {WARMUP_TABLE}\")\n",
    "print(\"JVM Warmup 완료 — 이후 측정은 초기화 비용이 제거된 상태\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8d9e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 1: INSERT 500,000건\n",
    "\n",
    "동일한 데이터를 양쪽 테이블에 삽입합니다.  \n",
    "INSERT는 COW/MOR 모두 **새 데이터 파일을 생성**하는 동일한 동작이므로, 시간 차이가 거의 없어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d9e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW INSERT 시간: 2.114초\n",
      "MOR INSERT 시간: 1.198초\n",
      "\n",
      "양쪽 테이블 모두 500,000건 삽입 완료\n",
      "INSERT 시간 차이: 0.916초 (거의 동일해야 정상)\n",
      "\n",
      "파티션당 Parquet 파일 크기 합계: 3418 KB\n",
      "→ UPDATE/DELETE 시 COW는 이 크기만큼 전체를 재작성해야 합니다\n"
     ]
    }
   ],
   "source": [
    "NUM_RECORDS = 500_000\n",
    "\n",
    "orders = generate_orders(num_records=NUM_RECORDS, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.cache()\n",
    "df.count()  # 캐시 실체화\n",
    "\n",
    "# COW INSERT\n",
    "start = time.time()\n",
    "df.writeTo(COW_TABLE).append()\n",
    "cow_insert_time = time.time() - start\n",
    "print(f\"COW INSERT 시간: {cow_insert_time:.3f}초\")\n",
    "\n",
    "# MOR INSERT\n",
    "start = time.time()\n",
    "df.writeTo(MOR_TABLE).append()\n",
    "mor_insert_time = time.time() - start\n",
    "print(f\"MOR INSERT 시간: {mor_insert_time:.3f}초\")\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "cow_count_after_insert = spark.sql(f\"SELECT COUNT(*) FROM {COW_TABLE}\").collect()[0][0]\n",
    "print(f\"\\n양쪽 테이블 모두 {cow_count_after_insert:,}건 삽입 완료\")\n",
    "print(f\"INSERT 시간 차이: {abs(cow_insert_time - mor_insert_time):.3f}초 (거의 동일해야 정상)\")\n",
    "\n",
    "# INSERT 후 파일 크기 확인 — 이 파일들이 COW에서 재작성 대상\n",
    "cow_data_size = total_size(COW_PATH, ext=\".parquet\")\n",
    "print(f\"\\n파티션당 Parquet 파일 크기 합계: {cow_data_size / 1024:.0f} KB\")\n",
    "print(f\"→ UPDATE/DELETE 시 COW는 이 크기만큼 전체를 재작성해야 합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0f1a2",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — INSERT\n",
    "\n",
    "INSERT는 COW/MOR 차이가 없습니다. 둘 다 **새 데이터 파일을 생성**할 뿐이므로, Warmup 이후에는 시간이 거의 동일합니다.\n",
    "\n",
    "- COW: 새 Parquet 파일 생성\n",
    "- MOR: 새 Parquet 파일 생성\n",
    "- **차이점 없음** — 기존 데이터를 수정하지 않으므로 전략이 개입하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1a2b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 2: UPDATE 50,000건 (전체의 10%)\n",
    "\n",
    "order_id 기준으로 처음 50,000건의 상태를 변경합니다.  \n",
    "**여기서부터 COW와 MOR의 차이가 드러납니다.**\n",
    "\n",
    "- **COW**: 50,000건이 속한 파티션의 데이터 파일을 **통째로 재작성** — 나머지 450,000건도 다시 씀\n",
    "- **MOR**: 50,000건의 **Positional Delete File** + 변경된 값의 **새 데이터 파일**만 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW UPDATE 시간: 2.000초\n",
      "MOR UPDATE 시간: 1.472초\n",
      "\n",
      "COW/MOR 비율: 1.36x — COW가 1.4배 느림\n",
      "(50,000건 변경, COW는 500,000건 분량의 파일을 재작성)\n"
     ]
    }
   ],
   "source": [
    "UPDATE_COUNT = 50_000\n",
    "\n",
    "# COW UPDATE\n",
    "start = time.time()\n",
    "spark.sql(f\"UPDATE {COW_TABLE} SET status = 'refunded' WHERE order_id <= {UPDATE_COUNT}\")\n",
    "cow_update_time = time.time() - start\n",
    "print(f\"COW UPDATE 시간: {cow_update_time:.3f}초\")\n",
    "\n",
    "# MOR UPDATE\n",
    "start = time.time()\n",
    "spark.sql(f\"UPDATE {MOR_TABLE} SET status = 'refunded' WHERE order_id <= {UPDATE_COUNT}\")\n",
    "mor_update_time = time.time() - start\n",
    "print(f\"MOR UPDATE 시간: {mor_update_time:.3f}초\")\n",
    "\n",
    "ratio = cow_update_time / mor_update_time if mor_update_time > 0 else float('inf')\n",
    "print(f\"\\nCOW/MOR 비율: {ratio:.2f}x — COW가 {ratio:.1f}배 느림\")\n",
    "print(f\"({UPDATE_COUNT:,}건 변경, COW는 {NUM_RECORDS:,}건 분량의 파일을 재작성)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — UPDATE\n",
    "\n",
    "| | COW | MOR |\n",
    "|---|---|---|\n",
    "| **동작** | 영향받은 파티션의 데이터 파일을 통째로 재작성 | Delete File + 새 데이터 파일만 추가 |\n",
    "| **I/O** | 50,000건 변경인데 500,000건 분량을 기록 → **쓰기 증폭** | 변경된 50,000건 분량만 기록 → 최소 I/O |\n",
    "| **결과 파일** | 기존 파일 DELETED → 새 파일 ADDED | 기존 파일 유지 + Delete File ADDED + 새 데이터 ADDED |\n",
    "\n",
    "> 50만 건 규모에서는 COW의 **파일 재작성 비용**이 실행 시간에 명확히 반영됩니다.  \n",
    "> 3개 파티션 파일(각 수 MB)을 통째로 재작성하는 COW vs 작은 Delete File만 추가하는 MOR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 3: DELETE 20,000건 (전체의 4%)\n",
    "\n",
    "order_id 50,001~70,000번 주문을 삭제합니다.\n",
    "\n",
    "- **COW**: 해당 행이 빠진 **새 데이터 파일을 재작성** (나머지 행도 다시 씀)\n",
    "- **MOR**: 삭제 위치만 기록한 **Positional Delete File**만 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d5e6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW DELETE 시간: 2.313초\n",
      "MOR DELETE 시간: 1.769초\n",
      "\n",
      "COW/MOR 비율: 1.31x — COW가 1.3배 느림\n",
      "(20,000건 삭제, COW는 전체 파티션 파일을 재작성)\n"
     ]
    }
   ],
   "source": [
    "DELETE_COUNT = 20_000\n",
    "\n",
    "# COW DELETE\n",
    "start = time.time()\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {COW_TABLE}\n",
    "    WHERE order_id BETWEEN {UPDATE_COUNT + 1} AND {UPDATE_COUNT + DELETE_COUNT}\n",
    "\"\"\")\n",
    "cow_delete_time = time.time() - start\n",
    "print(f\"COW DELETE 시간: {cow_delete_time:.3f}초\")\n",
    "\n",
    "# MOR DELETE\n",
    "start = time.time()\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {MOR_TABLE}\n",
    "    WHERE order_id BETWEEN {UPDATE_COUNT + 1} AND {UPDATE_COUNT + DELETE_COUNT}\n",
    "\"\"\")\n",
    "mor_delete_time = time.time() - start\n",
    "print(f\"MOR DELETE 시간: {mor_delete_time:.3f}초\")\n",
    "\n",
    "ratio = cow_delete_time / mor_delete_time if mor_delete_time > 0 else float('inf')\n",
    "print(f\"\\nCOW/MOR 비율: {ratio:.2f}x — COW가 {ratio:.1f}배 느림\")\n",
    "print(f\"({DELETE_COUNT:,}건 삭제, COW는 전체 파티션 파일을 재작성)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f7a8",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — DELETE\n",
    "\n",
    "| | COW | MOR |\n",
    "|---|---|---|\n",
    "| **동작** | 삭제 행이 빠진 새 파일 재작성 | Positional Delete File만 추가 |\n",
    "| **I/O** | 20,000건 삭제인데 전체 파일 재작성 | 삭제 위치(file_path + pos)만 기록 → **매우 작은 I/O** |\n",
    "| **결과** | 기존 데이터 파일 DELETED → 새 파일 ADDED | 기존 데이터 파일 유지 + Delete File ADDED |\n",
    "\n",
    "> DELETE는 UPDATE보다 MOR의 이점이 더 큽니다.  \n",
    "> UPDATE는 새 값을 기록한 데이터 파일이 필요하지만, DELETE는 **Delete File만으로 충분**합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7a8b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 4: SELECT — 읽기 성능 비교\n",
    "\n",
    "전체 데이터를 읽는 시간을 비교합니다.\n",
    "\n",
    "- **COW**: 데이터 파일만 읽으면 됨 (이미 최신 상태)\n",
    "- **MOR**: 데이터 파일 + Delete File을 **병합(merge)**해야 정확한 결과를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a8b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW SELECT 시간: 0.145초 (레코드 수: 480,000)\n",
      "MOR SELECT 시간: 0.288초 (레코드 수: 480,000)\n",
      "\n",
      "MOR/COW 비율: 1.99x\n",
      "레코드 수 일치 여부: True (480,000건)\n"
     ]
    }
   ],
   "source": [
    "# COW SELECT\n",
    "start = time.time()\n",
    "cow_count = spark.sql(f\"SELECT COUNT(*) FROM {COW_TABLE}\").collect()[0][0]\n",
    "cow_read_time = time.time() - start\n",
    "print(f\"COW SELECT 시간: {cow_read_time:.3f}초 (레코드 수: {cow_count:,})\")\n",
    "\n",
    "# MOR SELECT\n",
    "start = time.time()\n",
    "mor_count = spark.sql(f\"SELECT COUNT(*) FROM {MOR_TABLE}\").collect()[0][0]\n",
    "mor_read_time = time.time() - start\n",
    "print(f\"MOR SELECT 시간: {mor_read_time:.3f}초 (레코드 수: {mor_count:,})\")\n",
    "\n",
    "ratio = mor_read_time / cow_read_time if cow_read_time > 0 else float('inf')\n",
    "print(f\"\\nMOR/COW 비율: {ratio:.2f}x\")\n",
    "print(f\"레코드 수 일치 여부: {cow_count == mor_count} ({cow_count:,}건)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9c0d1",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — SELECT\n",
    "\n",
    "| | COW | MOR |\n",
    "|---|---|---|\n",
    "| **읽기 대상** | 데이터 파일만 | 데이터 파일 + Delete File 병합 |\n",
    "| **Merge 비용** | 없음 | Delete File 기반 필터링 필요 |\n",
    "| **특성** | 쓰기 시 이미 최적화 완료 | 읽기 시 병합 비용 발생 |\n",
    "\n",
    "> 소규모 데이터에서는 읽기 차이가 미미하지만, Delete File이 누적될수록 MOR의 읽기 비용이 증가합니다.  \n",
    "> 이를 해결하기 위해 **정기적인 컴팩션(compaction)**이 필요합니다 (→ 4_optimization 모듈)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 파일 시스템 비교\n",
    "\n",
    "시간 측정만으로는 **왜** 차이가 나는지 알기 어렵습니다.  \n",
    "파일 시스템 수준에서 **데이터 파일 수, Delete File 유무, 메타데이터 계층**을 직접 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d1e2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COW 테이블 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00001-25-47934f7c-9286-4223-93db-bc2af0da7f84-00001.parquet  (1.1 MB)\n",
      "│   │   ├── 00001-40-8762e0fa-f3ed-41ca-ac9e-4d6e7cccf513-00001.parquet  (1.1 MB)\n",
      "│   │   └── 00001-47-342c099a-7482-4b17-ba40-441433059299-00001.parquet  (1.1 MB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00002-26-47934f7c-9286-4223-93db-bc2af0da7f84-00001.parquet  (1.1 MB)\n",
      "│   │   ├── 00002-41-8762e0fa-f3ed-41ca-ac9e-4d6e7cccf513-00001.parquet  (1.1 MB)\n",
      "│   │   └── 00002-48-342c099a-7482-4b17-ba40-441433059299-00001.parquet  (1.0 MB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-24-47934f7c-9286-4223-93db-bc2af0da7f84-00001.parquet  (1.1 MB)\n",
      "│       ├── 00000-39-8762e0fa-f3ed-41ca-ac9e-4d6e7cccf513-00001.parquet  (1.1 MB)\n",
      "│       └── 00000-46-342c099a-7482-4b17-ba40-441433059299-00001.parquet  (1.1 MB)\n",
      "└── metadata/\n",
      "    ├── 33eff80a-ec43-44cf-a6b4-7e0f178788b1-m0.avro  (7.3 KB)\n",
      "    ├── 7819d505-28d8-47a7-8484-68bff6ac517d-m0.avro  (7.3 KB)\n",
      "    ├── 7819d505-28d8-47a7-8484-68bff6ac517d-m1.avro  (7.3 KB)\n",
      "    ├── 9c1c6bb8-eb0e-4e10-9c6f-6c0ed049f256-m0.avro  (7.3 KB)\n",
      "    ├── 9c1c6bb8-eb0e-4e10-9c6f-6c0ed049f256-m1.avro  (7.3 KB)\n",
      "    ├── snap-2678696525839283789-1-33eff80a-ec43-44cf-a6b4-7e0f178788b1.avro  (4.2 KB)\n",
      "    ├── snap-2967345037190015042-1-7819d505-28d8-47a7-8484-68bff6ac517d.avro  (4.2 KB)\n",
      "    ├── snap-6307199624804126826-1-9c1c6bb8-eb0e-4e10-9c6f-6c0ed049f256.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.6 KB)\n",
      "    ├── v2.metadata.json  (2.7 KB)\n",
      "    ├── v3.metadata.json  (3.8 KB)\n",
      "    ├── v4.metadata.json  (4.9 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "Parquet 파일: 9개, 총 크기: 10,400,816 bytes\n",
      "\n",
      "============================================================\n",
      "MOR 테이블 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00001-deletes.parquet  (34.2 KB)\n",
      "│   │   ├── 00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00001.parquet  (116.1 KB)\n",
      "│   │   ├── 00000-51-9d080ee6-a67e-4976-bbf2-087b1504ac4d-00001-deletes.parquet  (8.6 KB)\n",
      "│   │   └── 00001-33-9a0e2130-dc45-41e3-a81e-9d93d6bf199a-00001.parquet  (1.1 MB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00002-deletes.parquet  (32.4 KB)\n",
      "│   │   ├── 00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00002.parquet  (108.9 KB)\n",
      "│   │   ├── 00000-51-9d080ee6-a67e-4976-bbf2-087b1504ac4d-00002-deletes.parquet  (8.0 KB)\n",
      "│   │   └── 00002-34-9a0e2130-dc45-41e3-a81e-9d93d6bf199a-00001.parquet  (1.1 MB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-32-9a0e2130-dc45-41e3-a81e-9d93d6bf199a-00001.parquet  (1.1 MB)\n",
      "│       ├── 00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00003-deletes.parquet  (34.4 KB)\n",
      "│       ├── 00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00003.parquet  (116.3 KB)\n",
      "│       └── 00000-51-9d080ee6-a67e-4976-bbf2-087b1504ac4d-00003-deletes.parquet  (8.4 KB)\n",
      "└── metadata/\n",
      "    ├── 6eab82a3-1b88-44cf-98f8-88d44d924ea3-m0.avro  (7.2 KB)\n",
      "    ├── dd972ccc-b199-447e-8988-1ce3bbecd774-m0.avro  (7.3 KB)\n",
      "    ├── e7631b72-f976-4829-9d64-2295add68d08-m0.avro  (7.3 KB)\n",
      "    ├── e7631b72-f976-4829-9d64-2295add68d08-m1.avro  (7.2 KB)\n",
      "    ├── snap-4856791218193879087-1-e7631b72-f976-4829-9d64-2295add68d08.avro  (4.3 KB)\n",
      "    ├── snap-7119044451299626513-1-6eab82a3-1b88-44cf-98f8-88d44d924ea3.avro  (4.3 KB)\n",
      "    ├── snap-7999838354019313109-1-dd972ccc-b199-447e-8988-1ce3bbecd774.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.6 KB)\n",
      "    ├── v2.metadata.json  (2.7 KB)\n",
      "    ├── v3.metadata.json  (3.8 KB)\n",
      "    ├── v4.metadata.json  (4.9 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "Parquet 파일: 12개, 총 크기: 4,034,562 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COW 테이블 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(COW_PATH)\n",
    "\n",
    "cow_data_files = count_files(COW_PATH, ext=\".parquet\")\n",
    "cow_total = total_size(COW_PATH)\n",
    "\n",
    "print(f\"\\nParquet 파일: {cow_data_files}개, 총 크기: {cow_total:,} bytes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOR 테이블 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(MOR_PATH)\n",
    "\n",
    "mor_data_files = count_files(MOR_PATH, ext=\".parquet\")\n",
    "mor_total = total_size(MOR_PATH)\n",
    "\n",
    "print(f\"\\nParquet 파일: {mor_data_files}개, 총 크기: {mor_total:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ovb22bqcxa",
   "metadata": {},
   "source": [
    "### 데이터 파일 vs Delete File 비교\n",
    "\n",
    "`.files` 메타데이터 테이블로 **현재 활성 상태의 파일**을 비교합니다.  \n",
    "COW는 데이터 파일만, MOR는 데이터 파일 + Delete File이 혼재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vuybwhq2dan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW 활성 파일:\n",
      "  DATA: 3개 파일, 480000행, 3271.2 KB\n",
      "\n",
      "MOR 활성 파일:\n",
      "  DATA: 6개 파일, 550000행, 3759.3 KB\n",
      "  DELETES: 6개 파일, 70000행, 126.0 KB\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "                       COW        MOR\n",
      "──────────────────────────────────────────────────\n",
      "Data 파일                  3          6\n",
      "Delete 파일                0          6\n",
      "총 파일                     3         12\n",
      "──────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# COW — 활성 파일 통계\n",
    "cow_files = spark.sql(f\"\"\"\n",
    "    SELECT content,\n",
    "           COUNT(*) AS file_count,\n",
    "           SUM(record_count) AS total_records,\n",
    "           SUM(file_size_in_bytes) AS total_bytes\n",
    "    FROM {COW_TABLE}.files\n",
    "    GROUP BY content\n",
    "    ORDER BY content\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"COW 활성 파일:\")\n",
    "for row in cow_files:\n",
    "    content_type = \"DATA\" if row[\"content\"] == 0 else \"DELETES\"\n",
    "    print(f\"  {content_type}: {row['file_count']}개 파일, \"\n",
    "          f\"{row['total_records']}행, \"\n",
    "          f\"{row['total_bytes'] / 1024:.1f} KB\")\n",
    "\n",
    "# MOR — 활성 파일 통계\n",
    "mor_files = spark.sql(f\"\"\"\n",
    "    SELECT content,\n",
    "           COUNT(*) AS file_count,\n",
    "           SUM(record_count) AS total_records,\n",
    "           SUM(file_size_in_bytes) AS total_bytes\n",
    "    FROM {MOR_TABLE}.files\n",
    "    GROUP BY content\n",
    "    ORDER BY content\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"\\nMOR 활성 파일:\")\n",
    "for row in mor_files:\n",
    "    content_type = \"DATA\" if row[\"content\"] == 0 else \"DELETES\"\n",
    "    print(f\"  {content_type}: {row['file_count']}개 파일, \"\n",
    "          f\"{row['total_records']}행, \"\n",
    "          f\"{row['total_bytes'] / 1024:.1f} KB\")\n",
    "\n",
    "# 비교 요약\n",
    "cow_data_count = sum(r[\"file_count\"] for r in cow_files if r[\"content\"] == 0)\n",
    "cow_delete_count = sum(r[\"file_count\"] for r in cow_files if r[\"content\"] == 1)\n",
    "mor_data_count = sum(r[\"file_count\"] for r in mor_files if r[\"content\"] == 0)\n",
    "mor_delete_count = sum(r[\"file_count\"] for r in mor_files if r[\"content\"] == 1)\n",
    "\n",
    "print(f\"\\n{'─' * 50}\")\n",
    "print(f\"{'':15s} {'COW':>10s} {'MOR':>10s}\")\n",
    "print(f\"{'─' * 50}\")\n",
    "print(f\"{'Data 파일':15s} {cow_data_count:>10d} {mor_data_count:>10d}\")\n",
    "print(f\"{'Delete 파일':15s} {cow_delete_count:>10d} {mor_delete_count:>10d}\")\n",
    "print(f\"{'총 파일':15s} {cow_data_count + cow_delete_count:>10d} {mor_data_count + mor_delete_count:>10d}\")\n",
    "print(f\"{'─' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yhc396q2q5e",
   "metadata": {},
   "source": [
    "### 메타데이터 계층 비교\n",
    "\n",
    "`metadata.json → Manifest List → Manifest File → Data File` 체인을 통해  \n",
    "COW와 MOR이 **내부 메타데이터를 어떻게 다르게 구성하는지** 확인합니다.\n",
    "\n",
    "핵심 차이:\n",
    "- **COW**: DATA Manifest만 존재 (Delete File이 없으므로)\n",
    "- **MOR**: DATA Manifest + **DELETES Manifest**가 별도로 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uvnkevu6nzo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COW 메타데이터 계층:\n",
      "============================================================\n",
      "v4.metadata.json  (operation: overwrite)\n",
      "│\n",
      "└─▶ snap-2967345037190015042-1-7819d505-28d8-47a7-8484-68bff6ac517d.avro  [Manifest List]\n",
      "    ├─▶ 7819d505-28d8-47a7-8484-68bff6ac517d-m1.avro  [Manifest — DATA: 3 ADDED]\n",
      "        │   ├── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-03/00000-46-342c099a-7482-4b17-ba40-441433059299-00001.parquet  (163507행, ADDED)\n",
      "        │   ├── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-01/00001-47-342c099a-7482-4b17-ba40-441433059299-00001.parquet  (163392행, ADDED)\n",
      "        │   └── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-02/00002-48-342c099a-7482-4b17-ba40-441433059299-00001.parquet  (153101행, ADDED)\n",
      "    └─▶ 7819d505-28d8-47a7-8484-68bff6ac517d-m0.avro  [Manifest — DATA: 3 DELETED]\n",
      "            ├── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-03/00000-39-8762e0fa-f3ed-41ca-ac9e-4d6e7cccf513-00001.parquet  (170287행, DELETED)\n",
      "            ├── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-01/00001-40-8762e0fa-f3ed-41ca-ac9e-4d6e7cccf513-00001.parquet  (170214행, DELETED)\n",
      "            └── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-02/00002-41-8762e0fa-f3ed-41ca-ac9e-4d6e7cccf513-00001.parquet  (159499행, DELETED)\n",
      "\n",
      "============================================================\n",
      "MOR 메타데이터 계층:\n",
      "============================================================\n",
      "v4.metadata.json  (operation: overwrite)\n",
      "│\n",
      "└─▶ snap-7119044451299626513-1-6eab82a3-1b88-44cf-98f8-88d44d924ea3.avro  [Manifest List]\n",
      "    ├─▶ e7631b72-f976-4829-9d64-2295add68d08-m0.avro  [Manifest — DATA: 3 ADDED]\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-01/00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00001.parquet  (16965행, ADDED)\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-02/00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00002.parquet  (15921행, ADDED)\n",
      "        │   └── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-03/00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00003.parquet  (17114행, ADDED)\n",
      "    ├─▶ dd972ccc-b199-447e-8988-1ce3bbecd774-m0.avro  [Manifest — DATA: 3 ADDED]\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-03/00000-32-9a0e2130-dc45-41e3-a81e-9d93d6bf199a-00001.parquet  (170287행, ADDED)\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-01/00001-33-9a0e2130-dc45-41e3-a81e-9d93d6bf199a-00001.parquet  (170214행, ADDED)\n",
      "        │   └── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-02/00002-34-9a0e2130-dc45-41e3-a81e-9d93d6bf199a-00001.parquet  (159499행, ADDED)\n",
      "    ├─▶ 6eab82a3-1b88-44cf-98f8-88d44d924ea3-m0.avro  [Manifest — DELETES: empty]\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-01/00000-51-9d080ee6-a67e-4976-bbf2-087b1504ac4d-00001-deletes.parquet  (6822행, ADDED)  [DELETE FILE]\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-02/00000-51-9d080ee6-a67e-4976-bbf2-087b1504ac4d-00002-deletes.parquet  (6398행, ADDED)  [DELETE FILE]\n",
      "        │   └── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-03/00000-51-9d080ee6-a67e-4976-bbf2-087b1504ac4d-00003-deletes.parquet  (6780행, ADDED)  [DELETE FILE]\n",
      "    └─▶ e7631b72-f976-4829-9d64-2295add68d08-m1.avro  [Manifest — DELETES: empty]\n",
      "            ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-01/00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00001-deletes.parquet  (16965행, ADDED)  [DELETE FILE]\n",
      "            ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-02/00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00002-deletes.parquet  (15921행, ADDED)  [DELETE FILE]\n",
      "            └── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-03/00000-43-03d49cf7-14d7-4ca4-a516-7a105df5e95c-00003-deletes.parquet  (17114행, ADDED)  [DELETE FILE]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COW 메타데이터 계층:\")\n",
    "print(\"=\" * 60)\n",
    "show_metadata_hierarchy(COW_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOR 메타데이터 계층:\")\n",
    "print(\"=\" * 60)\n",
    "show_metadata_hierarchy(MOR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqm5o2qu8ho",
   "metadata": {},
   "source": [
    "### 메타데이터 구조 차이 정리\n",
    "\n",
    "```\n",
    "COW의 Manifest List:                   MOR의 Manifest List:\n",
    "├─▶ Manifest (DATA)  ← DELETE용       ├─▶ Manifest (DATA)      ← INSERT 원본\n",
    "│   └── 새 파일 (ADDED)                │   └── 원본 파일 (EXISTING)\n",
    "├─▶ Manifest (DATA)  ← UPDATE용       ├─▶ Manifest (DATA)      ← UPDATE 새 데이터\n",
    "│   └── 새 파일 (ADDED)                │   └── 새 파일 (ADDED)\n",
    "└─▶ Manifest (DATA)  ← INSERT 원본     ├─▶ Manifest (DELETES)   ← UPDATE용 Delete File\n",
    "    └── 원본 파일 (EXISTING/DELETED)    │   └── delete-*.parquet (ADDED)\n",
    "                                      └─▶ Manifest (DELETES)   ← DELETE용 Delete File\n",
    "                                           └── delete-*.parquet (ADDED)\n",
    "```\n",
    "\n",
    "| 특성 | COW | MOR |\n",
    "|------|-----|-----|\n",
    "| Manifest 종류 | DATA만 | DATA + DELETES |\n",
    "| 기존 파일 상태 | UPDATE/DELETE 시 DELETED로 표시 | EXISTING 유지 (삭제되지 않음) |\n",
    "| 새 파일 | 전체 데이터 재작성 파일 | 변경분 데이터 파일 + Delete File |\n",
    "| Manifest 크기 | 재작성된 파일 정보 포함 | 원본 유지 + 작은 추가 파일 정보 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc44f57-f6e6-4b22-a052-e3dc71643e51",
   "metadata": {},
   "source": [
    "### COW의 \"DELETED\" 상태 이해하기\n",
    "\n",
    "  Delete File vs DELETED 상태\n",
    "\n",
    "  COW는 Delete File을 사용하지 않습니다. 하지만 매니페스트에서 DELETED 상태가 보입니다. 이 둘은 완전히 다른 개념입니다.\n",
    "```\n",
    "  ┌─────────┬────────────────────────────────┬────────────────────────────────────────────────────────┐\n",
    "  │         │     Delete File (MOR 전용)     │             DELETED 상태 (Manifest entry)              │\n",
    "  ├─────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
    "  │ 정체    │ -deletes.parquet 파일          │ 매니페스트 엔트리의 상태 플래그                        │\n",
    "  ├─────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
    "  │ 의미    │ \"이 행들을 읽을 때 건너뛰어라\" │ \"이 데이터 파일은 더 이상 현재 스냅샷에 속하지 않는다\" │\n",
    "  ├─────────┼────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
    "  │ COW에서 │ 사용 안 함                     │ 사용함 — 재작성으로 대체된 옛 파일을 표시              │\n",
    "  └─────────┴────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "  COW가 UPDATE를 실행하면:\n",
    "  1. 기존 파티션 파일을 읽고, 변경 사항을 반영한 새 파일을 작성한다 → ADDED\n",
    "  2. 옛 파일은 새 파일로 대체되었으므로 → DELETED로 표시\n",
    "\n",
    "```\n",
    "  v3(UPDATE) manifest list:\n",
    "  ├─▶ m1.avro  [DATA: 3 ADDED]     ← 새로 재작성된 데이터 파일 3개\n",
    "  └─▶ m0.avro  [DATA: 3 DELETED]   ← 대체된 옛 데이터 파일 3개 (디스크에서 삭제되지는 않음)\n",
    "```\n",
    "\n",
    "  DELETED는 **\"이 파일이 테이블의 현재 뷰에서 제거됨\"**이라는 메타데이터 상태일 뿐, 디스크에서 물리 삭제되는 것이 아닙니다. 이전 스냅샷에서 Time Travel로 여전히 접근할 수 있습니다.\n",
    "\n",
    "  ---\n",
    "  metadata 디렉토리 vs 현재 스냅샷\n",
    "\n",
    "  metadata 디렉토리에는 모든 스냅샷의 manifest가 누적되어 있지만, 현재 스냅샷은 그 중 일부만 참조합니다.\n",
    "\n",
    "```\n",
    "  metadata 디렉토리 (전체):              현재 스냅샷(v4)이 참조하는 것:\n",
    "  ├── 33eff80a-m0.avro  ← v2(INSERT)용    (참조 안 함 — 이미 대체됨)\n",
    "  ├── 7819d505-m0.avro  ← v3(UPDATE)용    (참조 안 함 — 이미 대체됨)\n",
    "  ├── 7819d505-m1.avro  ← v3(UPDATE)용    (참조 안 함 — 이미 대체됨)\n",
    "  ├── 9c1c6bb8-m0.avro  ← v4(DELETE)용    ✅ 3 DELETED\n",
    "  └── 9c1c6bb8-m1.avro  ← v4(DELETE)용    ✅ 3 ADDED\n",
    "```\n",
    "\n",
    "  나머지 manifest 파일은 이전 스냅샷이 Time Travel용으로 참조하는 파일입니다.\n",
    "\n",
    "  ---\n",
    "  DELETED manifest가 \"하나\"인 이유\n",
    "\n",
    "  DELETED manifest 수는 교체 대상 파일이 몇 개의 manifest에 걸쳐 있었느냐에 따라 결정됩니다.\n",
    "\n",
    "  INSERT를 한 번에 했으므로, 3개 파티션 파일이 manifest 1개에 전부 들어 있었습니다:\n",
    "\n",
    "```\n",
    "  v2(INSERT):\n",
    "  └─▶ 33eff80a-m0.avro\n",
    "        ├── partition-01.parquet (ADDED)\n",
    "        ├── partition-02.parquet (ADDED)\n",
    "        └── partition-03.parquet (ADDED)\n",
    "```\n",
    "\n",
    "  UPDATE가 이 3개를 모두 교체할 때, 원본 manifest 1개만 재작성하면 됩니다:\n",
    "\n",
    "```\n",
    "  v3(UPDATE):\n",
    "  ├─▶ m1.avro  ← 새 파일 3개 (ADDED)\n",
    "  └─▶ m0.avro  ← 33eff80a-m0을 재작성: 옛 파일 3개 (DELETED)\n",
    "```\n",
    "\n",
    "  만약 INSERT를 3번에 나눠서 했다면, manifest가 3개 생기고, UPDATE 시 DELETED manifest도 3개가 됩니다:\n",
    "\n",
    "  만약 INSERT를 3번에 나눠 했다면:\n",
    "```\n",
    "  v3(UPDATE):\n",
    "  ├─▶ m3.avro  ← 새 파일 (ADDED)\n",
    "  ├─▶ m2.avro  ← 배치3 manifest 재작성 (DELETED)\n",
    "  ├─▶ m1.avro  ← 배치2 manifest 재작성 (DELETED)\n",
    "  └─▶ m0.avro  ← 배치1 manifest 재작성 (DELETED)\n",
    "```\n",
    "\n",
    "  DELETED manifest 수 = 교체 대상 파일이 흩어져 있던 manifest 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4",
   "metadata": {},
   "source": [
    "---\n",
    "## 종합 비교표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f3a4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "COW vs MOR 종합 비교 (500,000건, UPDATE 50,000건, DELETE 20,000건)\n",
      "=================================================================\n",
      "            항목     COW     MOR\n",
      " INSERT 시간 (초)   2.114   1.198\n",
      " UPDATE 시간 (초)   2.000   1.472\n",
      " DELETE 시간 (초)   2.313   1.769\n",
      " SELECT 시간 (초)   0.145   0.288\n",
      "  활성 Data 파일 수       3       6\n",
      "활성 Delete 파일 수       0       6\n",
      "     총 활성 파일 수       3      12\n",
      " 디스크 총 크기 (KB) 10157.0  3940.0\n",
      "      최종 레코드 수 480,000 480,000\n"
     ]
    }
   ],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    '항목': [\n",
    "        'INSERT 시간 (초)',\n",
    "        'UPDATE 시간 (초)',\n",
    "        'DELETE 시간 (초)',\n",
    "        'SELECT 시간 (초)',\n",
    "        '활성 Data 파일 수',\n",
    "        '활성 Delete 파일 수',\n",
    "        '총 활성 파일 수',\n",
    "        '디스크 총 크기 (KB)',\n",
    "        '최종 레코드 수',\n",
    "    ],\n",
    "    'COW': [\n",
    "        f\"{cow_insert_time:.3f}\",\n",
    "        f\"{cow_update_time:.3f}\",\n",
    "        f\"{cow_delete_time:.3f}\",\n",
    "        f\"{cow_read_time:.3f}\",\n",
    "        cow_data_count,\n",
    "        cow_delete_count,\n",
    "        cow_data_count + cow_delete_count,\n",
    "        f\"{cow_total / 1024:.1f}\",\n",
    "        f\"{cow_count:,}\",\n",
    "    ],\n",
    "    'MOR': [\n",
    "        f\"{mor_insert_time:.3f}\",\n",
    "        f\"{mor_update_time:.3f}\",\n",
    "        f\"{mor_delete_time:.3f}\",\n",
    "        f\"{mor_read_time:.3f}\",\n",
    "        mor_data_count,\n",
    "        mor_delete_count,\n",
    "        mor_data_count + mor_delete_count,\n",
    "        f\"{mor_total / 1024:.1f}\",\n",
    "        f\"{mor_count:,}\",\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"COW vs MOR 종합 비교 ({NUM_RECORDS:,}건, UPDATE {UPDATE_COUNT:,}건, DELETE {DELETE_COUNT:,}건)\")\n",
    "print(\"=\" * 65)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4b5c6",
   "metadata": {},
   "source": [
    "---\n",
    "## 비교 결과 분석\n",
    "\n",
    "### 1. INSERT — 차이 없음\n",
    "INSERT는 새 데이터 파일을 생성하는 동일한 동작입니다.  \n",
    "COW/MOR 전략은 **기존 데이터를 수정할 때만** 개입합니다.\n",
    "\n",
    "### 2. UPDATE/DELETE — 쓰기 성능에서 MOR 우위\n",
    "| 관점 | COW | MOR |\n",
    "|------|-----|-----|\n",
    "| **쓰기 I/O** | 50,000건 변경 → 500,000건 분량 재작성 (10x 증폭) | 50,000건 분량만 기록 |\n",
    "| **파일 생성** | 기존 파일 크기와 비슷한 새 파일 (수 MB) | 작은 Delete File (~KB) + 변경분 데이터 |\n",
    "| **메타데이터** | 기존 파일 DELETED → 새 파일 ADDED | 기존 파일 유지 + Delete File ADDED |\n",
    "\n",
    "**핵심 인사이트**: 데이터 파일이 수 MB 규모가 되면, COW의 **쓰기 증폭(write amplification)** 비용이 실행 시간에 명확히 반영됩니다.  \n",
    "프로덕션에서 파일이 수백 MB ~ 수 GB인 경우 이 차이는 **수십 배**로 벌어집니다.\n",
    "\n",
    "### 3. SELECT — 읽기 성능에서 COW 우위\n",
    "COW 테이블은 항상 최신 상태의 데이터 파일만 존재하므로, 추가 병합 없이 읽을 수 있습니다.  \n",
    "MOR 테이블은 Delete File을 데이터 파일과 병합해야 하므로, Delete File이 누적될수록 읽기 비용이 증가합니다.\n",
    "\n",
    "### 4. 파일 구조 — 근본적 차이\n",
    "```\n",
    "COW:  데이터 변경 → 파일 재작성 → 항상 \"깨끗한\" 데이터 파일만 존재\n",
    "MOR:  데이터 변경 → Delete File 추가 → 시간이 지나면 Delete File 누적 → 컴팩션 필요\n",
    "```\n",
    "\n",
    "> **트레이드오프 요약**: COW는 **쓰기 시 비용을 지불**하고, MOR는 **읽기 시 비용을 지불**합니다.  \n",
    "> MOR의 읽기 비용은 **컴팩션으로 관리**할 수 있으므로, 변경이 빈번한 워크로드에서는 MOR이 유리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7",
   "metadata": {},
   "source": [
    "---\n",
    "## 선택 기준 가이드\n",
    "\n",
    "| 워크로드 | 추천 | 이유 |\n",
    "|---------|------|------|\n",
    "| 읽기 중심, 수정 적음 | **COW** | 읽기 최적화, Delete File 없음 |\n",
    "| 쓰기 중심, 수정 빈번 | **MOR** | 쓰기 빠름, 정기 컴팩션으로 읽기 비용 관리 |\n",
    "| 대용량 파티션 + 소량 수정 | **MOR** | COW의 쓰기 증폭이 극대화되는 시나리오 |\n",
    "| 실시간 분석 (짧은 지연 허용 불가) | **COW** | 읽기 시 병합 비용 없음 |\n",
    "| 혼합 워크로드 | **작업별 혼합 설정** | DELETE/UPDATE/MERGE를 개별 설정 가능 |\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "- Iceberg에서는 **테이블 속성을 통해 작업별로 다른 전략을 설정**할 수 있습니다\n",
    "  ```sql\n",
    "  -- DELETE는 MOR, UPDATE는 COW로 혼합 설정\n",
    "  ALTER TABLE my_table SET TBLPROPERTIES (\n",
    "      'write.delete.mode'='merge-on-read',\n",
    "      'write.update.mode'='copy-on-write'\n",
    "  )\n",
    "  ```\n",
    "- MOR 테이블은 Delete File이 누적되므로, **정기적인 컴팩션(compaction)**이 필요합니다\n",
    "- 컴팩션은 Delete File을 데이터 파일에 병합하여 읽기 성능을 회복시킵니다\n",
    "- 프로덕션에서는 **Spark의 `rewrite_data_files`** 프로시저로 컴팩션을 수행합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d7e8",
   "metadata": {},
   "source": [
    "---\n",
    "## 다음 단계\n",
    "\n",
    "> MOR 테이블의 읽기 성능을 회복하려면 **정기적인 컴팩션**이 필요합니다.  \n",
    "> 이 내용은 **4_optimization** 모듈에서 자세히 다룹니다.\n",
    ">\n",
    "> 컴팩션을 통해 Delete File을 데이터 파일에 병합하면,  \n",
    "> MOR의 쓰기 장점을 유지하면서 읽기 성능도 관리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6d7e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
