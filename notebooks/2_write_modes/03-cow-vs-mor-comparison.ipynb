{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "# COW vs MOR 직접 비교 실험\n",
    "\n",
    "동일한 데이터와 동일한 작업을 COW/MOR 테이블에서 각각 수행하고,  \n",
    "**실행 시간, 파일 구조, 메타데이터 계층**을 정량적으로 비교합니다.\n",
    "\n",
    "COW와 MOR은 **쓰기 방식만 다를 뿐, 동일한 Iceberg 테이블 구조**를 사용합니다.  \n",
    "이 실험을 통해 두 전략이 **파일 시스템 수준에서 어떻게 다르게 동작하는지** 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "## 실험 설계\n",
    "\n",
    "| 단계 | 작업 | 데이터 규모 | 설명 |\n",
    "|------|------|------------|------|\n",
    "| 0 | Warmup | — | JVM/Iceberg 초기화 비용 제거 |\n",
    "| 1 | INSERT | 2,000건 | 초기 데이터 적재 (COW/MOR 동일) |\n",
    "| 2 | UPDATE | 600건 (30%) | 상태 변경 → COW는 파일 재작성, MOR는 Delete File + 새 데이터 |\n",
    "| 3 | DELETE | 200건 (10%) | 삭제 → COW는 파일 재작성, MOR는 Delete File만 추가 |\n",
    "| 4 | SELECT | 전체 | 읽기 → COW는 데이터만, MOR는 Delete File 병합 필요 |\n",
    "\n",
    "동일한 시나리오를 COW와 MOR 양쪽에서 실행하여 차이를 관찰합니다.\n",
    "\n",
    "> **Warmup**: Spark/JVM은 첫 작업에서 클래스 로딩, JIT 컴파일 등의 초기화 비용이 발생합니다.  \n",
    "> 먼저 실행되는 쪽이 불리해지는 것을 방지하기 위해, 별도의 테이블에 dummy 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e4f5",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e4f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, count_files, total_size, show_metadata_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f5a6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "COW_TABLE = \"demo.lab.cmp_cow_orders\"\n",
    "MOR_TABLE = \"demo.lab.cmp_mor_orders\"\n",
    "COW_PATH = \"/home/jovyan/data/warehouse/lab/cmp_cow_orders\"\n",
    "MOR_PATH = \"/home/jovyan/data/warehouse/lab/cmp_mor_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6b7c8",
   "metadata": {},
   "source": [
    "## 테이블 생성\n",
    "\n",
    "동일한 스키마, 동일한 파티셔닝 — **TBLPROPERTIES만 다릅니다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b7c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW 테이블 생성 완료: demo.lab.cmp_cow_orders\n",
      "MOR 테이블 생성 완료: demo.lab.cmp_mor_orders\n",
      "JVM Warmup 완료 — 이후 측정은 초기화 비용이 제거된 상태\n"
     ]
    }
   ],
   "source": [
    "# 기존 테이블 정리\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {COW_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {MOR_TABLE}\")\n",
    "\n",
    "# COW 테이블\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {COW_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.delete.mode'='copy-on-write',\n",
    "    'write.update.mode'='copy-on-write',\n",
    "    'write.merge.mode'='copy-on-write'\n",
    ")\n",
    "\"\"\")\n",
    "print(f\"COW 테이블 생성 완료: {COW_TABLE}\")\n",
    "\n",
    "# MOR 테이블\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {MOR_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.delete.mode'='merge-on-read',\n",
    "    'write.update.mode'='merge-on-read',\n",
    "    'write.merge.mode'='merge-on-read'\n",
    ")\n",
    "\"\"\")\n",
    "print(f\"MOR 테이블 생성 완료: {MOR_TABLE}\")\n",
    "\n",
    "# ── JVM Warmup ──\n",
    "# 별도 dummy 테이블에 INSERT → UPDATE → DELETE → DROP 수행하여\n",
    "# Spark/JVM/Iceberg 초기화 비용을 미리 소모합니다.\n",
    "WARMUP_TABLE = \"demo.lab.cmp_warmup\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {WARMUP_TABLE}\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {WARMUP_TABLE} (id BIGINT, val STRING)\n",
    "USING ICEBERG\n",
    "\"\"\")\n",
    "warmup_df = spark.createDataFrame([(i, \"x\") for i in range(100)], [\"id\", \"val\"])\n",
    "warmup_df.writeTo(WARMUP_TABLE).append()\n",
    "spark.sql(f\"UPDATE {WARMUP_TABLE} SET val = 'y' WHERE id < 10\")\n",
    "spark.sql(f\"DELETE FROM {WARMUP_TABLE} WHERE id >= 90\")\n",
    "spark.sql(f\"SELECT COUNT(*) FROM {WARMUP_TABLE}\").collect()\n",
    "spark.sql(f\"DROP TABLE {WARMUP_TABLE}\")\n",
    "print(\"JVM Warmup 완료 — 이후 측정은 초기화 비용이 제거된 상태\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8d9e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 1: INSERT 2,000건\n",
    "\n",
    "동일한 데이터(seed=42)를 양쪽 테이블에 삽입합니다.  \n",
    "INSERT는 COW/MOR 모두 **새 데이터 파일을 생성**하는 동일한 동작이므로, 시간 차이가 거의 없어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d9e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW INSERT 시간: 0.494초\n",
      "MOR INSERT 시간: 0.312초\n",
      "\n",
      "양쪽 테이블 모두 2000건 삽입 완료\n",
      "INSERT 시간 차이: 0.182초 (거의 동일해야 정상)\n"
     ]
    }
   ],
   "source": [
    "# 동일한 데이터 생성 (2,000건)\n",
    "orders = generate_orders(num_records=2000, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.cache()\n",
    "df.count()  # 캐시 실체화\n",
    "\n",
    "# COW INSERT\n",
    "start = time.time()\n",
    "df.writeTo(COW_TABLE).append()\n",
    "cow_insert_time = time.time() - start\n",
    "print(f\"COW INSERT 시간: {cow_insert_time:.3f}초\")\n",
    "\n",
    "# MOR INSERT\n",
    "start = time.time()\n",
    "df.writeTo(MOR_TABLE).append()\n",
    "mor_insert_time = time.time() - start\n",
    "print(f\"MOR INSERT 시간: {mor_insert_time:.3f}초\")\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "cow_count_after_insert = spark.sql(f\"SELECT COUNT(*) FROM {COW_TABLE}\").collect()[0][0]\n",
    "print(f\"\\n양쪽 테이블 모두 {cow_count_after_insert}건 삽입 완료\")\n",
    "print(f\"INSERT 시간 차이: {abs(cow_insert_time - mor_insert_time):.3f}초 (거의 동일해야 정상)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0f1a2",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — INSERT\n",
    "\n",
    "INSERT는 COW/MOR 차이가 없습니다. 둘 다 **새 데이터 파일을 생성**할 뿐이므로, Warmup 이후에는 시간이 거의 동일합니다.\n",
    "\n",
    "- COW: 새 Parquet 파일 생성\n",
    "- MOR: 새 Parquet 파일 생성\n",
    "- **차이점 없음** — 기존 데이터를 수정하지 않으므로 전략이 개입하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1a2b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 2: UPDATE 600건 (전체의 30%)\n",
    "\n",
    "order_id 기준으로 처음 600건의 상태를 변경합니다.  \n",
    "**여기서부터 COW와 MOR의 차이가 드러납니다.**\n",
    "\n",
    "- **COW**: 변경된 행이 속한 파티션의 데이터 파일을 **통째로 재작성** (영향받지 않은 행도 다시 씀)\n",
    "- **MOR**: 기존 행을 가리키는 **Positional Delete File** + 변경된 값의 **새 데이터 파일**만 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW UPDATE 시간: 0.537초\n",
      "MOR UPDATE 시간: 0.634초\n",
      "\n",
      "COW/MOR 비율: 0.85x — COW가 0.8배 느림\n"
     ]
    }
   ],
   "source": [
    "# COW UPDATE\n",
    "start = time.time()\n",
    "spark.sql(f\"UPDATE {COW_TABLE} SET status = 'refunded' WHERE order_id <= 600\")\n",
    "cow_update_time = time.time() - start\n",
    "print(f\"COW UPDATE 시간: {cow_update_time:.3f}초\")\n",
    "\n",
    "# MOR UPDATE\n",
    "start = time.time()\n",
    "spark.sql(f\"UPDATE {MOR_TABLE} SET status = 'refunded' WHERE order_id <= 600\")\n",
    "mor_update_time = time.time() - start\n",
    "print(f\"MOR UPDATE 시간: {mor_update_time:.3f}초\")\n",
    "\n",
    "ratio = cow_update_time / mor_update_time if mor_update_time > 0 else float('inf')\n",
    "print(f\"\\nCOW/MOR 비율: {ratio:.2f}x — COW가 {ratio:.1f}배 느림\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — UPDATE\n",
    "\n",
    "| | COW | MOR |\n",
    "|---|---|---|\n",
    "| **동작** | 영향받은 파티션의 데이터 파일을 통째로 재작성 | Delete File + 새 데이터 파일만 추가 |\n",
    "| **I/O** | 변경 안 된 행도 다시 써야 함 → **쓰기 증폭** | 변경된 행만큼만 기록 → 최소 I/O |\n",
    "| **결과 파일** | 기존 파일 DELETED → 새 파일 ADDED | 기존 파일 유지 + Delete File ADDED + 새 데이터 ADDED |\n",
    "\n",
    "> 600건(30%) UPDATE에서 COW가 느린 이유:  \n",
    "> 3개 파티션 모두 영향을 받으므로 **모든 파티션의 데이터 파일을 재작성**해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 3: DELETE 200건 (전체의 10%)\n",
    "\n",
    "order_id 601~800번 주문을 삭제합니다.\n",
    "\n",
    "- **COW**: 해당 행이 빠진 **새 데이터 파일을 재작성**\n",
    "- **MOR**: 삭제 위치만 기록한 **Positional Delete File**만 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d5e6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW DELETE 시간: 0.531초\n",
      "MOR DELETE 시간: 0.352초\n",
      "\n",
      "COW/MOR 비율: 1.51x — COW가 1.5배 느림\n"
     ]
    }
   ],
   "source": [
    "# COW DELETE\n",
    "start = time.time()\n",
    "spark.sql(f\"DELETE FROM {COW_TABLE} WHERE order_id BETWEEN 601 AND 800\")\n",
    "cow_delete_time = time.time() - start\n",
    "print(f\"COW DELETE 시간: {cow_delete_time:.3f}초\")\n",
    "\n",
    "# MOR DELETE\n",
    "start = time.time()\n",
    "spark.sql(f\"DELETE FROM {MOR_TABLE} WHERE order_id BETWEEN 601 AND 800\")\n",
    "mor_delete_time = time.time() - start\n",
    "print(f\"MOR DELETE 시간: {mor_delete_time:.3f}초\")\n",
    "\n",
    "ratio = cow_delete_time / mor_delete_time if mor_delete_time > 0 else float('inf')\n",
    "print(f\"\\nCOW/MOR 비율: {ratio:.2f}x — COW가 {ratio:.1f}배 느림\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f7a8",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — DELETE\n",
    "\n",
    "| | COW | MOR |\n",
    "|---|---|---|\n",
    "| **동작** | 삭제 행이 빠진 새 파일 재작성 | Positional Delete File만 추가 |\n",
    "| **I/O** | UPDATE와 마찬가지로 **전체 파일 재작성** | 삭제 위치(file_path + pos)만 기록 → **매우 작은 I/O** |\n",
    "| **결과** | 기존 데이터 파일 DELETED → 새 파일 ADDED | 기존 데이터 파일 유지 + Delete File ADDED |\n",
    "\n",
    "> DELETE는 UPDATE보다 MOR의 이점이 더 큽니다.  \n",
    "> UPDATE는 새 값을 기록한 데이터 파일이 필요하지만, DELETE는 **Delete File만으로 충분**합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7a8b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 4: SELECT — 읽기 성능 비교\n",
    "\n",
    "전체 데이터를 읽는 시간을 비교합니다.\n",
    "\n",
    "- **COW**: 데이터 파일만 읽으면 됨 (이미 최신 상태)\n",
    "- **MOR**: 데이터 파일 + Delete File을 **병합(merge)**해야 정확한 결과를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a8b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW SELECT 시간: 0.062초 (레코드 수: 1800)\n",
      "MOR SELECT 시간: 0.136초 (레코드 수: 1800)\n",
      "\n",
      "MOR/COW 비율: 2.19x\n",
      "레코드 수 일치 여부: True (1800건)\n"
     ]
    }
   ],
   "source": [
    "# COW SELECT\n",
    "start = time.time()\n",
    "cow_count = spark.sql(f\"SELECT COUNT(*) FROM {COW_TABLE}\").collect()[0][0]\n",
    "cow_read_time = time.time() - start\n",
    "print(f\"COW SELECT 시간: {cow_read_time:.3f}초 (레코드 수: {cow_count})\")\n",
    "\n",
    "# MOR SELECT\n",
    "start = time.time()\n",
    "mor_count = spark.sql(f\"SELECT COUNT(*) FROM {MOR_TABLE}\").collect()[0][0]\n",
    "mor_read_time = time.time() - start\n",
    "print(f\"MOR SELECT 시간: {mor_read_time:.3f}초 (레코드 수: {mor_count})\")\n",
    "\n",
    "ratio = mor_read_time / cow_read_time if cow_read_time > 0 else float('inf')\n",
    "print(f\"\\nMOR/COW 비율: {ratio:.2f}x\")\n",
    "print(f\"레코드 수 일치 여부: {cow_count == mor_count} ({cow_count}건)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9c0d1",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — SELECT\n",
    "\n",
    "| | COW | MOR |\n",
    "|---|---|---|\n",
    "| **읽기 대상** | 데이터 파일만 | 데이터 파일 + Delete File 병합 |\n",
    "| **Merge 비용** | 없음 | Delete File 기반 필터링 필요 |\n",
    "| **특성** | 쓰기 시 이미 최적화 완료 | 읽기 시 병합 비용 발생 |\n",
    "\n",
    "> 소규모 데이터에서는 읽기 차이가 미미하지만, Delete File이 누적될수록 MOR의 읽기 비용이 증가합니다.  \n",
    "> 이를 해결하기 위해 **정기적인 컴팩션(compaction)**이 필요합니다 (→ 4_optimization 모듈)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 파일 시스템 비교\n",
    "\n",
    "시간 측정만으로는 **왜** 차이가 나는지 알기 어렵습니다.  \n",
    "파일 시스템 수준에서 **데이터 파일 수, Delete File 유무, 메타데이터 계층**을 직접 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0d1e2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COW 테이블 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-24-f9838e57-9090-41d5-99f0-a749a587d9ac-00001.parquet  (7.3 KB)\n",
      "│   │   ├── 00000-35-9ba3c68f-d801-4864-afc5-91950d1755d0-00001.parquet  (7.3 KB)\n",
      "│   │   └── 00000-40-e3e46849-f884-450a-9d73-1f9ff95f495b-00001.parquet  (6.6 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-24-f9838e57-9090-41d5-99f0-a749a587d9ac-00002.parquet  (7.6 KB)\n",
      "│   │   ├── 00000-35-9ba3c68f-d801-4864-afc5-91950d1755d0-00002.parquet  (7.5 KB)\n",
      "│   │   └── 00000-40-e3e46849-f884-450a-9d73-1f9ff95f495b-00002.parquet  (7.0 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-24-f9838e57-9090-41d5-99f0-a749a587d9ac-00003.parquet  (8.4 KB)\n",
      "│       ├── 00000-35-9ba3c68f-d801-4864-afc5-91950d1755d0-00003.parquet  (8.3 KB)\n",
      "│       └── 00000-40-e3e46849-f884-450a-9d73-1f9ff95f495b-00003.parquet  (7.9 KB)\n",
      "└── metadata/\n",
      "    ├── 42f7346d-c6d9-4980-9f98-a5879f9c5cb8-m0.avro  (7.3 KB)\n",
      "    ├── 42f7346d-c6d9-4980-9f98-a5879f9c5cb8-m1.avro  (7.3 KB)\n",
      "    ├── b8fd46a8-e2ea-4eb6-89a4-fb92b5c91488-m0.avro  (7.3 KB)\n",
      "    ├── b8fd46a8-e2ea-4eb6-89a4-fb92b5c91488-m1.avro  (7.3 KB)\n",
      "    ├── f3fd6819-07f6-46a2-9ac6-55d9c21b61ff-m0.avro  (7.3 KB)\n",
      "    ├── snap-1571425460755231041-1-f3fd6819-07f6-46a2-9ac6-55d9c21b61ff.avro  (4.2 KB)\n",
      "    ├── snap-4813404451811166586-1-b8fd46a8-e2ea-4eb6-89a4-fb92b5c91488.avro  (4.2 KB)\n",
      "    ├── snap-5278229954013775918-1-42f7346d-c6d9-4980-9f98-a5879f9c5cb8.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.6 KB)\n",
      "    ├── v2.metadata.json  (2.7 KB)\n",
      "    ├── v3.metadata.json  (3.8 KB)\n",
      "    ├── v4.metadata.json  (4.9 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "Parquet 파일: 9개, 총 크기: 133,147 bytes\n",
      "\n",
      "============================================================\n",
      "MOR 테이블 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-30-f9da3f74-ea0c-4b82-b180-93a639b95a99-00001.parquet  (7.3 KB)\n",
      "│   │   ├── 00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00001-deletes.parquet  (1.9 KB)\n",
      "│   │   ├── 00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00001.parquet  (3.6 KB)\n",
      "│   │   └── 00000-43-728740ec-3e50-47ef-a9f5-e9e977a4c213-00001-deletes.parquet  (1.8 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-30-f9da3f74-ea0c-4b82-b180-93a639b95a99-00002.parquet  (7.6 KB)\n",
      "│   │   ├── 00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00002-deletes.parquet  (1.9 KB)\n",
      "│   │   ├── 00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00002.parquet  (3.7 KB)\n",
      "│   │   └── 00000-43-728740ec-3e50-47ef-a9f5-e9e977a4c213-00002-deletes.parquet  (1.8 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-30-f9da3f74-ea0c-4b82-b180-93a639b95a99-00003.parquet  (8.4 KB)\n",
      "│       ├── 00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00003-deletes.parquet  (1.9 KB)\n",
      "│       ├── 00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00003.parquet  (3.8 KB)\n",
      "│       └── 00000-43-728740ec-3e50-47ef-a9f5-e9e977a4c213-00003-deletes.parquet  (1.8 KB)\n",
      "└── metadata/\n",
      "    ├── 334bf2e7-5367-4db5-b86c-773e2ccdbe1a-m0.avro  (7.2 KB)\n",
      "    ├── 6994017d-7aaf-452a-84cc-8c7be8e1244a-m0.avro  (7.3 KB)\n",
      "    ├── fd7c1b09-ffdd-4a74-8091-62789a85c97e-m0.avro  (7.3 KB)\n",
      "    ├── fd7c1b09-ffdd-4a74-8091-62789a85c97e-m1.avro  (7.2 KB)\n",
      "    ├── snap-6250413271874140591-1-6994017d-7aaf-452a-84cc-8c7be8e1244a.avro  (4.2 KB)\n",
      "    ├── snap-626839564066066432-1-334bf2e7-5367-4db5-b86c-773e2ccdbe1a.avro  (4.3 KB)\n",
      "    ├── snap-7242954873185558123-1-fd7c1b09-ffdd-4a74-8091-62789a85c97e.avro  (4.3 KB)\n",
      "    ├── v1.metadata.json  (1.6 KB)\n",
      "    ├── v2.metadata.json  (2.7 KB)\n",
      "    ├── v3.metadata.json  (3.8 KB)\n",
      "    ├── v4.metadata.json  (4.8 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "Parquet 파일: 12개, 총 크기: 102,591 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COW 테이블 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(COW_PATH)\n",
    "\n",
    "cow_data_files = count_files(COW_PATH, ext=\".parquet\")\n",
    "cow_total = total_size(COW_PATH)\n",
    "\n",
    "print(f\"\\nParquet 파일: {cow_data_files}개, 총 크기: {cow_total:,} bytes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOR 테이블 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(MOR_PATH)\n",
    "\n",
    "mor_data_files = count_files(MOR_PATH, ext=\".parquet\")\n",
    "mor_total = total_size(MOR_PATH)\n",
    "\n",
    "print(f\"\\nParquet 파일: {mor_data_files}개, 총 크기: {mor_total:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ovb22bqcxa",
   "metadata": {},
   "source": [
    "### 데이터 파일 vs Delete File 비교\n",
    "\n",
    "`.files` 메타데이터 테이블로 **현재 활성 상태의 파일**을 비교합니다.  \n",
    "COW는 데이터 파일만, MOR는 데이터 파일 + Delete File이 혼재합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vuybwhq2dan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COW 활성 파일:\n",
      "  DATA: 3개 파일, 1800행, 21.5 KB\n",
      "\n",
      "MOR 활성 파일:\n",
      "  DATA: 6개 파일, 2600행, 34.5 KB\n",
      "  DELETES: 6개 파일, 800행, 11.1 KB\n",
      "\n",
      "──────────────────────────────────────────────────\n",
      "                       COW        MOR\n",
      "──────────────────────────────────────────────────\n",
      "Data 파일                  3          6\n",
      "Delete 파일                0          6\n",
      "총 파일                     3         12\n",
      "──────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "# COW — 활성 파일 통계\n",
    "cow_files = spark.sql(f\"\"\"\n",
    "    SELECT content,\n",
    "           COUNT(*) AS file_count,\n",
    "           SUM(record_count) AS total_records,\n",
    "           SUM(file_size_in_bytes) AS total_bytes\n",
    "    FROM {COW_TABLE}.files\n",
    "    GROUP BY content\n",
    "    ORDER BY content\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"COW 활성 파일:\")\n",
    "for row in cow_files:\n",
    "    content_type = \"DATA\" if row[\"content\"] == 0 else \"DELETES\"\n",
    "    print(f\"  {content_type}: {row['file_count']}개 파일, \"\n",
    "          f\"{row['total_records']}행, \"\n",
    "          f\"{row['total_bytes'] / 1024:.1f} KB\")\n",
    "\n",
    "# MOR — 활성 파일 통계\n",
    "mor_files = spark.sql(f\"\"\"\n",
    "    SELECT content,\n",
    "           COUNT(*) AS file_count,\n",
    "           SUM(record_count) AS total_records,\n",
    "           SUM(file_size_in_bytes) AS total_bytes\n",
    "    FROM {MOR_TABLE}.files\n",
    "    GROUP BY content\n",
    "    ORDER BY content\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"\\nMOR 활성 파일:\")\n",
    "for row in mor_files:\n",
    "    content_type = \"DATA\" if row[\"content\"] == 0 else \"DELETES\"\n",
    "    print(f\"  {content_type}: {row['file_count']}개 파일, \"\n",
    "          f\"{row['total_records']}행, \"\n",
    "          f\"{row['total_bytes'] / 1024:.1f} KB\")\n",
    "\n",
    "# 비교 요약\n",
    "cow_data_count = sum(r[\"file_count\"] for r in cow_files if r[\"content\"] == 0)\n",
    "cow_delete_count = sum(r[\"file_count\"] for r in cow_files if r[\"content\"] == 1)\n",
    "mor_data_count = sum(r[\"file_count\"] for r in mor_files if r[\"content\"] == 0)\n",
    "mor_delete_count = sum(r[\"file_count\"] for r in mor_files if r[\"content\"] == 1)\n",
    "\n",
    "print(f\"\\n{'─' * 50}\")\n",
    "print(f\"{'':15s} {'COW':>10s} {'MOR':>10s}\")\n",
    "print(f\"{'─' * 50}\")\n",
    "print(f\"{'Data 파일':15s} {cow_data_count:>10d} {mor_data_count:>10d}\")\n",
    "print(f\"{'Delete 파일':15s} {cow_delete_count:>10d} {mor_delete_count:>10d}\")\n",
    "print(f\"{'총 파일':15s} {cow_data_count + cow_delete_count:>10d} {mor_data_count + mor_delete_count:>10d}\")\n",
    "print(f\"{'─' * 50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yhc396q2q5e",
   "metadata": {},
   "source": [
    "### 메타데이터 계층 비교\n",
    "\n",
    "`metadata.json → Manifest List → Manifest File → Data File` 체인을 통해  \n",
    "COW와 MOR이 **내부 메타데이터를 어떻게 다르게 구성하는지** 확인합니다.\n",
    "\n",
    "핵심 차이:\n",
    "- **COW**: DATA Manifest만 존재 (Delete File이 없으므로)\n",
    "- **MOR**: DATA Manifest + **DELETES Manifest**가 별도로 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uvnkevu6nzo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COW 메타데이터 계층:\n",
      "============================================================\n",
      "v4.metadata.json  (operation: overwrite)\n",
      "│\n",
      "└─▶ snap-4813404451811166586-1-b8fd46a8-e2ea-4eb6-89a4-fb92b5c91488.avro  [Manifest List]\n",
      "    ├─▶ b8fd46a8-e2ea-4eb6-89a4-fb92b5c91488-m1.avro  [Manifest — DATA: 3 ADDED]\n",
      "        │   ├── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-01/00000-40-e3e46849-f884-450a-9d73-1f9ff95f495b-00001.parquet  (540행, ADDED)\n",
      "        │   ├── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-02/00000-40-e3e46849-f884-450a-9d73-1f9ff95f495b-00002.parquet  (576행, ADDED)\n",
      "        │   └── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-03/00000-40-e3e46849-f884-450a-9d73-1f9ff95f495b-00003.parquet  (684행, ADDED)\n",
      "    └─▶ b8fd46a8-e2ea-4eb6-89a4-fb92b5c91488-m0.avro  [Manifest — DATA: 3 DELETED]\n",
      "            ├── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-01/00000-35-9ba3c68f-d801-4864-afc5-91950d1755d0-00001.parquet  (610행, DELETED)\n",
      "            ├── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-02/00000-35-9ba3c68f-d801-4864-afc5-91950d1755d0-00002.parquet  (643행, DELETED)\n",
      "            └── data/warehouse/lab/cmp_cow_orders/data/order_date_month=2024-03/00000-35-9ba3c68f-d801-4864-afc5-91950d1755d0-00003.parquet  (747행, DELETED)\n",
      "\n",
      "============================================================\n",
      "MOR 메타데이터 계층:\n",
      "============================================================\n",
      "v4.metadata.json  (operation: overwrite)\n",
      "│\n",
      "└─▶ snap-626839564066066432-1-334bf2e7-5367-4db5-b86c-773e2ccdbe1a.avro  [Manifest List]\n",
      "    ├─▶ fd7c1b09-ffdd-4a74-8091-62789a85c97e-m0.avro  [Manifest — DATA: 3 ADDED]\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-01/00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00001.parquet  (185행, ADDED)\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-02/00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00002.parquet  (200행, ADDED)\n",
      "        │   └── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-03/00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00003.parquet  (215행, ADDED)\n",
      "    ├─▶ 6994017d-7aaf-452a-84cc-8c7be8e1244a-m0.avro  [Manifest — DATA: 3 ADDED]\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-01/00000-30-f9da3f74-ea0c-4b82-b180-93a639b95a99-00001.parquet  (610행, ADDED)\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-02/00000-30-f9da3f74-ea0c-4b82-b180-93a639b95a99-00002.parquet  (643행, ADDED)\n",
      "        │   └── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-03/00000-30-f9da3f74-ea0c-4b82-b180-93a639b95a99-00003.parquet  (747행, ADDED)\n",
      "    ├─▶ 334bf2e7-5367-4db5-b86c-773e2ccdbe1a-m0.avro  [Manifest — DELETES: empty]\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-01/00000-43-728740ec-3e50-47ef-a9f5-e9e977a4c213-00001-deletes.parquet  (70행, ADDED)  [DELETE FILE]\n",
      "        │   ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-02/00000-43-728740ec-3e50-47ef-a9f5-e9e977a4c213-00002-deletes.parquet  (67행, ADDED)  [DELETE FILE]\n",
      "        │   └── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-03/00000-43-728740ec-3e50-47ef-a9f5-e9e977a4c213-00003-deletes.parquet  (63행, ADDED)  [DELETE FILE]\n",
      "    └─▶ fd7c1b09-ffdd-4a74-8091-62789a85c97e-m1.avro  [Manifest — DELETES: empty]\n",
      "            ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-01/00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00001-deletes.parquet  (185행, ADDED)  [DELETE FILE]\n",
      "            ├── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-02/00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00002-deletes.parquet  (200행, ADDED)  [DELETE FILE]\n",
      "            └── data/warehouse/lab/cmp_mor_orders/data/order_date_month=2024-03/00000-37-33a44e48-b160-45c4-a4c0-77ed7f88533f-00003-deletes.parquet  (215행, ADDED)  [DELETE FILE]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COW 메타데이터 계층:\")\n",
    "print(\"=\" * 60)\n",
    "show_metadata_hierarchy(COW_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOR 메타데이터 계층:\")\n",
    "print(\"=\" * 60)\n",
    "show_metadata_hierarchy(MOR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqm5o2qu8ho",
   "metadata": {},
   "source": [
    "### 메타데이터 구조 차이 정리\n",
    "\n",
    "```\n",
    "COW의 Manifest List:                   MOR의 Manifest List:\n",
    "├─▶ Manifest (DATA)  ← DELETE용       ├─▶ Manifest (DATA)      ← INSERT 원본\n",
    "│   └── 새 파일 (ADDED)               │   └── 원본 파일 (EXISTING)\n",
    "├─▶ Manifest (DATA)  ← UPDATE용       ├─▶ Manifest (DATA)      ← UPDATE 새 데이터\n",
    "│   └── 새 파일 (ADDED)               │   └── 새 파일 (ADDED)\n",
    "└─▶ Manifest (DATA)  ← INSERT 원본    ├─▶ Manifest (DELETES)   ← UPDATE용 Delete File\n",
    "    └── 원본 파일 (EXISTING/DELETED)   │   └── delete-*.parquet (ADDED)\n",
    "                                       └─▶ Manifest (DELETES)   ← DELETE용 Delete File\n",
    "                                           └── delete-*.parquet (ADDED)\n",
    "```\n",
    "\n",
    "| 특성 | COW | MOR |\n",
    "|------|-----|-----|\n",
    "| Manifest 종류 | DATA만 | DATA + DELETES |\n",
    "| 기존 파일 상태 | UPDATE/DELETE 시 DELETED로 표시 | EXISTING 유지 (삭제되지 않음) |\n",
    "| 새 파일 | 전체 데이터 재작성 파일 | 변경분 데이터 파일 + Delete File |\n",
    "| Manifest 크기 | 재작성된 파일 정보 포함 | 원본 유지 + 작은 추가 파일 정보 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4",
   "metadata": {},
   "source": [
    "---\n",
    "## 종합 비교표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f3a4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "COW vs MOR 종합 비교 (2,000건, UPDATE 600건, DELETE 200건)\n",
      "=================================================================\n",
      "            항목   COW   MOR\n",
      " INSERT 시간 (초) 0.494 0.312\n",
      " UPDATE 시간 (초) 0.537 0.634\n",
      " DELETE 시간 (초) 0.531 0.352\n",
      " SELECT 시간 (초) 0.062 0.136\n",
      "  활성 Data 파일 수     3     6\n",
      "활성 Delete 파일 수     0     6\n",
      "     총 활성 파일 수     3    12\n",
      " 디스크 총 크기 (KB) 130.0 100.2\n",
      "      최종 레코드 수  1800  1800\n"
     ]
    }
   ],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    '항목': [\n",
    "        'INSERT 시간 (초)',\n",
    "        'UPDATE 시간 (초)',\n",
    "        'DELETE 시간 (초)',\n",
    "        'SELECT 시간 (초)',\n",
    "        '활성 Data 파일 수',\n",
    "        '활성 Delete 파일 수',\n",
    "        '총 활성 파일 수',\n",
    "        '디스크 총 크기 (KB)',\n",
    "        '최종 레코드 수',\n",
    "    ],\n",
    "    'COW': [\n",
    "        f\"{cow_insert_time:.3f}\",\n",
    "        f\"{cow_update_time:.3f}\",\n",
    "        f\"{cow_delete_time:.3f}\",\n",
    "        f\"{cow_read_time:.3f}\",\n",
    "        cow_data_count,\n",
    "        cow_delete_count,\n",
    "        cow_data_count + cow_delete_count,\n",
    "        f\"{cow_total / 1024:.1f}\",\n",
    "        cow_count,\n",
    "    ],\n",
    "    'MOR': [\n",
    "        f\"{mor_insert_time:.3f}\",\n",
    "        f\"{mor_update_time:.3f}\",\n",
    "        f\"{mor_delete_time:.3f}\",\n",
    "        f\"{mor_read_time:.3f}\",\n",
    "        mor_data_count,\n",
    "        mor_delete_count,\n",
    "        mor_data_count + mor_delete_count,\n",
    "        f\"{mor_total / 1024:.1f}\",\n",
    "        mor_count,\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"COW vs MOR 종합 비교 (2,000건, UPDATE 600건, DELETE 200건)\")\n",
    "print(\"=\" * 65)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4b5c6",
   "metadata": {},
   "source": [
    "---\n",
    "## 비교 결과 분석\n",
    "\n",
    "### 1. INSERT — 차이 없음\n",
    "INSERT는 새 데이터 파일을 생성하는 동일한 동작입니다.  \n",
    "COW/MOR 전략은 **기존 데이터를 수정할 때만** 개입합니다.\n",
    "\n",
    "### 2. UPDATE/DELETE — 쓰기 성능에서 MOR 우위\n",
    "| 관점 | COW | MOR |\n",
    "|------|-----|-----|\n",
    "| **쓰기 I/O** | 변경 안 된 행까지 재작성 (쓰기 증폭) | 변경분만 기록 (최소 I/O) |\n",
    "| **파일 생성** | 기존 파일 크기와 비슷한 새 파일 | 작은 Delete File + 변경분 데이터 |\n",
    "| **메타데이터** | 기존 파일 DELETED → 새 파일 ADDED | 기존 파일 유지 + Delete File ADDED |\n",
    "\n",
    "**핵심 인사이트**: 파티션 내 일부 행만 변경해도 COW는 전체 파일을 재작성합니다.  \n",
    "데이터 파일이 클수록 (수백 MB ~ 수 GB) 이 **쓰기 증폭(write amplification)**의 비용이 커집니다.\n",
    "\n",
    "### 3. SELECT — 읽기 성능에서 COW 우위\n",
    "COW 테이블은 항상 최신 상태의 데이터 파일만 존재하므로, 추가 병합 없이 읽을 수 있습니다.  \n",
    "MOR 테이블은 Delete File을 데이터 파일과 병합해야 하므로, Delete File이 누적될수록 읽기 비용이 증가합니다.\n",
    "\n",
    "### 4. 파일 구조 — 근본적 차이\n",
    "```\n",
    "COW:  데이터 변경 → 파일 재작성 → 항상 \"깨끗한\" 데이터 파일만 존재\n",
    "MOR:  데이터 변경 → Delete File 추가 → 시간이 지나면 Delete File 누적 → 컴팩션 필요\n",
    "```\n",
    "\n",
    "> **트레이드오프 요약**: COW는 **쓰기 시 비용을 지불**하고, MOR는 **읽기 시 비용을 지불**합니다.  \n",
    "> MOR의 읽기 비용은 **컴팩션으로 관리**할 수 있으므로, 변경이 빈번한 워크로드에서는 MOR이 유리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7",
   "metadata": {},
   "source": [
    "---\n",
    "## 선택 기준 가이드\n",
    "\n",
    "| 워크로드 | 추천 | 이유 |\n",
    "|---------|------|------|\n",
    "| 읽기 중심, 수정 적음 | **COW** | 읽기 최적화, Delete File 없음 |\n",
    "| 쓰기 중심, 수정 빈번 | **MOR** | 쓰기 빠름, 정기 컴팩션으로 읽기 비용 관리 |\n",
    "| 대용량 파티션 + 소량 수정 | **MOR** | COW의 쓰기 증폭이 극대화되는 시나리오 |\n",
    "| 실시간 분석 (짧은 지연 허용 불가) | **COW** | 읽기 시 병합 비용 없음 |\n",
    "| 혼합 워크로드 | **작업별 혼합 설정** | DELETE/UPDATE/MERGE를 개별 설정 가능 |\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "- Iceberg에서는 **테이블 속성을 통해 작업별로 다른 전략을 설정**할 수 있습니다\n",
    "  ```sql\n",
    "  -- DELETE는 MOR, UPDATE는 COW로 혼합 설정\n",
    "  ALTER TABLE my_table SET TBLPROPERTIES (\n",
    "      'write.delete.mode'='merge-on-read',\n",
    "      'write.update.mode'='copy-on-write'\n",
    "  )\n",
    "  ```\n",
    "- MOR 테이블은 Delete File이 누적되므로, **정기적인 컴팩션(compaction)**이 필요합니다\n",
    "- 컴팩션은 Delete File을 데이터 파일에 병합하여 읽기 성능을 회복시킵니다\n",
    "- 프로덕션에서는 **Spark의 `rewrite_data_files`** 프로시저로 컴팩션을 수행합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d7e8",
   "metadata": {},
   "source": [
    "---\n",
    "## 다음 단계\n",
    "\n",
    "> MOR 테이블의 읽기 성능을 회복하려면 **정기적인 컴팩션**이 필요합니다.  \n",
    "> 이 내용은 **4_optimization** 모듈에서 자세히 다룹니다.\n",
    ">\n",
    "> 컴팩션을 통해 Delete File을 데이터 파일에 병합하면,  \n",
    "> MOR의 쓰기 장점을 유지하면서 읽기 성능도 관리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d7e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
