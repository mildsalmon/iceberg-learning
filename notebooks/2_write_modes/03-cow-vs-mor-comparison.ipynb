{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "# COW vs MOR 직접 비교 실험\n",
    "\n",
    "동일한 데이터와 동일한 작업을 COW/MOR 테이블에서 각각 수행하고,  \n",
    "**파일 수, 크기, 쓰기/읽기 시간**을 정량적으로 비교합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "## 실험 설계\n",
    "\n",
    "| 단계 | 작업 | 설명 |\n",
    "|------|------|------|\n",
    "| 1 | INSERT | 200건 초기 데이터 적재 |\n",
    "| 2 | UPDATE | 50건 상태 변경 |\n",
    "| 3 | DELETE | 30건 삭제 |\n",
    "| 4 | SELECT | 전체 읽기 |\n",
    "\n",
    "동일한 시나리오를 COW와 MOR 양쪽에서 실행하여 차이를 관찰합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e4f5",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, snapshot_tree, diff_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "COW_TABLE = \"demo.lab.cmp_cow_orders\"\n",
    "MOR_TABLE = \"demo.lab.cmp_mor_orders\"\n",
    "COW_PATH = \"/home/jovyan/data/warehouse/lab/cmp_cow_orders\"\n",
    "MOR_PATH = \"/home/jovyan/data/warehouse/lab/cmp_mor_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6b7c8",
   "metadata": {},
   "source": [
    "## 테이블 생성\n",
    "\n",
    "동일한 스키마, 동일한 파티셔닝 — **TBLPROPERTIES만 다릅니다**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 테이블 정리\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {COW_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {MOR_TABLE}\")\n",
    "\n",
    "# COW 테이블\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {COW_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.delete.mode'='copy-on-write',\n",
    "    'write.update.mode'='copy-on-write',\n",
    "    'write.merge.mode'='copy-on-write'\n",
    ")\n",
    "\"\"\")\n",
    "print(f\"COW 테이블 생성 완료: {COW_TABLE}\")\n",
    "\n",
    "# MOR 테이블\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {MOR_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.delete.mode'='merge-on-read',\n",
    "    'write.update.mode'='merge-on-read',\n",
    "    'write.merge.mode'='merge-on-read'\n",
    ")\n",
    "\"\"\")\n",
    "print(f\"MOR 테이블 생성 완료: {MOR_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8d9e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 1: INSERT 200건\n",
    "\n",
    "동일한 데이터(seed=42)를 양쪽 테이블에 삽입합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일한 데이터 생성\n",
    "orders = generate_orders(num_records=200, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.cache()  # 동일 데이터를 두 번 쓰므로 캐시\n",
    "\n",
    "# COW INSERT\n",
    "start = time.time()\n",
    "df.writeTo(COW_TABLE).append()\n",
    "cow_insert_time = time.time() - start\n",
    "print(f\"COW INSERT 시간: {cow_insert_time:.3f}초\")\n",
    "\n",
    "# MOR INSERT\n",
    "start = time.time()\n",
    "df.writeTo(MOR_TABLE).append()\n",
    "mor_insert_time = time.time() - start\n",
    "print(f\"MOR INSERT 시간: {mor_insert_time:.3f}초\")\n",
    "\n",
    "df.unpersist()\n",
    "print(f\"\\n양쪽 테이블 모두 {spark.sql(f'SELECT COUNT(*) FROM {COW_TABLE}').collect()[0][0]}건 삽입 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0f1a2",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — INSERT\n",
    "\n",
    "INSERT는 COW/MOR 차이가 없습니다. 둘 다 새 데이터 파일을 생성할 뿐입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1a2b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 2: UPDATE 50건\n",
    "\n",
    "order_id 기준으로 처음 50건의 상태를 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COW UPDATE\n",
    "start = time.time()\n",
    "spark.sql(f\"UPDATE {COW_TABLE} SET status = 'refunded' WHERE order_id <= 50\")\n",
    "cow_update_time = time.time() - start\n",
    "print(f\"COW UPDATE 시간: {cow_update_time:.3f}초\")\n",
    "\n",
    "# MOR UPDATE\n",
    "start = time.time()\n",
    "spark.sql(f\"UPDATE {MOR_TABLE} SET status = 'refunded' WHERE order_id <= 50\")\n",
    "mor_update_time = time.time() - start\n",
    "print(f\"MOR UPDATE 시간: {mor_update_time:.3f}초\")\n",
    "\n",
    "ratio = cow_update_time / mor_update_time if mor_update_time > 0 else float('inf')\n",
    "print(f\"\\nCOW/MOR 비율: {ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — UPDATE\n",
    "\n",
    "- COW: 영향받은 파티션의 데이터 파일을 **통째로 재작성** → 느림\n",
    "- MOR: Delete File + 작은 데이터 파일만 추가 → 빠름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 3: DELETE 30건\n",
    "\n",
    "order_id 기준으로 51~80번 주문을 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COW DELETE\n",
    "start = time.time()\n",
    "spark.sql(f\"DELETE FROM {COW_TABLE} WHERE order_id BETWEEN 51 AND 80\")\n",
    "cow_delete_time = time.time() - start\n",
    "print(f\"COW DELETE 시간: {cow_delete_time:.3f}초\")\n",
    "\n",
    "# MOR DELETE\n",
    "start = time.time()\n",
    "spark.sql(f\"DELETE FROM {MOR_TABLE} WHERE order_id BETWEEN 51 AND 80\")\n",
    "mor_delete_time = time.time() - start\n",
    "print(f\"MOR DELETE 시간: {mor_delete_time:.3f}초\")\n",
    "\n",
    "ratio = cow_delete_time / mor_delete_time if mor_delete_time > 0 else float('inf')\n",
    "print(f\"\\nCOW/MOR 비율: {ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f7a8",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — DELETE\n",
    "\n",
    "- COW: 역시 데이터 파일 재작성\n",
    "- MOR: Delete File만 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7a8b9",
   "metadata": {},
   "source": [
    "---\n",
    "## 단계 4: SELECT — 읽기 성능 비교\n",
    "\n",
    "전체 데이터를 읽는 시간을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COW SELECT\n",
    "start = time.time()\n",
    "cow_count = spark.sql(f\"SELECT COUNT(*) FROM {COW_TABLE}\").collect()[0][0]\n",
    "cow_read_time = time.time() - start\n",
    "print(f\"COW SELECT 시간: {cow_read_time:.3f}초 (레코드 수: {cow_count})\")\n",
    "\n",
    "# MOR SELECT\n",
    "start = time.time()\n",
    "mor_count = spark.sql(f\"SELECT COUNT(*) FROM {MOR_TABLE}\").collect()[0][0]\n",
    "mor_read_time = time.time() - start\n",
    "print(f\"MOR SELECT 시간: {mor_read_time:.3f}초 (레코드 수: {mor_count})\")\n",
    "\n",
    "ratio = mor_read_time / cow_read_time if cow_read_time > 0 else float('inf')\n",
    "print(f\"\\nMOR/COW 비율: {ratio:.2f}x (MOR이 읽기에서 느릴 수 있음)\")\n",
    "print(f\"레코드 수 일치 여부: {cow_count == mor_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9c0d1",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — SELECT\n",
    "\n",
    "- COW: 데이터 파일만 읽으면 됨 → 빠름\n",
    "- MOR: 데이터 파일 + Delete File 병합 필요 → 상대적으로 느림\n",
    "- 소규모 데이터에서는 차이가 미미할 수 있지만, 대규모에서 차이가 커집니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 파일 시스템 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d1e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cow_file_count = count_files(COW_PATH)\n",
    "mor_file_count = count_files(MOR_PATH)\n",
    "cow_total_size = total_size(COW_PATH)\n",
    "mor_total_size = total_size(MOR_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COW 테이블 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(COW_PATH)\n",
    "\n",
    "print(f\"\\n파일 수: {cow_file_count}, 총 크기: {cow_total_size:,} bytes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOR 테이블 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(MOR_PATH)\n",
    "\n",
    "print(f\"\\n파일 수: {mor_file_count}, 총 크기: {mor_total_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4",
   "metadata": {},
   "source": [
    "---\n",
    "## 종합 비교표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    '항목': [\n",
    "        'INSERT 시간 (초)',\n",
    "        'UPDATE 시간 (초)',\n",
    "        'DELETE 시간 (초)',\n",
    "        'SELECT 시간 (초)',\n",
    "        '파일 수',\n",
    "        '총 크기 (bytes)',\n",
    "        '최종 레코드 수',\n",
    "    ],\n",
    "    'COW': [\n",
    "        f\"{cow_insert_time:.3f}\",\n",
    "        f\"{cow_update_time:.3f}\",\n",
    "        f\"{cow_delete_time:.3f}\",\n",
    "        f\"{cow_read_time:.3f}\",\n",
    "        cow_file_count,\n",
    "        f\"{cow_total_size:,}\",\n",
    "        cow_count,\n",
    "    ],\n",
    "    'MOR': [\n",
    "        f\"{mor_insert_time:.3f}\",\n",
    "        f\"{mor_update_time:.3f}\",\n",
    "        f\"{mor_delete_time:.3f}\",\n",
    "        f\"{mor_read_time:.3f}\",\n",
    "        mor_file_count,\n",
    "        f\"{mor_total_size:,}\",\n",
    "        mor_count,\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COW vs MOR 종합 비교\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4b5c6",
   "metadata": {},
   "source": [
    "---\n",
    "## 비교 결과 분석\n",
    "\n",
    "### 쓰기 성능 (UPDATE/DELETE)\n",
    "- **MOR이 빠름**: Delete File만 추가하므로 파일 재작성이 없음\n",
    "- **COW가 느림**: 변경된 행이 포함된 파티션 파일 전체를 재작성\n",
    "\n",
    "### 읽기 성능 (SELECT)\n",
    "- **COW가 빠름**: 데이터 파일만 스캔하면 됨\n",
    "- **MOR이 느림**: 데이터 파일과 Delete File을 병합해야 함\n",
    "\n",
    "### 파일 수와 크기\n",
    "- **COW**: 재작성으로 인해 누적 파일 크기가 클 수 있음\n",
    "- **MOR**: Delete File이 추가로 생성되어 파일 수가 더 많을 수 있음\n",
    "\n",
    "> **참고**: 소규모 데이터(200건)에서는 차이가 미미할 수 있습니다.  \n",
    "> 실제 프로덕션(수백만~수십억 건)에서는 이 차이가 **수배~수십배**로 벌어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7",
   "metadata": {},
   "source": [
    "---\n",
    "## 선택 기준 가이드\n",
    "\n",
    "| 워크로드 | 추천 | 이유 |\n",
    "|---------|------|------|\n",
    "| 읽기 중심, 수정 적음 | **COW** | 읽기 최적화, Delete File 없음 |\n",
    "| 쓰기 중심, 수정 빈번 | **MOR** | 쓰기 빠름, 정기 컴팩션으로 읽기 비용 관리 |\n",
    "| 혼합 워크로드 | **작업별 혼합 설정** | `write.delete.mode=mor`, `write.update.mode=mor` 등 개별 설정 가능 |\n",
    "\n",
    "### 실무 팁\n",
    "\n",
    "- Iceberg에서는 **테이블 속성을 통해 작업별로 다른 전략을 설정**할 수 있습니다\n",
    "  ```sql\n",
    "  -- DELETE는 MOR, UPDATE는 COW로 혼합 설정\n",
    "  ALTER TABLE my_table SET TBLPROPERTIES (\n",
    "      'write.delete.mode'='merge-on-read',\n",
    "      'write.update.mode'='copy-on-write'\n",
    "  )\n",
    "  ```\n",
    "- MOR 테이블은 Delete File이 누적되므로, **정기적인 컴팩션(compaction)**이 필요합니다\n",
    "- 컴팩션은 Delete File을 데이터 파일에 병합하여 읽기 성능을 회복시킵니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d7e8",
   "metadata": {},
   "source": [
    "---\n",
    "## 다음 단계\n",
    "\n",
    "> MOR 테이블의 읽기 성능을 회복하려면 **정기적인 컴팩션**이 필요합니다.  \n",
    "> 이 내용은 **4_optimization** 모듈에서 자세히 다룹니다.\n",
    ">\n",
    "> 컴팩션을 통해 Delete File을 데이터 파일에 병합하면,  \n",
    "> MOR의 쓰기 장점을 유지하면서 읽기 성능도 관리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}