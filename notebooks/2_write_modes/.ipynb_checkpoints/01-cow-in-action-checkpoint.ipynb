{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "# COW(Copy-on-Write) 동작 원리 실습\n",
    "\n",
    "이 노트북에서는 Iceberg의 **Copy-on-Write(COW)** 전략이 실제로 어떻게 동작하는지 파일 수준에서 관찰합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "## COW(Copy-on-Write)란?\n",
    "\n",
    "COW는 Iceberg의 **기본 쓰기 전략**입니다.\n",
    "\n",
    "### 핵심 원리\n",
    "- 데이터 파일의 **단 한 행**이라도 업데이트/삭제되면, 해당 데이터 파일 **전체를 다시 씁니다**\n",
    "- 기존 파일은 그대로 두고, 변경된 내용을 반영한 **새 파일**을 생성합니다\n",
    "\n",
    "### 장점\n",
    "- **읽기에 최적화**: 삭제 파일(delete file) 없이 데이터 파일만 읽으면 됩니다\n",
    "- 읽기 시 추가적인 병합(merge) 작업이 필요 없습니다\n",
    "\n",
    "### 단점\n",
    "- **행 수준 업데이트/삭제가 느림**: 1건만 바꿔도 파일 전체를 재작성해야 합니다\n",
    "- 쓰기 증폭(write amplification)이 발생합니다\n",
    "\n",
    "```\n",
    "예시: 1000행 파일에서 1행 UPDATE\n",
    "┌─────────────────┐         ┌─────────────────┐\n",
    "│  기존 파일        │         │  새 파일          │\n",
    "│  (1000행)        │  ──→   │  (1000행, 1행 변경)│\n",
    "│  ❌ 더 이상 참조 안됨│         │  ✅ 새 스냅샷이 참조│\n",
    "└─────────────────┘         └─────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e4f5",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, snapshot_tree, diff_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "TABLE_NAME = \"demo.lab.cow_orders\"\n",
    "TABLE_PATH = \"/home/jovyan/data/warehouse/lab/cow_orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6b7c8",
   "metadata": {},
   "source": [
    "## COW 테이블 생성\n",
    "\n",
    "모든 쓰기 모드(delete, update, merge)를 `copy-on-write`로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_NAME} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.delete.mode'='copy-on-write',\n",
    "    'write.update.mode'='copy-on-write',\n",
    "    'write.merge.mode'='copy-on-write'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(f\"테이블 생성 완료: {TABLE_NAME}\")\n",
    "\n",
    "# 설정 확인\n",
    "props = spark.sql(f\"SHOW TBLPROPERTIES {TABLE_NAME}\").collect()\n",
    "for row in props:\n",
    "    if 'write' in row['key']:\n",
    "        print(f\"  {row['key']} = {row['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8d9e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 1: INSERT — 초기 데이터 적재\n",
    "\n",
    "100건의 주문 데이터를 삽입하고, 파일 시스템 상태를 관찰합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_insert = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# 100건 INSERT\n",
    "orders = generate_orders(num_records=100, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(TABLE_NAME).append()\n",
    "\n",
    "after_insert = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INSERT 후 변경 사항:\")\n",
    "print(\"=\" * 60)\n",
    "diff_tree(before_insert, after_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INSERT 후 테이블 트리 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH)\n",
    "\n",
    "print(f\"\\n파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "print(f\"총 레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")\n",
    "print(\"\\n상태별 건수:\")\n",
    "spark.sql(f\"SELECT status, COUNT(*) as cnt FROM {TABLE_NAME} GROUP BY status ORDER BY cnt DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스냅샷 확인\n",
    "print(\"현재 스냅샷:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — INSERT\n",
    "\n",
    "- INSERT는 COW/MOR에 관계없이 동일하게 동작합니다\n",
    "- 파티션(months)별로 데이터 파일이 생성되었습니다\n",
    "- 스냅샷이 1개 생성되었습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 2: UPDATE — COW의 핵심 동작 관찰\n",
    "\n",
    "`status='cancelled'`인 주문을 `status='refunded'`로 변경합니다.  \n",
    "COW에서는 변경된 행이 포함된 파티션의 데이터 파일이 **통째로 재작성**됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE 대상 확인\n",
    "cancelled_count = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE status = 'cancelled'\").collect()[0][0]\n",
    "print(f\"UPDATE 대상 (status='cancelled'): {cancelled_count}건\")\n",
    "print(\"\\n파티션별 분포:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT months(order_date) as partition_month, COUNT(*) as cnt\n",
    "    FROM {TABLE_NAME}\n",
    "    WHERE status = 'cancelled'\n",
    "    GROUP BY months(order_date)\n",
    "    ORDER BY partition_month\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_update = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# UPDATE 실행\n",
    "spark.sql(f\"UPDATE {TABLE_NAME} SET status = 'refunded' WHERE status = 'cancelled'\")\n",
    "\n",
    "after_update = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UPDATE 후 변경 사항 (COW):\")\n",
    "print(\"=\" * 60)\n",
    "diff_tree(before_update, after_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UPDATE 후 테이블 트리 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH)\n",
    "\n",
    "print(f\"\\n파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 수준에서 비교: 기존 파일 vs 새 파일의 레코드 수\n",
    "print(\"파일별 레코드 수 확인:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT file_path, record_count, file_size_in_bytes\n",
    "    FROM {TABLE_NAME}.files\n",
    "    ORDER BY file_path\n",
    "\"\"\").show(truncate=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스냅샷 확인\n",
    "print(\"현재 스냅샷:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0d1e2",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — UPDATE (COW)\n",
    "\n",
    "- `cancelled` 상태의 주문이 포함된 **파티션의 데이터 파일이 통째로 재작성**되었습니다\n",
    "- 새 데이터 파일이 추가되고, 기존 파일은 메타데이터에서 참조가 해제됩니다\n",
    "- **Delete File은 생성되지 않았습니다** — 이것이 COW의 특징입니다\n",
    "- 영향받지 않은 파티션의 파일은 그대로 유지됩니다\n",
    "- 스냅샷이 1개 더 추가되었습니다 (총 2개)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1e2f3",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 3: DELETE — 파일 재작성 패턴 재확인\n",
    "\n",
    "`amount < 200`인 주문을 삭제합니다.  \n",
    "DELETE도 UPDATE와 동일하게 파일 전체가 재작성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE 대상 확인\n",
    "delete_count = spark.sql(f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE amount < 200\").collect()[0][0]\n",
    "print(f\"DELETE 대상 (amount < 200): {delete_count}건\")\n",
    "print(f\"DELETE 전 총 레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_delete = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "# DELETE 실행\n",
    "spark.sql(f\"DELETE FROM {TABLE_NAME} WHERE amount < 200\")\n",
    "\n",
    "after_delete = snapshot_tree(TABLE_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DELETE 후 변경 사항 (COW):\")\n",
    "print(\"=\" * 60)\n",
    "diff_tree(before_delete, after_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DELETE 후 테이블 트리 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(TABLE_PATH)\n",
    "\n",
    "print(f\"\\n파일 수: {count_files(TABLE_PATH)}\")\n",
    "print(f\"총 크기: {total_size(TABLE_PATH):,} bytes\")\n",
    "print(f\"DELETE 후 총 레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {TABLE_NAME}').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스냅샷 확인\n",
    "print(\"현재 스냅샷:\")\n",
    "spark.sql(f\"SELECT * FROM {TABLE_NAME}.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d7e8",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — DELETE (COW)\n",
    "\n",
    "- DELETE도 UPDATE와 동일한 패턴: 영향받은 파티션의 **데이터 파일이 통째로 재작성**됩니다\n",
    "- 삭제된 행을 제외한 나머지 행만 포함된 **새 파일**이 생성됩니다\n",
    "- 역시 **Delete File은 생성되지 않았습니다**\n",
    "- 스냅샷이 1개 더 추가되었습니다 (총 3개)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7e8f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 정리: COW(Copy-on-Write) 핵심 요약\n",
    "\n",
    "| 항목 | 설명 |\n",
    "|------|------|\n",
    "| **쓰기 방식** | 변경된 행이 포함된 데이터 파일 전체를 새로 작성 |\n",
    "| **Delete File** | 생성하지 않음 |\n",
    "| **읽기 성능** | 최적 — 데이터 파일만 읽으면 됨 |\n",
    "| **쓰기 성능** | 느림 — 1건 변경에도 파일 전체 재작성 |\n",
    "| **적합한 워크로드** | 읽기 중심, 업데이트/삭제가 적은 경우 |\n",
    "\n",
    "> **핵심**: COW에서는 **1건만 바꿔도 해당 파티션의 데이터 파일이 통째로 재작성**됩니다.  \n",
    "> 다음 노트북에서 MOR(Merge-on-Read)이 이 문제를 어떻게 해결하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}