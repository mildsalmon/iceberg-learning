{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975a0a9b-3728-4dee-9a03-ae57519f8f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark + Iceberg ì„¤ì • ì™„ë£Œ!\n",
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os \n",
    "\n",
    "# Spark + Iceberg ì„¤ì •\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergBlogDemo\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/iceberg-spark.jar\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"file:///home/jovyan/data/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark + Iceberg ì„¤ì • ì™„ë£Œ!\")\n",
    "\n",
    "# ì¹´íƒˆë¡œê·¸ í™•ì¸\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d56986-8908-42f2-8f9f-7750a4e8e243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ COW vs MOR ë°©ì‹ ì™„ì „ ë¹„êµ ë¶„ì„\n",
      "==================================================\n",
      "ğŸ“ íŒŒì¼ êµ¬ì¡° ë¹„êµ:\n",
      "==============================\n",
      "ğŸ“Š COW í…Œì´ë¸” (orders):\n",
      "  ì´ íŒŒì¼: 66ê°œ\n",
      "  ğŸ“„ ë°ì´í„° íŒŒì¼: 66ê°œ\n",
      "  ğŸ—‘ï¸ Delete íŒŒì¼: 0ê°œ\n",
      "\n",
      "ğŸ“Š MOR í…Œì´ë¸” (orders_mor):\n",
      "  ì´ íŒŒì¼: 101ê°œ\n",
      "  ğŸ“„ ë°ì´í„° íŒŒì¼: 72ê°œ\n",
      "  ğŸ—‘ï¸ Delete íŒŒì¼: 29ê°œ\n",
      "\n",
      "ğŸ’¾ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ë¹„êµ:\n",
      "  COW: 121,883 bytes (0.12 MB)\n",
      "  MOR: 178,872 bytes (0.17 MB)\n",
      "  ì°¨ì´: +56,989 bytes\n",
      "\n",
      "ğŸ—‚ï¸ ë©”íƒ€ë°ì´í„° ë³µì¡ë„:\n",
      "  COW Manifest/List íŒŒì¼: 11ê°œ\n",
      "  MOR Manifest/List íŒŒì¼: 11ê°œ\n",
      "\n",
      "ğŸ“¸ ìŠ¤ëƒ…ìƒ· ê°œìˆ˜:\n",
      "  COW: 6ê°œ\n",
      "  MOR: 5ê°œ\n",
      "  ğŸ‘‰ MORì´ ë” ë§ì€ ì´ìœ : ì‚­ì œ/ì—…ë°ì´íŠ¸ ì‹œ ìƒˆ ìŠ¤ëƒ…ìƒ· ìƒì„±\n",
      "\n",
      "ğŸš€ ì¿¼ë¦¬ ì„±ëŠ¥ íŠ¹ì„± ë¹„êµ:\n",
      "==============================\n",
      "ğŸ“– ì½ê¸° ì„±ëŠ¥:\n",
      "  COW: â­â­â­â­â­ (Delete Files ì—†ìŒ, ë¹ ë¥¸ ìŠ¤ìº”)\n",
      "  MOR: â­â­â­â­â˜† (Delete Files ë³‘í•© í•„ìš”)\n",
      "\n",
      "âœï¸ ì“°ê¸° ì„±ëŠ¥:\n",
      "  COW: â­â­â­â˜†â˜† (ì „ì²´ íŒŒì¼ ì¬ì‘ì„±)\n",
      "  MOR: â­â­â­â­â­ (Delete Filesë§Œ ìƒì„±)\n",
      "\n",
      "ğŸ”„ ì—…ë°ì´íŠ¸/ì‚­ì œ ì„±ëŠ¥:\n",
      "  COW: â­â­â˜†â˜†â˜† (ì „ì²´ íŒŒì¼ ì¬ì‘ì„± í•„ìš”)\n",
      "  MOR: â­â­â­â­â­ (Delete Filesë¡œ ë¹ ë¥¸ ì²˜ë¦¬)\n",
      "\n",
      "â±ï¸ ì‹¤ì œ ì‚­ì œ ì‘ì—… ì‹œê°„ ë¹„êµ:\n",
      "==============================\n",
      "ğŸ”„ COW í…Œì´ë¸”ì—ì„œ ì‚­ì œ ì‘ì—…...\n",
      "ğŸ”„ MOR í…Œì´ë¸”ì—ì„œ ì‚­ì œ ì‘ì—…...\n",
      "  COW ì‚­ì œ ì‹œê°„: 0.65ì´ˆ\n",
      "  MOR ì‚­ì œ ì‹œê°„: 0.47ì´ˆ\n",
      "  ì„±ëŠ¥ ì°¨ì´: +27.7%\n",
      "\n",
      "ğŸ’¡ ì‚¬ìš© ê¶Œì¥ ì‚¬í•­:\n",
      "====================\n",
      "ğŸ¯ COW ë°©ì‹ ì¶”ì²œ:\n",
      "  âœ… ì½ê¸° ì¤‘ì‹¬ ì›Œí¬ë¡œë“œ\n",
      "  âœ… ì—…ë°ì´íŠ¸/ì‚­ì œê°€ ê±°ì˜ ì—†ìŒ\n",
      "  âœ… ë°°ì¹˜ ì²˜ë¦¬ ìœ„ì£¼\n",
      "  âœ… ì¿¼ë¦¬ ì„±ëŠ¥ì´ ìµœìš°ì„ \n",
      "\n",
      "ğŸ¯ MOR ë°©ì‹ ì¶”ì²œ:\n",
      "  âœ… ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸/ì‚­ì œ í•„ìš”\n",
      "  âœ… ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°\n",
      "  âœ… ë¹ˆë²ˆí•œ ë³€ê²½ ì‘ì—…\n",
      "  âœ… ì“°ê¸° ì„±ëŠ¥ì´ ì¤‘ìš”\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# âš–ï¸ COW vs MOR ì™„ì „ ë¹„êµ ë¶„ì„\n",
    "# =============================================================================\n",
    "\n",
    "print(\"âš–ï¸ COW vs MOR ë°©ì‹ ì™„ì „ ë¹„êµ ë¶„ì„\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ê¸°ì¡´ COW í…Œì´ë¸”ê³¼ ìƒˆ MOR í…Œì´ë¸” ë¹„êµ\n",
    "cow_table_path = \"/home/jovyan/data/warehouse/blog/orders\"\n",
    "mor_table_path = \"/home/jovyan/data/warehouse/blog/orders_mor\"\n",
    "\n",
    "# 1. íŒŒì¼ êµ¬ì¡° ë¹„êµ\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“ íŒŒì¼ êµ¬ì¡° ë¹„êµ:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# COW í…Œì´ë¸” ë¶„ì„\n",
    "cow_all_files = glob.glob(f\"{cow_table_path}/**/*.parquet\", recursive=True)\n",
    "cow_data_files = [f for f in cow_all_files if 'delete' not in os.path.basename(f).lower()]\n",
    "cow_delete_files = [f for f in cow_all_files if 'delete' in os.path.basename(f).lower()]\n",
    "\n",
    "# MOR í…Œì´ë¸” ë¶„ì„\n",
    "mor_all_files = glob.glob(f\"{mor_table_path}/**/*.parquet\", recursive=True)\n",
    "mor_data_files = [f for f in mor_all_files if 'delete' not in os.path.basename(f).lower()]\n",
    "mor_delete_files = [f for f in mor_all_files if 'delete' in os.path.basename(f).lower()]\n",
    "\n",
    "print(f\"ğŸ“Š COW í…Œì´ë¸” (orders):\")\n",
    "print(f\"  ì´ íŒŒì¼: {len(cow_all_files)}ê°œ\")\n",
    "print(f\"  ğŸ“„ ë°ì´í„° íŒŒì¼: {len(cow_data_files)}ê°œ\")\n",
    "print(f\"  ğŸ—‘ï¸ Delete íŒŒì¼: {len(cow_delete_files)}ê°œ\")\n",
    "\n",
    "print(f\"\\nğŸ“Š MOR í…Œì´ë¸” (orders_mor):\")\n",
    "print(f\"  ì´ íŒŒì¼: {len(mor_all_files)}ê°œ\")\n",
    "print(f\"  ğŸ“„ ë°ì´í„° íŒŒì¼: {len(mor_data_files)}ê°œ\")\n",
    "print(f\"  ğŸ—‘ï¸ Delete íŒŒì¼: {len(mor_delete_files)}ê°œ\")\n",
    "\n",
    "# 2. ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ë¹„êµ\n",
    "cow_total_size = sum(os.path.getsize(f) for f in cow_all_files)\n",
    "mor_total_size = sum(os.path.getsize(f) for f in mor_all_files)\n",
    "\n",
    "print(f\"\\nğŸ’¾ ìŠ¤í† ë¦¬ì§€ ì‚¬ìš©ëŸ‰ ë¹„êµ:\")\n",
    "print(f\"  COW: {cow_total_size:,} bytes ({cow_total_size/1024/1024:.2f} MB)\")\n",
    "print(f\"  MOR: {mor_total_size:,} bytes ({mor_total_size/1024/1024:.2f} MB)\")\n",
    "print(f\"  ì°¨ì´: {mor_total_size - cow_total_size:+,} bytes\")\n",
    "\n",
    "# 3. ë©”íƒ€ë°ì´í„° ë³µì¡ë„ ë¹„êµ\n",
    "cow_metadata_files = glob.glob(f\"{cow_table_path}/metadata/*.avro\")\n",
    "mor_metadata_files = glob.glob(f\"{mor_table_path}/metadata/*.avro\")\n",
    "\n",
    "print(f\"\\nğŸ—‚ï¸ ë©”íƒ€ë°ì´í„° ë³µì¡ë„:\")\n",
    "print(f\"  COW Manifest/List íŒŒì¼: {len(cow_metadata_files)}ê°œ\")\n",
    "print(f\"  MOR Manifest/List íŒŒì¼: {len(mor_metadata_files)}ê°œ\")\n",
    "\n",
    "# 4. ìŠ¤ëƒ…ìƒ· ê°œìˆ˜ ë¹„êµ\n",
    "cow_snapshots = spark.sql(\"SELECT COUNT(*) FROM demo.blog.orders.snapshots\").collect()[0][0]\n",
    "mor_snapshots = spark.sql(\"SELECT COUNT(*) FROM demo.blog.orders_mor.snapshots\").collect()[0][0]\n",
    "\n",
    "print(f\"\\nğŸ“¸ ìŠ¤ëƒ…ìƒ· ê°œìˆ˜:\")\n",
    "print(f\"  COW: {cow_snapshots}ê°œ\")\n",
    "print(f\"  MOR: {mor_snapshots}ê°œ\")\n",
    "print(f\"  ğŸ‘‰ MORì´ ë” ë§ì€ ì´ìœ : ì‚­ì œ/ì—…ë°ì´íŠ¸ ì‹œ ìƒˆ ìŠ¤ëƒ…ìƒ· ìƒì„±\")\n",
    "\n",
    "# 5. ì¿¼ë¦¬ ì„±ëŠ¥ ë¹„êµ ì‹œë®¬ë ˆì´ì…˜\n",
    "print(f\"\\nğŸš€ ì¿¼ë¦¬ ì„±ëŠ¥ íŠ¹ì„± ë¹„êµ:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"ğŸ“– ì½ê¸° ì„±ëŠ¥:\")\n",
    "print(\"  COW: â­â­â­â­â­ (Delete Files ì—†ìŒ, ë¹ ë¥¸ ìŠ¤ìº”)\")\n",
    "print(\"  MOR: â­â­â­â­â˜† (Delete Files ë³‘í•© í•„ìš”)\")\n",
    "\n",
    "print(\"\\nâœï¸ ì“°ê¸° ì„±ëŠ¥:\")\n",
    "print(\"  COW: â­â­â­â˜†â˜† (ì „ì²´ íŒŒì¼ ì¬ì‘ì„±)\")\n",
    "print(\"  MOR: â­â­â­â­â­ (Delete Filesë§Œ ìƒì„±)\")\n",
    "\n",
    "print(\"\\nğŸ”„ ì—…ë°ì´íŠ¸/ì‚­ì œ ì„±ëŠ¥:\")\n",
    "print(\"  COW: â­â­â˜†â˜†â˜† (ì „ì²´ íŒŒì¼ ì¬ì‘ì„± í•„ìš”)\")\n",
    "print(\"  MOR: â­â­â­â­â­ (Delete Filesë¡œ ë¹ ë¥¸ ì²˜ë¦¬)\")\n",
    "\n",
    "# 6. ì‹¤ì œ ì‚­ì œ ì‘ì—… ì‹œê°„ ë¹„êµ\n",
    "import time\n",
    "\n",
    "print(f\"\\nâ±ï¸ ì‹¤ì œ ì‚­ì œ ì‘ì—… ì‹œê°„ ë¹„êµ:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# COW ì‚­ì œ ì‹œê°„ ì¸¡ì •\n",
    "print(\"ğŸ”„ COW í…Œì´ë¸”ì—ì„œ ì‚­ì œ ì‘ì—…...\")\n",
    "start_time = time.time()\n",
    "spark.sql(\"DELETE FROM demo.blog.orders WHERE order_id = 999\")  # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ID\n",
    "cow_delete_time = time.time() - start_time\n",
    "\n",
    "# MOR ì‚­ì œ ì‹œê°„ ì¸¡ì •  \n",
    "print(\"ğŸ”„ MOR í…Œì´ë¸”ì—ì„œ ì‚­ì œ ì‘ì—…...\")\n",
    "start_time = time.time()\n",
    "spark.sql(\"DELETE FROM demo.blog.orders_mor WHERE order_id = 1000\")  # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ID\n",
    "mor_delete_time = time.time() - start_time\n",
    "\n",
    "print(f\"  COW ì‚­ì œ ì‹œê°„: {cow_delete_time:.2f}ì´ˆ\")\n",
    "print(f\"  MOR ì‚­ì œ ì‹œê°„: {mor_delete_time:.2f}ì´ˆ\")\n",
    "print(f\"  ì„±ëŠ¥ ì°¨ì´: {((cow_delete_time - mor_delete_time) / cow_delete_time * 100):+.1f}%\")\n",
    "\n",
    "# 7. ì‚¬ìš© ê¶Œì¥ ì‚¬í•­\n",
    "print(f\"\\nğŸ’¡ ì‚¬ìš© ê¶Œì¥ ì‚¬í•­:\")\n",
    "print(\"=\" * 20)\n",
    "print(\"ğŸ¯ COW ë°©ì‹ ì¶”ì²œ:\")\n",
    "print(\"  âœ… ì½ê¸° ì¤‘ì‹¬ ì›Œí¬ë¡œë“œ\")\n",
    "print(\"  âœ… ì—…ë°ì´íŠ¸/ì‚­ì œê°€ ê±°ì˜ ì—†ìŒ\")\n",
    "print(\"  âœ… ë°°ì¹˜ ì²˜ë¦¬ ìœ„ì£¼\")\n",
    "print(\"  âœ… ì¿¼ë¦¬ ì„±ëŠ¥ì´ ìµœìš°ì„ \")\n",
    "\n",
    "print(\"\\nğŸ¯ MOR ë°©ì‹ ì¶”ì²œ:\")\n",
    "print(\"  âœ… ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸/ì‚­ì œ í•„ìš”\")\n",
    "print(\"  âœ… ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°\")\n",
    "print(\"  âœ… ë¹ˆë²ˆí•œ ë³€ê²½ ì‘ì—…\")\n",
    "print(\"  âœ… ì“°ê¸° ì„±ëŠ¥ì´ ì¤‘ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bf6eb2-5546-44b0-a42c-92230604daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Schema Evolution ì™„ì „ ì‹¤ìŠµ\n",
      "==================================================\n",
      "ğŸ“‹ í˜„ì¬ MOR í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ:\n",
      "+--------------------+-------------+-------+\n",
      "|            col_name|    data_type|comment|\n",
      "+--------------------+-------------+-------+\n",
      "|            order_id|       bigint|   null|\n",
      "|         customer_id|       bigint|   null|\n",
      "|        product_name|       string|   null|\n",
      "|          order_date|         date|   null|\n",
      "|              amount|decimal(10,2)|   null|\n",
      "|              status|       string|   null|\n",
      "|# Partition Infor...|             |       |\n",
      "|          # col_name|    data_type|comment|\n",
      "|          order_date|         date|   null|\n",
      "+--------------------+-------------+-------+\n",
      "\n",
      "í˜„ì¬ CREATE TABLE ë¬¸:\n",
      "CREATE TABLE demo.blog.orders_mor (\n",
      "  order_id BIGINT,\n",
      "  customer_id BIGINT,\n",
      "  product_name STRING,\n",
      "  order_date DATE,\n",
      "  amount DECIMAL(10,2),\n",
      "  status STRING)\n",
      "USING iceberg\n",
      "PARTITIONED BY (order_date)\n",
      "LOCATION 'file:///home/jovyan/data/warehouse/blog/orders_mor'\n",
      "TBLPROPERTIES (\n",
      "  'current-snapshot-id' = '2226134241659144142',\n",
      "  'format' = 'iceberg/parquet',\n",
      "  'format-version' = '2',\n",
      "  'write.delete.mode' = 'merge-on-read',\n",
      "  'write.merge.mode' = 'merge-on-read',\n",
      "  'write.parquet.compression-codec' = 'zstd',\n",
      "  'write.update.mode' = 'merge-on-read')\n",
      "\n",
      "\n",
      "â• Step 1: ìƒˆ ì»¬ëŸ¼ ì¶”ê°€\n",
      "====================\n",
      "âœ… ìƒˆ ì»¬ëŸ¼ 3ê°œ ì¶”ê°€ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“‹ ë³€ê²½ëœ ìŠ¤í‚¤ë§ˆ:\n",
      "+--------------------+-------------+---------+\n",
      "|            col_name|    data_type|  comment|\n",
      "+--------------------+-------------+---------+\n",
      "|            order_id|       bigint|     null|\n",
      "|         customer_id|       bigint|     null|\n",
      "|        product_name|       string|     null|\n",
      "|          order_date|         date|     null|\n",
      "|              amount|decimal(10,2)|     null|\n",
      "|              status|       string|     null|\n",
      "|       discount_rate| decimal(5,2)|   í• ì¸ìœ¨|\n",
      "|    shipping_address|       string|ë°°ì†¡ ì£¼ì†Œ|\n",
      "|          created_at|    timestamp|ìƒì„± ì‹œê°„|\n",
      "|# Partition Infor...|             |         |\n",
      "|          # col_name|    data_type|  comment|\n",
      "|          order_date|         date|     null|\n",
      "+--------------------+-------------+---------+\n",
      "\n",
      "\n",
      "ğŸ“ Step 2: ìƒˆ ìŠ¤í‚¤ë§ˆë¡œ ë°ì´í„° ì‚½ì…\n",
      "==============================\n",
      "âœ… ìƒˆ ìŠ¤í‚¤ë§ˆë¡œ ë°ì´í„° ì‚½ì… ì™„ë£Œ!\n",
      "\n",
      "ğŸ” Step 3: ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± í™•ì¸\n",
      "==============================\n",
      "ğŸ“Š ì „ì²´ ë°ì´í„° (NULL ì²˜ë¦¬ í™•ì¸):\n",
      "+--------+---------------+-------------+----------------+--------------------------+\n",
      "|order_id|product_name   |discount_rate|shipping_address|created_at                |\n",
      "+--------+---------------+-------------+----------------+--------------------------+\n",
      "|2002    |MacBook Air M3 |0.05         |ë¶€ì‚°ì‹œ í•´ìš´ëŒ€êµ¬ |2025-08-16 11:50:01.515208|\n",
      "|2001    |New iPhone 15  |0.10         |ì„œìš¸ì‹œ ê°•ë‚¨êµ¬   |2025-08-16 11:50:01.514961|\n",
      "|999     |New Product    |null         |null            |null                      |\n",
      "|998     |Another Product|null         |null            |null                      |\n",
      "|100     |Apple TV       |null         |null            |null                      |\n",
      "|99      |Apple TV       |null         |null            |null                      |\n",
      "|98      |Mac Mini       |null         |null            |null                      |\n",
      "|97      |Pro Display    |null         |null            |null                      |\n",
      "|96      |Mac Studio     |null         |null            |null                      |\n",
      "|95      |Apple TV       |null         |null            |null                      |\n",
      "+--------+---------------+-------------+----------------+--------------------------+\n",
      "\n",
      "\n",
      "ğŸ“ˆ ìƒˆ ì»¬ëŸ¼ ì±„ì›€ìœ¨:\n",
      "  ì´ ë ˆì½”ë“œ: 80\n",
      "  discount_rate ì±„ì›€ìœ¨: 2.50%\n",
      "\n",
      "ğŸ”„ Step 4: ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\n",
      "====================\n",
      "âœ… shipping_address â†’ delivery_address ë³€ê²½ ì™„ë£Œ!\n",
      "\n",
      "ğŸ”„ Step 5: ì»¬ëŸ¼ íƒ€ì… í™•ì¥\n",
      "====================\n",
      "âœ… amount ì»¬ëŸ¼ íƒ€ì… í™•ì¥ ì™„ë£Œ!\n",
      "\n",
      "ğŸ“š Step 6: ìŠ¤í‚¤ë§ˆ íˆìŠ¤í† ë¦¬ í™•ì¸\n",
      "==============================\n",
      "ğŸ“‹ ìŠ¤í‚¤ë§ˆ ì§„í™” íˆìŠ¤í† ë¦¬:\n",
      "  ìŠ¤í‚¤ë§ˆ #0 (ì´ì „):\n",
      "    ì»¬ëŸ¼ ìˆ˜: 6ê°œ\n",
      "    ì»¬ëŸ¼ë“¤: ['order_id', 'customer_id', 'product_name', 'order_date', 'amount', 'status']\n",
      "  ìŠ¤í‚¤ë§ˆ #1 (ì´ì „):\n",
      "    ì»¬ëŸ¼ ìˆ˜: 9ê°œ\n",
      "    ì»¬ëŸ¼ë“¤: ['order_id', 'customer_id', 'product_name', 'order_date', 'amount', 'status', 'discount_rate', 'shipping_address', 'created_at']\n",
      "  ìŠ¤í‚¤ë§ˆ #2 (ì´ì „):\n",
      "    ì»¬ëŸ¼ ìˆ˜: 9ê°œ\n",
      "    ì»¬ëŸ¼ë“¤: ['order_id', 'customer_id', 'product_name', 'order_date', 'amount', 'status', 'discount_rate', 'delivery_address', 'created_at']\n",
      "  ìŠ¤í‚¤ë§ˆ #3 (í˜„ì¬):\n",
      "    ì»¬ëŸ¼ ìˆ˜: 9ê°œ\n",
      "    ì»¬ëŸ¼ë“¤: ['order_id', 'customer_id', 'product_name', 'order_date', 'amount', 'status', 'discount_rate', 'delivery_address', 'created_at']\n",
      "\n",
      "â° Step 7: ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì „ Time Travel\n",
      "==============================\n",
      "ğŸ“¸ ì²« ë²ˆì§¸ ìŠ¤ëƒ…ìƒ· (ID: 3289915886533077444) ë°ì´í„°:\n",
      "+--------+--------------+-------+----------+\n",
      "|order_id|  product_name| amount|    status|\n",
      "+--------+--------------+-------+----------+\n",
      "|       1|   Pro Display|5373.62|   pending|\n",
      "|       2|     iPhone 13| 671.06|   pending|\n",
      "|       3|Studio Display|1803.00|processing|\n",
      "|       4|      Apple TV| 179.95| completed|\n",
      "|       5|          iMac|1754.08|   pending|\n",
      "+--------+--------------+-------+----------+\n",
      "\n",
      "ğŸ‘‰ ìƒˆ ì»¬ëŸ¼ë“¤ì€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ (ìŠ¤í‚¤ë§ˆ ì§„í™” ì „)\n",
      "\n",
      "ğŸ’¡ Schema Evolution ëª¨ë²” ì‚¬ë¡€:\n",
      "========================================\n",
      "âœ… ê¶Œì¥ ì‚¬í•­:\n",
      "  ğŸ”¹ í•­ìƒ ì»¬ëŸ¼ ì¶”ê°€ë¶€í„° ì‹œì‘\n",
      "  ğŸ”¹ NULL í—ˆìš©ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° í˜¸í™˜ì„± ìœ ì§€\n",
      "  ğŸ”¹ íƒ€ì… í™•ì¥ì€ í˜¸í™˜ ê°€ëŠ¥í•œ ë°©í–¥ìœ¼ë¡œë§Œ\n",
      "  ğŸ”¹ ì»¬ëŸ¼ ì‚­ì œëŠ” ì‹ ì¤‘í•˜ê²Œ (ì½ê¸° ìŠ¤í‚¤ë§ˆ í™œìš©)\n",
      "  ğŸ”¹ ë³€ê²½ ì „í›„ ë°ì´í„° ê²€ì¦ í•„ìˆ˜\n",
      "\n",
      "âŒ ì£¼ì˜ ì‚¬í•­:\n",
      "  ğŸ”¸ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê¸ˆì§€\n",
      "  ğŸ”¸ ê¸°ì¡´ ì»¬ëŸ¼ ì‚­ì œ ì‹œ Time Travel ê³ ë ¤\n",
      "  ğŸ”¸ ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì—ì„œëŠ” ì„±ëŠ¥ ì˜í–¥ ê²€í† \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ”„ Schema Evolution ì™„ì „ ì‹¤ìŠµ\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ”„ Schema Evolution ì™„ì „ ì‹¤ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. í˜„ì¬ ìŠ¤í‚¤ë§ˆ í™•ì¸\n",
    "print(\"ğŸ“‹ í˜„ì¬ MOR í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ:\")\n",
    "spark.sql(\"DESCRIBE demo.blog.orders_mor\").show()\n",
    "\n",
    "current_schema = spark.sql(\"SHOW CREATE TABLE demo.blog.orders_mor\").collect()[0][0]\n",
    "print(f\"í˜„ì¬ CREATE TABLE ë¬¸:\\n{current_schema}\")\n",
    "\n",
    "# 2. ì»¬ëŸ¼ ì¶”ê°€ (ê°€ì¥ ì¼ë°˜ì ì¸ ì§„í™”)\n",
    "print(f\"\\nâ• Step 1: ìƒˆ ì»¬ëŸ¼ ì¶”ê°€\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# ìƒˆ ì»¬ëŸ¼ë“¤ ì¶”ê°€\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.blog.orders_mor \n",
    "    ADD COLUMNS (\n",
    "        discount_rate DECIMAL(5,2) COMMENT 'í• ì¸ìœ¨',\n",
    "        shipping_address STRING COMMENT 'ë°°ì†¡ ì£¼ì†Œ',\n",
    "        created_at TIMESTAMP COMMENT 'ìƒì„± ì‹œê°„'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… ìƒˆ ì»¬ëŸ¼ 3ê°œ ì¶”ê°€ ì™„ë£Œ!\")\n",
    "\n",
    "# ìŠ¤í‚¤ë§ˆ ë³€ê²½ í™•ì¸\n",
    "print(\"\\nğŸ“‹ ë³€ê²½ëœ ìŠ¤í‚¤ë§ˆ:\")\n",
    "spark.sql(\"DESCRIBE demo.blog.orders_mor\").show()\n",
    "\n",
    "# 3. ìƒˆ ì»¬ëŸ¼ì— ë°ì´í„° ì‚½ì…\n",
    "print(f\"\\nğŸ“ Step 2: ìƒˆ ìŠ¤í‚¤ë§ˆë¡œ ë°ì´í„° ì‚½ì…\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO demo.blog.orders_mor VALUES \n",
    "    (2001, 500, 'New iPhone 15', DATE('2023-04-15'), 1199.99, 'new', \n",
    "     0.10, 'ì„œìš¸ì‹œ ê°•ë‚¨êµ¬', CURRENT_TIMESTAMP()),\n",
    "    (2002, 501, 'MacBook Air M3', DATE('2023-04-16'), 1599.99, 'processing',\n",
    "     0.05, 'ë¶€ì‚°ì‹œ í•´ìš´ëŒ€êµ¬', CURRENT_TIMESTAMP())\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… ìƒˆ ìŠ¤í‚¤ë§ˆë¡œ ë°ì´í„° ì‚½ì… ì™„ë£Œ!\")\n",
    "\n",
    "# 4. ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° í˜¸í™˜ì„± í™•ì¸\n",
    "print(f\"\\nğŸ” Step 3: ìŠ¤í‚¤ë§ˆ í˜¸í™˜ì„± í™•ì¸\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"ğŸ“Š ì „ì²´ ë°ì´í„° (NULL ì²˜ë¦¬ í™•ì¸):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, product_name, discount_rate, shipping_address, created_at\n",
    "    FROM demo.blog.orders_mor \n",
    "    ORDER BY order_id DESC \n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# ìƒˆ ì»¬ëŸ¼ì˜ NULL ë¹„ìœ¨ í™•ì¸\n",
    "null_stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(discount_rate) as discount_rate_non_null,\n",
    "        COUNT(shipping_address) as shipping_address_non_null,\n",
    "        COUNT(created_at) as created_at_non_null,\n",
    "        ROUND(COUNT(discount_rate) * 100.0 / COUNT(*), 2) as discount_rate_fill_rate\n",
    "    FROM demo.blog.orders_mor\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ìƒˆ ì»¬ëŸ¼ ì±„ì›€ìœ¨:\")\n",
    "print(f\"  ì´ ë ˆì½”ë“œ: {null_stats['total_records']}\")\n",
    "print(f\"  discount_rate ì±„ì›€ìœ¨: {null_stats['discount_rate_fill_rate']}%\")\n",
    "\n",
    "# 5. ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\n",
    "print(f\"\\nğŸ”„ Step 4: ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.blog.orders_mor \n",
    "    RENAME COLUMN shipping_address TO delivery_address\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… shipping_address â†’ delivery_address ë³€ê²½ ì™„ë£Œ!\")\n",
    "\n",
    "# 6. ì»¬ëŸ¼ íƒ€ì… ì§„í™” (í˜¸í™˜ ê°€ëŠ¥í•œ íƒ€ì…ë§Œ)\n",
    "print(f\"\\nğŸ”„ Step 5: ì»¬ëŸ¼ íƒ€ì… í™•ì¥\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# ì˜ˆ: DECIMAL(10,2) â†’ DECIMAL(12,2) (ë” í° ì •ë°€ë„)\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.blog.orders_mor \n",
    "    ALTER COLUMN amount TYPE DECIMAL(12,2)\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… amount ì»¬ëŸ¼ íƒ€ì… í™•ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "# 7. ìŠ¤í‚¤ë§ˆ íˆìŠ¤í† ë¦¬ í™•ì¸\n",
    "print(f\"\\nğŸ“š Step 6: ìŠ¤í‚¤ë§ˆ íˆìŠ¤í† ë¦¬ í™•ì¸\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# í…Œì´ë¸”ì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ìŠ¤í‚¤ë§ˆ ì§„í™” í™•ì¸\n",
    "metadata_path = f\"{mor_table_path}/metadata\"\n",
    "metadata_files = [f for f in os.listdir(metadata_path) if f.endswith('.metadata.json')]\n",
    "latest_metadata_file = max(metadata_files, key=lambda x: os.path.getctime(os.path.join(metadata_path, x)))\n",
    "\n",
    "import json\n",
    "with open(os.path.join(metadata_path, latest_metadata_file), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"ğŸ“‹ ìŠ¤í‚¤ë§ˆ ì§„í™” íˆìŠ¤í† ë¦¬:\")\n",
    "for i, schema in enumerate(metadata['schemas']):\n",
    "    schema_id = schema['schema-id']\n",
    "    field_count = len(schema['fields'])\n",
    "    is_current = schema_id == metadata['current-schema-id']\n",
    "    \n",
    "    print(f\"  ìŠ¤í‚¤ë§ˆ #{schema_id} ({'í˜„ì¬' if is_current else 'ì´ì „'}):\")\n",
    "    print(f\"    ì»¬ëŸ¼ ìˆ˜: {field_count}ê°œ\")\n",
    "    print(f\"    ì»¬ëŸ¼ë“¤: {[f['name'] for f in schema['fields']]}\")\n",
    "\n",
    "# 8. Time Travelë¡œ ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì „ ë°ì´í„° ì¡°íšŒ\n",
    "print(f\"\\nâ° Step 7: ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì „ Time Travel\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ì²« ë²ˆì§¸ ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ëŒì•„ê°€ê¸° (ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì „)\n",
    "snapshots = spark.sql(\"SELECT * FROM demo.blog.orders_mor.snapshots ORDER BY committed_at\").collect()\n",
    "if len(snapshots) > 0:\n",
    "    first_snapshot = snapshots[0]['snapshot_id']\n",
    "    \n",
    "    print(f\"ğŸ“¸ ì²« ë²ˆì§¸ ìŠ¤ëƒ…ìƒ· (ID: {first_snapshot}) ë°ì´í„°:\")\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT order_id, product_name, amount, status\n",
    "            FROM demo.blog.orders_mor \n",
    "            VERSION AS OF {first_snapshot}\n",
    "            ORDER BY order_id \n",
    "            LIMIT 5\n",
    "        \"\"\").show()\n",
    "        \n",
    "        print(\"ğŸ‘‰ ìƒˆ ì»¬ëŸ¼ë“¤ì€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ (ìŠ¤í‚¤ë§ˆ ì§„í™” ì „)\")\n",
    "    except Exception as e:\n",
    "        print(f\"ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# 9. ìŠ¤í‚¤ë§ˆ ì§„í™” ëª¨ë²” ì‚¬ë¡€\n",
    "print(f\"\\nğŸ’¡ Schema Evolution ëª¨ë²” ì‚¬ë¡€:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"âœ… ê¶Œì¥ ì‚¬í•­:\")\n",
    "print(\"  ğŸ”¹ í•­ìƒ ì»¬ëŸ¼ ì¶”ê°€ë¶€í„° ì‹œì‘\")\n",
    "print(\"  ğŸ”¹ NULL í—ˆìš©ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° í˜¸í™˜ì„± ìœ ì§€\")\n",
    "print(\"  ğŸ”¹ íƒ€ì… í™•ì¥ì€ í˜¸í™˜ ê°€ëŠ¥í•œ ë°©í–¥ìœ¼ë¡œë§Œ\")\n",
    "print(\"  ğŸ”¹ ì»¬ëŸ¼ ì‚­ì œëŠ” ì‹ ì¤‘í•˜ê²Œ (ì½ê¸° ìŠ¤í‚¤ë§ˆ í™œìš©)\")\n",
    "print(\"  ğŸ”¹ ë³€ê²½ ì „í›„ ë°ì´í„° ê²€ì¦ í•„ìˆ˜\")\n",
    "\n",
    "print(\"\\nâŒ ì£¼ì˜ ì‚¬í•­:\")\n",
    "print(\"  ğŸ”¸ í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ ê¸ˆì§€\")\n",
    "print(\"  ğŸ”¸ ê¸°ì¡´ ì»¬ëŸ¼ ì‚­ì œ ì‹œ Time Travel ê³ ë ¤\")\n",
    "print(\"  ğŸ”¸ ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì—ì„œëŠ” ì„±ëŠ¥ ì˜í–¥ ê²€í† \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37da3a5b-e4bb-4326-80e0-0f50a21351e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—œï¸ Compaction (íŒŒì¼ ìµœì í™”) ì™„ì „ ì‹¤ìŠµ\n",
      "==================================================\n",
      "ğŸ“Š Compaction ì „ íŒŒì¼ ìƒíƒœ ë¶„ì„:\n",
      "==============================\n",
      "ğŸ“„ ì´ ë°ì´í„° íŒŒì¼: 74ê°œ\n",
      "ğŸ—‘ï¸ Delete íŒŒì¼: 29ê°œ\n",
      "ğŸ’¾ ì´ ë°ì´í„° í¬ê¸°: 137,935 bytes (0.13 MB)\n",
      "ğŸ“ í‰ê·  íŒŒì¼ í¬ê¸°: 1,864 bytes (1.8 KB)\n",
      "ğŸ“ ìµœì†Œ íŒŒì¼ í¬ê¸°: 1,753 bytes\n",
      "ğŸ“ ìµœëŒ€ íŒŒì¼ í¬ê¸°: 2,821 bytes\n",
      "ğŸ” ì‘ì€ íŒŒì¼ (< 100KB): 74ê°œ\n",
      "\n",
      "ğŸ“ˆ Compaction ì „ í…Œì´ë¸” ì„±ëŠ¥:\n",
      "==============================\n",
      "ğŸ“Š ì´ ë ˆì½”ë“œ: 80ê°œ\n",
      "ğŸ“Š íŒŒì¼ë‹¹ í‰ê·  ë ˆì½”ë“œ: 1ê°œ\n",
      "\n",
      "ğŸ“‹ íŒŒì¼ë³„ ìƒì„¸ ì •ë³´ (ì²˜ìŒ 5ê°œ):\n",
      "  1. /data/order_date=2023-01-01/00000-5-226e50bc-6f72-465e-be45-604bea069c98-00001.parquet\n",
      "     í¬ê¸°: 1,834 bytes (1.8 KB)\n",
      "     ë ˆì½”ë“œ: 1ê°œ\n",
      "  2. /data/order_date=2023-01-02/00000-5-226e50bc-6f72-465e-be45-604bea069c98-00002.parquet\n",
      "     í¬ê¸°: 1,865 bytes (1.8 KB)\n",
      "     ë ˆì½”ë“œ: 2ê°œ\n",
      "  3. /data/order_date=2023-01-03/00000-20-e1625583-dde6-47ea-bd2c-cdeb07d86e69-00001.parquet\n",
      "     í¬ê¸°: 1,806 bytes (1.8 KB)\n",
      "     ë ˆì½”ë“œ: 1ê°œ\n",
      "  4. /data/order_date=2023-01-03/00000-5-226e50bc-6f72-465e-be45-604bea069c98-00003.parquet\n",
      "     í¬ê¸°: 1,814 bytes (1.8 KB)\n",
      "     ë ˆì½”ë“œ: 1ê°œ\n",
      "  5. /data/order_date=2023-01-05/00000-5-226e50bc-6f72-465e-be45-604bea069c98-00004.parquet\n",
      "     í¬ê¸°: 1,835 bytes (1.8 KB)\n",
      "     ë ˆì½”ë“œ: 1ê°œ\n",
      "\n",
      "ğŸ—œï¸ Data Files Compaction ì‹¤í–‰:\n",
      "==============================\n",
      "ğŸ”„ Data files compaction ì‹œì‘...\n",
      "âœ… Data files compaction ì™„ë£Œ!\n",
      "\n",
      "ğŸ“Š Compaction í›„ íŒŒì¼ ìƒíƒœ ë¶„ì„:\n",
      "==============================\n",
      "ğŸ“„ Compaction í›„ ë°ì´í„° íŒŒì¼: 74ê°œ\n",
      "ğŸ—‘ï¸ Delete íŒŒì¼: 29ê°œ\n",
      "ğŸ’¾ ì´ ë°ì´í„° í¬ê¸°: 137,935 bytes (0.13 MB)\n",
      "ğŸ“ í‰ê·  íŒŒì¼ í¬ê¸°: 1,864 bytes (1.8 KB)\n",
      "\n",
      "ğŸ“ˆ Compaction íš¨ê³¼ ë¶„ì„:\n",
      "==============================\n",
      "ğŸ“Š íŒŒì¼ ìˆ˜ ë³€í™”: 74 â†’ 74 (+0ê°œ, +0.0%)\n",
      "ğŸ’¾ í¬ê¸° ë³€í™”: +0 bytes (+0.0%)\n",
      "â±ï¸ Compaction ì‹œê°„: 0.53ì´ˆ\n",
      "ğŸ“ í‰ê·  íŒŒì¼ í¬ê¸° ë³€í™”: +0 bytes (+0.0%)\n",
      "\n",
      "ğŸ—‘ï¸ Delete Files Compaction:\n",
      "==============================\n",
      "í˜„ì¬ Delete íŒŒì¼: 29ê°œ\n",
      "ğŸ”„ Delete files compaction ì‹œë„...\n",
      "âœ… Delete files compaction ì™„ë£Œ!\n",
      "\n",
      "ğŸš€ Compaction í›„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\n",
      "==============================\n",
      "\n",
      "ğŸ“Š ì¿¼ë¦¬ #1: SELECT COUNT(*) FROM demo.blog.orders_mor\n",
      "  â±ï¸ ì‹¤í–‰ ì‹œê°„: 0.904ì´ˆ\n",
      "\n",
      "ğŸ“Š ì¿¼ë¦¬ #2: SELECT * FROM demo.blog.orders_mor WHERE order_date = '2023-01-15'\n",
      "  â±ï¸ ì‹¤í–‰ ì‹œê°„: 0.252ì´ˆ\n",
      "\n",
      "ğŸ“Š ì¿¼ë¦¬ #3: SELECT product_name, SUM(amount) FROM demo.blog.orders_mor GROUP BY product_name\n",
      "  â±ï¸ ì‹¤í–‰ ì‹œê°„: 2.884ì´ˆ\n",
      "\n",
      "ğŸ’¡ Compaction ëª¨ë²” ì‚¬ë¡€:\n",
      "==============================\n",
      "âœ… ì–¸ì œ Compaction í•´ì•¼ í•˜ë‚˜:\n",
      "  ğŸ”¹ ì‘ì€ íŒŒì¼ì´ ë§ì„ ë•Œ (< 100MB)\n",
      "  ğŸ”¹ Delete íŒŒì¼ì´ ë§ì´ ìŒ“ì˜€ì„ ë•Œ\n",
      "  ğŸ”¹ ì¿¼ë¦¬ ì„±ëŠ¥ì´ ëŠë ¤ì¡Œì„ ë•Œ\n",
      "  ğŸ”¹ ì •ê¸°ì ì¸ ìœ ì§€ë³´ìˆ˜ ì‘ì—…ìœ¼ë¡œ\n",
      "\n",
      "âš™ï¸ Compaction ì „ëµ:\n",
      "  ğŸ”¹ ì˜¤í”„ í”¼í¬ ì‹œê°„ì— ì‹¤í–‰\n",
      "  ğŸ”¹ íŒŒí‹°ì…˜ë³„ë¡œ ë‚˜ëˆ„ì–´ ì‹¤í–‰\n",
      "  ğŸ”¹ íƒ€ê²Ÿ íŒŒì¼ í¬ê¸° ì„¤ì • (128MB~1GB)\n",
      "  ğŸ”¹ Compaction í›„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ğŸ—œï¸ Compaction (íŒŒì¼ ìµœì í™”) ì™„ì „ ì‹¤ìŠµ\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ—œï¸ Compaction (íŒŒì¼ ìµœì í™”) ì™„ì „ ì‹¤ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Compaction ì „ ìƒíƒœ ë¶„ì„\n",
    "print(\"ğŸ“Š Compaction ì „ íŒŒì¼ ìƒíƒœ ë¶„ì„:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# í˜„ì¬ íŒŒì¼ ìƒíƒœ í™•ì¸\n",
    "current_files = glob.glob(f\"{mor_table_path}/**/*.parquet\", recursive=True)\n",
    "data_files = [f for f in current_files if 'delete' not in os.path.basename(f).lower()]\n",
    "delete_files = [f for f in current_files if 'delete' in os.path.basename(f).lower()]\n",
    "\n",
    "print(f\"ğŸ“„ ì´ ë°ì´í„° íŒŒì¼: {len(data_files)}ê°œ\")\n",
    "print(f\"ğŸ—‘ï¸ Delete íŒŒì¼: {len(delete_files)}ê°œ\")\n",
    "\n",
    "# íŒŒì¼ í¬ê¸° ë¶„ì„\n",
    "file_sizes = [os.path.getsize(f) for f in data_files]\n",
    "total_size = sum(file_sizes)\n",
    "avg_size = total_size / len(file_sizes) if file_sizes else 0\n",
    "\n",
    "print(f\"ğŸ’¾ ì´ ë°ì´í„° í¬ê¸°: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)\")\n",
    "print(f\"ğŸ“ í‰ê·  íŒŒì¼ í¬ê¸°: {avg_size:,.0f} bytes ({avg_size/1024:.1f} KB)\")\n",
    "print(f\"ğŸ“ ìµœì†Œ íŒŒì¼ í¬ê¸°: {min(file_sizes):,} bytes\")\n",
    "print(f\"ğŸ“ ìµœëŒ€ íŒŒì¼ í¬ê¸°: {max(file_sizes):,} bytes\")\n",
    "\n",
    "# ì‘ì€ íŒŒì¼ë“¤ ì‹ë³„ (100KB ë¯¸ë§Œ)\n",
    "small_files = [f for f in data_files if os.path.getsize(f) < 100 * 1024]\n",
    "print(f\"ğŸ” ì‘ì€ íŒŒì¼ (< 100KB): {len(small_files)}ê°œ\")\n",
    "\n",
    "# 2. í˜„ì¬ í…Œì´ë¸” ì„±ëŠ¥ í™•ì¸\n",
    "print(f\"\\nğŸ“ˆ Compaction ì „ í…Œì´ë¸” ì„±ëŠ¥:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ë ˆì½”ë“œ ìˆ˜ì™€ íŒŒì¼ ìˆ˜ ë¹„ìœ¨\n",
    "total_records = spark.sql(\"SELECT COUNT(*) FROM demo.blog.orders_mor\").collect()[0][0]\n",
    "records_per_file = total_records / len(data_files) if data_files else 0\n",
    "\n",
    "print(f\"ğŸ“Š ì´ ë ˆì½”ë“œ: {total_records:,}ê°œ\")\n",
    "print(f\"ğŸ“Š íŒŒì¼ë‹¹ í‰ê·  ë ˆì½”ë“œ: {records_per_file:.0f}ê°œ\")\n",
    "\n",
    "# íŒŒì¼ë³„ ìƒì„¸ ì •ë³´\n",
    "print(f\"\\nğŸ“‹ íŒŒì¼ë³„ ìƒì„¸ ì •ë³´ (ì²˜ìŒ 5ê°œ):\")\n",
    "for i, file_path in enumerate(data_files[:5]):\n",
    "    size = os.path.getsize(file_path)\n",
    "    relative_path = file_path.replace(mor_table_path, \"\")\n",
    "    \n",
    "    # íŒŒì¼ì˜ ë ˆì½”ë“œ ìˆ˜ í™•ì¸\n",
    "    try:\n",
    "        file_df = spark.read.parquet(file_path)\n",
    "        record_count = file_df.count()\n",
    "    except:\n",
    "        record_count = 0\n",
    "    \n",
    "    print(f\"  {i+1}. {relative_path}\")\n",
    "    print(f\"     í¬ê¸°: {size:,} bytes ({size/1024:.1f} KB)\")\n",
    "    print(f\"     ë ˆì½”ë“œ: {record_count:,}ê°œ\")\n",
    "\n",
    "# 3. Data Files Compaction ì‹¤í–‰\n",
    "print(f\"\\nğŸ—œï¸ Data Files Compaction ì‹¤í–‰:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Compaction ì „ ì‹œê°„ ê¸°ë¡\n",
    "import time\n",
    "compaction_start_time = time.time()\n",
    "\n",
    "# ì‹¤ì œ Compaction ì‹¤í–‰ (Spark 3.x ê¸°ì¤€)\n",
    "try:\n",
    "    print(\"ğŸ”„ Data files compaction ì‹œì‘...\")\n",
    "    \n",
    "    # ë°©ë²• 1: Spark í”„ë¡œì‹œì € ì‚¬ìš©\n",
    "    spark.sql(\"\"\"\n",
    "        CALL demo.system.rewrite_data_files(\n",
    "            table => 'demo.blog.orders_mor'\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"âœ… Data files compaction ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Compaction ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ëŒ€ì•ˆ ë°©ë²• ì‹œë„...\")\n",
    "    \n",
    "    # ë°©ë²• 2: ìˆ˜ë™ Compaction (INSERT OVERWRITE)\n",
    "    try:\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TABLE demo.blog.orders_mor_temp \n",
    "            USING ICEBERG\n",
    "            PARTITIONED BY (order_date)\n",
    "            TBLPROPERTIES (\n",
    "                'write.delete.mode' = 'merge-on-read',\n",
    "                'write.update.mode' = 'merge-on-read'\n",
    "            )\n",
    "            AS SELECT * FROM demo.blog.orders_mor\n",
    "        \"\"\")\n",
    "        \n",
    "        spark.sql(\"DROP TABLE demo.blog.orders_mor\")\n",
    "        spark.sql(\"ALTER TABLE demo.blog.orders_mor_temp RENAME TO demo.blog.orders_mor\")\n",
    "        \n",
    "        print(\"âœ… ìˆ˜ë™ Compaction ì™„ë£Œ!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ ìˆ˜ë™ Compactionë„ ì‹¤íŒ¨: {e2}\")\n",
    "\n",
    "compaction_time = time.time() - compaction_start_time\n",
    "\n",
    "# 4. Compaction í›„ ìƒíƒœ ë¶„ì„\n",
    "print(f\"\\nğŸ“Š Compaction í›„ íŒŒì¼ ìƒíƒœ ë¶„ì„:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ìƒˆë¡œìš´ íŒŒì¼ ìƒíƒœ í™•ì¸\n",
    "after_files = glob.glob(f\"{mor_table_path}/**/*.parquet\", recursive=True)\n",
    "after_data_files = [f for f in after_files if 'delete' not in os.path.basename(f).lower()]\n",
    "after_delete_files = [f for f in after_files if 'delete' in os.path.basename(f).lower()]\n",
    "\n",
    "print(f\"ğŸ“„ Compaction í›„ ë°ì´í„° íŒŒì¼: {len(after_data_files)}ê°œ\")\n",
    "print(f\"ğŸ—‘ï¸ Delete íŒŒì¼: {len(after_delete_files)}ê°œ\")\n",
    "\n",
    "# íŒŒì¼ í¬ê¸° ë¶„ì„\n",
    "if after_data_files:\n",
    "    after_file_sizes = [os.path.getsize(f) for f in after_data_files]\n",
    "    after_total_size = sum(after_file_sizes)\n",
    "    after_avg_size = after_total_size / len(after_file_sizes)\n",
    "    \n",
    "    print(f\"ğŸ’¾ ì´ ë°ì´í„° í¬ê¸°: {after_total_size:,} bytes ({after_total_size/1024/1024:.2f} MB)\")\n",
    "    print(f\"ğŸ“ í‰ê·  íŒŒì¼ í¬ê¸°: {after_avg_size:,.0f} bytes ({after_avg_size/1024:.1f} KB)\")\n",
    "\n",
    "# 5. Compaction íš¨ê³¼ ë¶„ì„\n",
    "print(f\"\\nğŸ“ˆ Compaction íš¨ê³¼ ë¶„ì„:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "file_reduction = len(data_files) - len(after_data_files)\n",
    "file_reduction_pct = (file_reduction / len(data_files) * 100) if data_files else 0\n",
    "\n",
    "size_change = after_total_size - total_size if after_data_files else 0\n",
    "size_change_pct = (size_change / total_size * 100) if total_size > 0 else 0\n",
    "\n",
    "print(f\"ğŸ“Š íŒŒì¼ ìˆ˜ ë³€í™”: {len(data_files)} â†’ {len(after_data_files)} ({file_reduction:+d}ê°œ, {file_reduction_pct:+.1f}%)\")\n",
    "print(f\"ğŸ’¾ í¬ê¸° ë³€í™”: {size_change:+,} bytes ({size_change_pct:+.1f}%)\")\n",
    "print(f\"â±ï¸ Compaction ì‹œê°„: {compaction_time:.2f}ì´ˆ\")\n",
    "\n",
    "if after_data_files:\n",
    "    avg_size_change = after_avg_size - avg_size\n",
    "    avg_size_change_pct = (avg_size_change / avg_size * 100) if avg_size > 0 else 0\n",
    "    print(f\"ğŸ“ í‰ê·  íŒŒì¼ í¬ê¸° ë³€í™”: {avg_size_change:+,.0f} bytes ({avg_size_change_pct:+.1f}%)\")\n",
    "\n",
    "# 6. Delete Files Compaction\n",
    "print(f\"\\nğŸ—‘ï¸ Delete Files Compaction:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if after_delete_files:\n",
    "    print(f\"í˜„ì¬ Delete íŒŒì¼: {len(after_delete_files)}ê°œ\")\n",
    "    print(\"ğŸ”„ Delete files compaction ì‹œë„...\")\n",
    "    \n",
    "    try:\n",
    "        # Delete files compaction\n",
    "        spark.sql(\"\"\"\n",
    "            CALL demo.system.rewrite_manifests(\n",
    "                table => 'demo.blog.orders_mor'\n",
    "            )\n",
    "        \"\"\")\n",
    "        print(\"âœ… Delete files compaction ì™„ë£Œ!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Delete files compaction ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ğŸ‘‰ ì¼ë¶€ Spark ë²„ì „ì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âœ… Delete íŒŒì¼ì´ ì—†ì–´ Compaction ë¶ˆí•„ìš”\")\n",
    "\n",
    "# 7. ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "print(f\"\\nğŸš€ Compaction í›„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ê°„ë‹¨í•œ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •\n",
    "queries = [\n",
    "    \"SELECT COUNT(*) FROM demo.blog.orders_mor\",\n",
    "    \"SELECT * FROM demo.blog.orders_mor WHERE order_date = '2023-01-15'\",\n",
    "    \"SELECT product_name, SUM(amount) FROM demo.blog.orders_mor GROUP BY product_name\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    print(f\"\\nğŸ“Š ì¿¼ë¦¬ #{i+1}: {query}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = spark.sql(query)\n",
    "    result.collect()  # ì‹¤ì œ ì‹¤í–‰\n",
    "    query_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  â±ï¸ ì‹¤í–‰ ì‹œê°„: {query_time:.3f}ì´ˆ\")\n",
    "\n",
    "# 8. Compaction ëª¨ë²” ì‚¬ë¡€\n",
    "print(f\"\\nğŸ’¡ Compaction ëª¨ë²” ì‚¬ë¡€:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"âœ… ì–¸ì œ Compaction í•´ì•¼ í•˜ë‚˜:\")\n",
    "print(\"  ğŸ”¹ ì‘ì€ íŒŒì¼ì´ ë§ì„ ë•Œ (< 100MB)\")\n",
    "print(\"  ğŸ”¹ Delete íŒŒì¼ì´ ë§ì´ ìŒ“ì˜€ì„ ë•Œ\")\n",
    "print(\"  ğŸ”¹ ì¿¼ë¦¬ ì„±ëŠ¥ì´ ëŠë ¤ì¡Œì„ ë•Œ\")\n",
    "print(\"  ğŸ”¹ ì •ê¸°ì ì¸ ìœ ì§€ë³´ìˆ˜ ì‘ì—…ìœ¼ë¡œ\")\n",
    "\n",
    "print(\"\\nâš™ï¸ Compaction ì „ëµ:\")\n",
    "print(\"  ğŸ”¹ ì˜¤í”„ í”¼í¬ ì‹œê°„ì— ì‹¤í–‰\")\n",
    "print(\"  ğŸ”¹ íŒŒí‹°ì…˜ë³„ë¡œ ë‚˜ëˆ„ì–´ ì‹¤í–‰\")\n",
    "print(\"  ğŸ”¹ íƒ€ê²Ÿ íŒŒì¼ í¬ê¸° ì„¤ì • (128MB~1GB)\")\n",
    "print(\"  ğŸ”¹ Compaction í›„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18944a1-f125-4c20-bab2-e28987167f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ Iceberg ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ì „ í…ŒìŠ¤íŠ¸\n",
      "==================================================\n",
      "ğŸ“ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±:\n",
      "==============================\n",
      "ğŸ”„ ë²¤ì¹˜ë§ˆí¬ìš© ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìƒì„±...\n",
      "âœ… 1000ê°œ ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ!\n",
      "â±ï¸ ì‚½ì… ì‹œê°„: 9.46ì´ˆ\n",
      "\n",
      "ğŸ“– ì½ê¸° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:\n",
      "==============================\n",
      "\n",
      "ğŸ” ì „ì²´ ì¹´ìš´íŠ¸ í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ í‰ê· : 0.054ì´ˆ\n",
      "  â±ï¸ ìµœì†Œ: 0.048ì´ˆ\n",
      "  â±ï¸ ìµœëŒ€: 0.064ì´ˆ\n",
      "\n",
      "ğŸ” í•„í„°ë§ í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ í‰ê· : 0.858ì´ˆ\n",
      "  â±ï¸ ìµœì†Œ: 0.634ì´ˆ\n",
      "  â±ï¸ ìµœëŒ€: 1.284ì´ˆ\n",
      "\n",
      "ğŸ” ì§‘ê³„ í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ í‰ê· : 0.632ì´ˆ\n",
      "  â±ï¸ ìµœì†Œ: 0.523ì´ˆ\n",
      "  â±ï¸ ìµœëŒ€: 0.805ì´ˆ\n",
      "\n",
      "ğŸ” íŒŒí‹°ì…˜ ìŠ¤ìº” í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ í‰ê· : 0.171ì´ˆ\n",
      "  â±ï¸ ìµœì†Œ: 0.069ì´ˆ\n",
      "  â±ï¸ ìµœëŒ€: 0.347ì´ˆ\n",
      "\n",
      "ğŸ” ì¡°ì¸ í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ í‰ê· : 0.805ì´ˆ\n",
      "  â±ï¸ ìµœì†Œ: 0.624ì´ˆ\n",
      "  â±ï¸ ìµœëŒ€: 1.040ì´ˆ\n",
      "\n",
      "âœï¸ ì“°ê¸° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:\n",
      "==============================\n",
      "\n",
      "ğŸ“ ë°°ì¹˜ í¬ê¸° 10ê°œ ì‚½ì… í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ ì‹œê°„: 0.679ì´ˆ\n",
      "  ğŸš€ ì²˜ë¦¬ìœ¨: 14.7 records/sec\n",
      "\n",
      "ğŸ“ ë°°ì¹˜ í¬ê¸° 50ê°œ ì‚½ì… í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ ì‹œê°„: 0.958ì´ˆ\n",
      "  ğŸš€ ì²˜ë¦¬ìœ¨: 52.2 records/sec\n",
      "\n",
      "ğŸ“ ë°°ì¹˜ í¬ê¸° 100ê°œ ì‚½ì… í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ ì‹œê°„: 1.421ì´ˆ\n",
      "  ğŸš€ ì²˜ë¦¬ìœ¨: 70.4 records/sec\n",
      "\n",
      "ğŸ“ ë°°ì¹˜ í¬ê¸° 200ê°œ ì‚½ì… í…ŒìŠ¤íŠ¸:\n",
      "  â±ï¸ ì‹œê°„: 3.097ì´ˆ\n",
      "  ğŸš€ ì²˜ë¦¬ìœ¨: 64.6 records/sec\n",
      "\n",
      "ğŸ”„ ì—…ë°ì´íŠ¸/ì‚­ì œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:\n",
      "==============================\n",
      "ğŸ—‘ï¸ ì‚­ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\n",
      "\n",
      "  ì†ŒëŸ‰ ì‚­ì œ:\n",
      "    â±ï¸ ì‹œê°„: 1.115ì´ˆ\n",
      "\n",
      "  ì¡°ê±´ë¶€ ì‚­ì œ:\n",
      "    â±ï¸ ì‹œê°„: 0.733ì´ˆ\n",
      "\n",
      "  íŒŒí‹°ì…˜ ì‚­ì œ:\n",
      "    â±ï¸ ì‹œê°„: 0.181ì´ˆ\n",
      "\n",
      "ğŸ“ ì—…ë°ì´íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\n",
      "\n",
      "  ìƒíƒœ ì—…ë°ì´íŠ¸:\n",
      "    â±ï¸ ì‹œê°„: 0.427ì´ˆ\n",
      "\n",
      "  ê°€ê²© ì—…ë°ì´íŠ¸:\n",
      "    â±ï¸ ì‹œê°„: 2.161ì´ˆ\n",
      "\n",
      "â° Time Travel ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:\n",
      "==============================\n",
      "ğŸ“Š í˜„ì¬ ë°ì´í„° ì¡°íšŒ: 0.086ì´ˆ\n",
      "â° Time Travel ì¡°íšŒ: 0.078ì´ˆ\n",
      "ğŸ“ˆ Time Travel ì˜¤ë²„í—¤ë“œ: -8.9%\n",
      "\n",
      "ğŸ’¾ íŒŒì¼ I/O ë¶„ì„:\n",
      "====================\n",
      "ğŸ“„ ì´ íŒŒì¼ ìˆ˜: 755\n",
      "ğŸ’¾ ì´ í¬ê¸°: 1,417,173 bytes (1.35 MB)\n",
      "ğŸ“ í‰ê·  íŒŒì¼ í¬ê¸°: 1,877 bytes (1.8 KB)\n",
      "\n",
      "ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ë¦¬í¬íŠ¸:\n",
      "==================================================\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ í•­ëª©:\n",
      "  ğŸ“– ì½ê¸°: ì „ì²´ ì¹´ìš´íŠ¸ (0.054ì´ˆ)\n",
      "  âœï¸ ì“°ê¸°: ë°°ì¹˜ í¬ê¸° 10 (0.679ì´ˆ)\n",
      "\n",
      "ğŸ“ˆ ì„±ëŠ¥ íŠ¹ì„±:\n",
      "  ğŸ“Š í‰ê·  ì½ê¸° ì„±ëŠ¥: 0.504ì´ˆ\n",
      "  ğŸ“Š í‰ê·  ì“°ê¸° ì„±ëŠ¥: 1.539ì´ˆ\n",
      "  ğŸ“„ íŒŒì¼ë‹¹ í‰ê·  ë ˆì½”ë“œ: 1ê°œ\n",
      "\n",
      "ğŸ’¡ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­:\n",
      "  ğŸ”¹ íŒŒì¼ í¬ê¸°: 128MB~1GB ìœ ì§€\n",
      "  ğŸ”¹ íŒŒí‹°ì…”ë‹: ì¿¼ë¦¬ íŒ¨í„´ì— ë§ê²Œ ì„¤ê³„\n",
      "  ğŸ”¹ ì •ê¸°ì  Compaction ìˆ˜í–‰\n",
      "  ğŸ”¹ ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°ë¡œ ì“°ê¸°\n",
      "  ğŸ”¹ í•„ìš”ì‹œ Z-ordering ê³ ë ¤\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ë‹¤ë¥¸ í•„ìš”í•œ importsë„ í•¨ê»˜\n",
    "from pyspark.sql.functions import col, min as spark_min, max as spark_max\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ Iceberg ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ì „ í…ŒìŠ¤íŠ¸\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ Iceberg ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ì „ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (ë” í° ë°ì´í„°ì…‹)\n",
    "print(\"ğŸ“ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def generate_large_dataset(num_records=1000):\n",
    "    \"\"\"ë” í° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    products = [\n",
    "        'iPhone 14', 'iPhone 14 Pro', 'MacBook Pro', 'MacBook Air', 'iPad Air', \n",
    "        'iPad Pro', 'AirPods', 'AirPods Pro', 'Apple Watch', 'Magic Mouse',\n",
    "        'Magic Keyboard', 'iMac', 'Mac Studio', 'Apple TV', 'HomePod'\n",
    "    ] * 10  # ë” ë§ì€ variety\n",
    "    \n",
    "    statuses = ['completed', 'pending', 'shipped', 'cancelled', 'processing']\n",
    "    \n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2023, 12, 31)\n",
    "    \n",
    "    data = []\n",
    "    for i in range(num_records):\n",
    "        random_days = random.randint(0, (end_date - start_date).days)\n",
    "        order_date = start_date + timedelta(days=random_days)\n",
    "        \n",
    "        data.append({\n",
    "            'order_id': i + 10000,  # ê¸°ì¡´ ë°ì´í„°ì™€ êµ¬ë¶„\n",
    "            'customer_id': random.randint(1000, 9999),\n",
    "            'product_name': random.choice(products),\n",
    "            'order_date': order_date.strftime('%Y-%m-%d'),\n",
    "            'amount': round(random.uniform(50, 3000), 2),\n",
    "            'status': random.choice(statuses)\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ë²¤ì¹˜ë§ˆí¬ìš© ìƒˆ í…Œì´ë¸” ìƒì„±\n",
    "print(\"ğŸ”„ ë²¤ì¹˜ë§ˆí¬ìš© ëŒ€ìš©ëŸ‰ í…Œì´ë¸” ìƒì„±...\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS demo.blog.orders_benchmark\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE demo.blog.orders_benchmark (\n",
    "        order_id BIGINT,\n",
    "        customer_id BIGINT,\n",
    "        product_name STRING,\n",
    "        order_date DATE,\n",
    "        amount DECIMAL(10,2),\n",
    "        status STRING\n",
    "    ) USING ICEBERG\n",
    "    PARTITIONED BY (order_date)\n",
    "    TBLPROPERTIES (\n",
    "        'write.target-file-size-bytes' = '134217728'  -- 128MB\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„± ë° ì‚½ì…\n",
    "large_data = generate_large_dataset(1000)  # 1000ê°œ ë ˆì½”ë“œ\n",
    "df_large = spark.createDataFrame(pd.DataFrame(large_data))\n",
    "df_large = df_large.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    "\n",
    "start_time = time.time()\n",
    "df_large.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"demo.blog.orders_benchmark\")\n",
    "insert_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… 1000ê°œ ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ!\")\n",
    "print(f\"â±ï¸ ì‚½ì… ì‹œê°„: {insert_time:.2f}ì´ˆ\")\n",
    "\n",
    "# 2. ì½ê¸° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "print(f\"\\nğŸ“– ì½ê¸° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "read_queries = [\n",
    "    (\"ì „ì²´ ì¹´ìš´íŠ¸\", \"SELECT COUNT(*) FROM demo.blog.orders_benchmark\"),\n",
    "    (\"í•„í„°ë§\", \"SELECT * FROM demo.blog.orders_benchmark WHERE amount > 1000\"),\n",
    "    (\"ì§‘ê³„\", \"SELECT product_name, COUNT(*), AVG(amount) FROM demo.blog.orders_benchmark GROUP BY product_name\"),\n",
    "    (\"íŒŒí‹°ì…˜ ìŠ¤ìº”\", \"SELECT * FROM demo.blog.orders_benchmark WHERE order_date >= '2023-06-01' AND order_date <= '2023-06-30'\"),\n",
    "    (\"ì¡°ì¸\", \"\"\"\n",
    "        SELECT b1.product_name, COUNT(*) \n",
    "        FROM demo.blog.orders_benchmark b1 \n",
    "        JOIN demo.blog.orders_benchmark b2 ON b1.customer_id = b2.customer_id \n",
    "        GROUP BY b1.product_name\n",
    "    \"\"\")\n",
    "]\n",
    "\n",
    "read_results = {}\n",
    "for query_name, query in read_queries:\n",
    "    print(f\"\\nğŸ” {query_name} í…ŒìŠ¤íŠ¸:\")\n",
    "    \n",
    "    # ì›Œë°ì—… ì‹¤í–‰\n",
    "    spark.sql(query).collect()\n",
    "    \n",
    "    # ì‹¤ì œ ì¸¡ì • (3íšŒ í‰ê· )\n",
    "    times = []\n",
    "    for i in range(3):\n",
    "        start_time = time.time()\n",
    "        result = spark.sql(query)\n",
    "        result.collect()\n",
    "        times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_time = statistics.mean(times)\n",
    "    min_time = min(times)\n",
    "    max_time = max(times)\n",
    "    \n",
    "    read_results[query_name] = avg_time\n",
    "    \n",
    "    print(f\"  â±ï¸ í‰ê· : {avg_time:.3f}ì´ˆ\")\n",
    "    print(f\"  â±ï¸ ìµœì†Œ: {min_time:.3f}ì´ˆ\") \n",
    "    print(f\"  â±ï¸ ìµœëŒ€: {max_time:.3f}ì´ˆ\")\n",
    "\n",
    "# 3. ì“°ê¸° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "print(f\"\\nâœï¸ ì“°ê¸° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ë°°ì¹˜ í¬ê¸°ë³„ ì‚½ì… ì„±ëŠ¥\n",
    "batch_sizes = [10, 50, 100, 200]\n",
    "write_results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nğŸ“ ë°°ì¹˜ í¬ê¸° {batch_size}ê°œ ì‚½ì… í…ŒìŠ¤íŠ¸:\")\n",
    "    \n",
    "    test_data = generate_large_dataset(batch_size)\n",
    "    df_test = spark.createDataFrame(pd.DataFrame(test_data))\n",
    "    df_test = df_test.withColumn(\"order_date\", col(\"order_date\").cast(\"date\"))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    df_test.write.format(\"iceberg\").mode(\"append\").saveAsTable(\"demo.blog.orders_benchmark\")\n",
    "    write_time = time.time() - start_time\n",
    "    \n",
    "    records_per_second = batch_size / write_time\n",
    "    write_results[batch_size] = write_time\n",
    "    \n",
    "    print(f\"  â±ï¸ ì‹œê°„: {write_time:.3f}ì´ˆ\")\n",
    "    print(f\"  ğŸš€ ì²˜ë¦¬ìœ¨: {records_per_second:.1f} records/sec\")\n",
    "\n",
    "# 4. ì—…ë°ì´íŠ¸/ì‚­ì œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬\n",
    "print(f\"\\nğŸ”„ ì—…ë°ì´íŠ¸/ì‚­ì œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# ì‚­ì œ ì„±ëŠ¥\n",
    "print(\"ğŸ—‘ï¸ ì‚­ì œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\")\n",
    "delete_queries = [\n",
    "    (\"ì†ŒëŸ‰ ì‚­ì œ\", \"DELETE FROM demo.blog.orders_benchmark WHERE order_id BETWEEN 10000 AND 10010\"),\n",
    "    (\"ì¡°ê±´ë¶€ ì‚­ì œ\", \"DELETE FROM demo.blog.orders_benchmark WHERE amount < 100\"),\n",
    "    (\"íŒŒí‹°ì…˜ ì‚­ì œ\", \"DELETE FROM demo.blog.orders_benchmark WHERE order_date = '2023-01-01'\")\n",
    "]\n",
    "\n",
    "delete_results = {}\n",
    "for delete_name, delete_query in delete_queries:\n",
    "    print(f\"\\n  {delete_name}:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    spark.sql(delete_query)\n",
    "    delete_time = time.time() - start_time\n",
    "    \n",
    "    delete_results[delete_name] = delete_time\n",
    "    print(f\"    â±ï¸ ì‹œê°„: {delete_time:.3f}ì´ˆ\")\n",
    "\n",
    "# ì—…ë°ì´íŠ¸ ì„±ëŠ¥\n",
    "print(f\"\\nğŸ“ ì—…ë°ì´íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:\")\n",
    "update_queries = [\n",
    "    (\"ìƒíƒœ ì—…ë°ì´íŠ¸\", \"UPDATE demo.blog.orders_benchmark SET status = 'updated' WHERE customer_id BETWEEN 1000 AND 1010\"),\n",
    "    (\"ê°€ê²© ì—…ë°ì´íŠ¸\", \"UPDATE demo.blog.orders_benchmark SET amount = amount * 1.1 WHERE product_name = 'iPhone 14'\")\n",
    "]\n",
    "\n",
    "update_results = {}\n",
    "for update_name, update_query in update_queries:\n",
    "    print(f\"\\n  {update_name}:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    spark.sql(update_query)\n",
    "    update_time = time.time() - start_time\n",
    "    \n",
    "    update_results[update_name] = update_time\n",
    "    print(f\"    â±ï¸ ì‹œê°„: {update_time:.3f}ì´ˆ\")\n",
    "\n",
    "# 5. Time Travel ì„±ëŠ¥\n",
    "print(f\"\\nâ° Time Travel ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "snapshots = spark.sql(\"SELECT snapshot_id FROM demo.blog.orders_benchmark.snapshots ORDER BY committed_at\").collect()\n",
    "\n",
    "if len(snapshots) >= 2:\n",
    "    old_snapshot = snapshots[0]['snapshot_id']\n",
    "    \n",
    "    # í˜„ì¬ ë°ì´í„° ì¡°íšŒ\n",
    "    start_time = time.time()\n",
    "    current_result = spark.sql(\"SELECT COUNT(*) FROM demo.blog.orders_benchmark\").collect()\n",
    "    current_time = time.time() - start_time\n",
    "    \n",
    "    # Time Travel ì¡°íšŒ\n",
    "    start_time = time.time()\n",
    "    tt_result = spark.sql(f\"SELECT COUNT(*) FROM demo.blog.orders_benchmark VERSION AS OF {old_snapshot}\").collect()\n",
    "    tt_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"ğŸ“Š í˜„ì¬ ë°ì´í„° ì¡°íšŒ: {current_time:.3f}ì´ˆ\")\n",
    "    print(f\"â° Time Travel ì¡°íšŒ: {tt_time:.3f}ì´ˆ\")\n",
    "    print(f\"ğŸ“ˆ Time Travel ì˜¤ë²„í—¤ë“œ: {((tt_time - current_time) / current_time * 100):+.1f}%\")\n",
    "\n",
    "# 6. íŒŒì¼ I/O ë¶„ì„\n",
    "print(f\"\\nğŸ’¾ íŒŒì¼ I/O ë¶„ì„:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "benchmark_table_path = \"/home/jovyan/data/warehouse/blog/orders_benchmark\"\n",
    "benchmark_files = glob.glob(f\"{benchmark_table_path}/**/*.parquet\", recursive=True)\n",
    "\n",
    "total_files = len(benchmark_files)\n",
    "total_size = sum(os.path.getsize(f) for f in benchmark_files)\n",
    "avg_file_size = total_size / total_files if total_files > 0 else 0\n",
    "\n",
    "print(f\"ğŸ“„ ì´ íŒŒì¼ ìˆ˜: {total_files}\")\n",
    "print(f\"ğŸ’¾ ì´ í¬ê¸°: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)\")\n",
    "print(f\"ğŸ“ í‰ê·  íŒŒì¼ í¬ê¸°: {avg_file_size:,.0f} bytes ({avg_file_size/1024:.1f} KB)\")\n",
    "\n",
    "# 7. ì„±ëŠ¥ ìš”ì•½ ë¦¬í¬íŠ¸\n",
    "print(f\"\\nğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ë¦¬í¬íŠ¸:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"ğŸ† ìµœê³  ì„±ëŠ¥ í•­ëª©:\")\n",
    "best_read = min(read_results.items(), key=lambda x: x[1])\n",
    "best_write = min(write_results.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"  ğŸ“– ì½ê¸°: {best_read[0]} ({best_read[1]:.3f}ì´ˆ)\")\n",
    "print(f\"  âœï¸ ì“°ê¸°: ë°°ì¹˜ í¬ê¸° {best_write[0]} ({best_write[1]:.3f}ì´ˆ)\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ì„±ëŠ¥ íŠ¹ì„±:\")\n",
    "print(f\"  ğŸ“Š í‰ê·  ì½ê¸° ì„±ëŠ¥: {statistics.mean(read_results.values()):.3f}ì´ˆ\")\n",
    "print(f\"  ğŸ“Š í‰ê·  ì“°ê¸° ì„±ëŠ¥: {statistics.mean(write_results.values()):.3f}ì´ˆ\")\n",
    "\n",
    "# íŒŒì¼ë‹¹ ì²˜ë¦¬ ì„±ëŠ¥\n",
    "records_per_file = 1000 / total_files if total_files > 0 else 0\n",
    "print(f\"  ğŸ“„ íŒŒì¼ë‹¹ í‰ê·  ë ˆì½”ë“œ: {records_per_file:.0f}ê°œ\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­:\")\n",
    "print(\"  ğŸ”¹ íŒŒì¼ í¬ê¸°: 128MB~1GB ìœ ì§€\")\n",
    "print(\"  ğŸ”¹ íŒŒí‹°ì…”ë‹: ì¿¼ë¦¬ íŒ¨í„´ì— ë§ê²Œ ì„¤ê³„\")\n",
    "print(\"  ğŸ”¹ ì •ê¸°ì  Compaction ìˆ˜í–‰\")\n",
    "print(\"  ğŸ”¹ ì ì ˆí•œ ë°°ì¹˜ í¬ê¸°ë¡œ ì“°ê¸°\")\n",
    "print(\"  ğŸ”¹ í•„ìš”ì‹œ Z-ordering ê³ ë ¤\")\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e314b-2cb6-4811-9691-28d6c0bc3b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
