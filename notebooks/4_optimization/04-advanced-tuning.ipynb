{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1000001",
   "metadata": {},
   "source": [
    "# Advanced Tuning — Metrics, Write Distribution, Bloom Filters\n",
    "\n",
    "이 노트북에서는 Iceberg의 **세밀한 성능 튜닝** 옵션을 실습합니다.\n",
    "\n",
    "기본적인 Compaction/Partitioning/Maintenance를 적용했는데도 병목이 남을 때,\n",
    "메트릭 수집 레벨, 쓰기 분배 모드, Bloom Filter, Puffin 기반 고급 통계를 조정합니다.\n",
    "\n",
    "### 진입 기준 (Decision Tree)\n",
    "\n",
    "```\n",
    "느린 쿼리 발생\n",
    "  ├─ 파일 수 과다?          → 01-compaction 먼저\n",
    "  ├─ 스캔 범위 과다?         → 02-partitioning 먼저\n",
    "  ├─ 스냅샷/manifest 누적?   → 03-table-maintenance 먼저\n",
    "  └─ 위 3개 적용 후에도 병목 → 이 노트북(advanced tuning) 진입\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000002",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "import json\n",
    "import glob as glob_mod\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "METRICS_TABLE = \"demo.lab.tuning_metrics\"\n",
    "METRICS_PATH = \"/home/jovyan/data/warehouse/lab/tuning_metrics\"\n",
    "\n",
    "DIST_TABLE = \"demo.lab.tuning_distribution\"\n",
    "DIST_PATH = \"/home/jovyan/data/warehouse/lab/tuning_distribution\"\n",
    "\n",
    "BLOOM_TABLE = \"demo.lab.tuning_bloom\"\n",
    "BLOOM_PATH = \"/home/jovyan/data/warehouse/lab/tuning_bloom\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000005",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 1: Metrics Collection — 컬럼별 통계 수집 레벨\n",
    "\n",
    "Iceberg는 각 데이터 파일에 대해 **컬럼별 통계**를 manifest 파일에 기록합니다. 이 통계는 쿼리 시 파일을 건너뛸 수 있게 해주는 핵심 정보입니다.\n",
    "\n",
    "### 4가지 메트릭 모드\n",
    "\n",
    "| 모드 | 수집 정보 | 용도 |\n",
    "|------|----------|------|\n",
    "| `none` | 통계 없음 | 넓은 테이블에서 메타데이터 크기 절약 |\n",
    "| `counts` | null 수, 값 수 | 기본 정보만 필요할 때 |\n",
    "| `truncate(N)` | counts + min/max (N바이트로 잘림) | **기본값** (N=16), 대부분의 경우 충분 |\n",
    "| `full` | counts + 전체 min/max | 가장 정밀, 메타데이터 크기 증가 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테이블 생성 + 500건 삽입 완료: demo.lab.tuning_metrics\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {METRICS_TABLE}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {METRICS_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "# 데이터 삽입\n",
    "orders = generate_orders(num_records=500, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(METRICS_TABLE).append()\n",
    "\n",
    "print(f\"테이블 생성 + 500건 삽입 완료: {METRICS_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 기본 메트릭 (truncate(16)) — 사람이 읽기 쉬운 뷰 ===\n",
      "+--------------------------------------------------------------------------------------------------------------------------+------------+----------------+----------------+----------+----------+------------------------+-----------------------+------------------+-----------------+\n",
      "|file_path                                                                                                                 |record_count|product_name_min|product_name_max|status_min|status_max|product_name_value_count|product_name_null_count|status_value_count|status_null_count|\n",
      "+--------------------------------------------------------------------------------------------------------------------------+------------+----------------+----------------+----------+----------+------------------------+-----------------------+------------------+-----------------+\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-01/00000-5-425e04e1-e36b-4e69-b2e9-f34bd02b0d4b-00001.parquet|159         |AirPods         |iPhone SE       |cancelled |shipped   |159                     |0                      |159               |0                |\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-02/00000-5-425e04e1-e36b-4e69-b2e9-f34bd02b0d4b-00002.parquet|161         |AirPods         |iPhone SE       |cancelled |shipped   |161                     |0                      |161               |0                |\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-03/00000-5-425e04e1-e36b-4e69-b2e9-f34bd02b0d4b-00003.parquet|180         |AirPods         |iPhone SE       |cancelled |shipped   |180                     |0                      |180               |0                |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+------------+----------------+----------------+----------+----------+------------------------+-----------------------+------------------+-----------------+\n",
      "\n",
      "\n",
      "참고: field-id 매핑 -> 1:order_id, 2:customer_id, 3:product_name, 4:order_date, 5:amount, 6:status\n"
     ]
    }
   ],
   "source": [
    "# 기본 메트릭 (truncate(16)) 확인\n",
    "# 참고: lower_bounds/upper_bounds는 map<int,binary>라 그대로 출력하면 깨진 문자열처럼 보일 수 있음\n",
    "print(\"=== 기본 메트릭 (truncate(16)) — 사람이 읽기 쉬운 뷰 ===\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    regexp_replace(file_path, '^file:.*?/(data/)', '$1') AS file_path,\n",
    "    record_count,\n",
    "    CAST(lower_bounds[3] AS STRING) AS product_name_min,\n",
    "    CAST(upper_bounds[3] AS STRING) AS product_name_max,\n",
    "    CAST(lower_bounds[6] AS STRING) AS status_min,\n",
    "    CAST(upper_bounds[6] AS STRING) AS status_max,\n",
    "    value_counts[3] AS product_name_value_count,\n",
    "    null_value_counts[3] AS product_name_null_count,\n",
    "    value_counts[6] AS status_value_count,\n",
    "    null_value_counts[6] AS status_null_count\n",
    "FROM {METRICS_TABLE}.files\n",
    "WHERE content = 0\n",
    "ORDER BY file_path\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n참고: field-id 매핑 -> 1:order_id, 2:customer_id, 3:product_name, 4:order_date, 5:amount, 6:status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status 컬럼 메트릭을 'none'으로 변경\n",
      "(새로 쓰는 파일부터 적용됨)\n"
     ]
    }
   ],
   "source": [
    "# 특정 컬럼의 메트릭 모드 변경: status는 필터링에 안 쓰므로 none으로\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {METRICS_TABLE}\n",
    "SET TBLPROPERTIES (\n",
    "    'write.metadata.metrics.column.status' = 'none'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"status 컬럼 메트릭을 'none'으로 변경\")\n",
    "print(\"(새로 쓰는 파일부터 적용됨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1000009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 메트릭 변경 후 새 파일 정보 (readable) ===\n",
      "+---------------------------------------------------------------------------------------------------------------------------+------------+----------------+----------------+----------+----------+------------------------+-----------------------+------------------+-----------------+\n",
      "|file_path                                                                                                                  |record_count|product_name_min|product_name_max|status_min|status_max|product_name_value_count|product_name_null_count|status_value_count|status_null_count|\n",
      "+---------------------------------------------------------------------------------------------------------------------------+------------+----------------+----------------+----------+----------+------------------------+-----------------------+------------------+-----------------+\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-03/00000-5-425e04e1-e36b-4e69-b2e9-f34bd02b0d4b-00003.parquet |180         |AirPods         |iPhone SE       |cancelled |shipped   |180                     |0                      |180               |0                |\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-03/00000-12-f8c12b68-d8a7-42e2-9f22-8b6563789335-00003.parquet|36          |AirPods         |iPhone SE       |null      |null      |36                      |0                      |null              |null             |\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-02/00000-5-425e04e1-e36b-4e69-b2e9-f34bd02b0d4b-00002.parquet |161         |AirPods         |iPhone SE       |cancelled |shipped   |161                     |0                      |161               |0                |\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-02/00000-12-f8c12b68-d8a7-42e2-9f22-8b6563789335-00002.parquet|26          |AirPods Pro     |iPhone SE       |null      |null      |26                      |0                      |null              |null             |\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-01/00000-5-425e04e1-e36b-4e69-b2e9-f34bd02b0d4b-00001.parquet |159         |AirPods         |iPhone SE       |cancelled |shipped   |159                     |0                      |159               |0                |\n",
      "|data/warehouse/lab/tuning_metrics/data/order_date_month=2024-01/00000-12-f8c12b68-d8a7-42e2-9f22-8b6563789335-00001.parquet|38          |AirPods         |iPhone SE       |null      |null      |38                      |0                      |null              |null             |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+------------+----------------+----------------+----------+----------+------------------------+-----------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 메트릭 변경 후 새 데이터 삽입\n",
    "orders_new = generate_orders(num_records=100, seed=99, id_offset=501)\n",
    "df_new = to_spark_df(spark, orders_new)\n",
    "df_new.writeTo(METRICS_TABLE).append()\n",
    "\n",
    "# 새 파일의 메트릭 확인 (readable view)\n",
    "print(\"=== 메트릭 변경 후 새 파일 정보 (readable) ===\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    regexp_replace(file_path, '^file:.*?/(data/)', '$1') AS file_path,\n",
    "    record_count,\n",
    "    CAST(lower_bounds[3] AS STRING) AS product_name_min,\n",
    "    CAST(upper_bounds[3] AS STRING) AS product_name_max,\n",
    "    CAST(lower_bounds[6] AS STRING) AS status_min,\n",
    "    CAST(upper_bounds[6] AS STRING) AS status_max,\n",
    "    value_counts[3] AS product_name_value_count,\n",
    "    null_value_counts[3] AS product_name_null_count,\n",
    "    value_counts[6] AS status_value_count,\n",
    "    null_value_counts[6] AS status_null_count\n",
    "FROM {METRICS_TABLE}.files\n",
    "WHERE content = 0\n",
    "ORDER BY file_path DESC\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000010",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Metrics Collection\n",
    "\n",
    "- 기본적으로 모든 컬럼에 `truncate(16)` 통계가 수집됩니다\n",
    "- `files.lower_bounds/upper_bounds`는 `map<int,binary>`이므로 raw 출력 시 깨진 문자열처럼 보일 수 있습니다\n",
    "- 사람이 읽을 때는 `CAST(lower_bounds[field-id] AS STRING)`처럼 컬럼별로 조회하세요 (예: `product_name`=3, `status`=6)\n",
    "- **넓은 테이블**(컬럼 100개 이상)에서는 모든 컬럼의 통계를 수집하면 메타데이터가 매우 커집니다\n",
    "- 필터링에 사용하지 않는 컬럼은 `none`으로 설정하여 메타데이터 크기를 줄일 수 있습니다\n",
    "- 자주 필터링하는 핵심 컬럼은 `full`로 설정하여 프루닝 정밀도를 높일 수 있습니다\n",
    "\n",
    "```sql\n",
    "-- 넓은 테이블 전략 예시\n",
    "ALTER TABLE my_table SET TBLPROPERTIES (\n",
    "    'write.metadata.metrics.default' = 'none',              -- 기본: 통계 없음\n",
    "    'write.metadata.metrics.column.user_id' = 'full',       -- 핵심 필터 컬럼만 full\n",
    "    'write.metadata.metrics.column.created_at' = 'truncate(16)'\n",
    ");\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000011",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 2: Write Distribution Mode — 쓰기 분배 전략\n",
    "\n",
    "Write Distribution Mode는 Spark가 데이터를 파일로 쓸 때 **레코드를 어떻게 분배할지** 결정합니다.\n",
    "\n",
    "### 3가지 분배 모드\n",
    "\n",
    "| 모드 | 동작 | 장점 | 단점 |\n",
    "|------|------|------|------|\n",
    "| `none` | 분배 없음, 각 태스크가 자기 데이터를 바로 작성 | 쓰기 속도 최고 | 파티션당 파일 폭증 가능 |\n",
    "| `hash` | 파티션 키 기준 해시 셔플 | 파티션당 파일 수 제어 | 셔플 비용 발생 |\n",
    "| `range` | 파티션 키 + sort 키 기준 범위 셔플 | 파일 내 정렬 보장 | 셔플 비용 가장 높음 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1000012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none / hash 모드 테이블 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# none 모드 테이블\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DIST_TABLE}_none\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {DIST_TABLE}_none (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES ('write.distribution-mode' = 'none')\n",
    "\"\"\")\n",
    "\n",
    "# hash 모드 테이블\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DIST_TABLE}_hash\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {DIST_TABLE}_hash (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES ('write.distribution-mode' = 'hash')\n",
    "\"\"\")\n",
    "\n",
    "print(\"none / hash 모드 테이블 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1000013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모드            쓰기 시간     파일 수\n",
      "==============================\n",
      "none         0.498s      15개\n",
      "hash         0.377s       3개\n"
     ]
    }
   ],
   "source": [
    "# 동일 데이터를 양쪽에 삽입하고 파일 구조 비교\n",
    "orders = generate_orders(num_records=500, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "\n",
    "# none 모드\n",
    "start = time.time()\n",
    "df.writeTo(f\"{DIST_TABLE}_none\").append()\n",
    "none_time = time.time() - start\n",
    "none_files = count_files(f\"{DIST_PATH}_none\")\n",
    "\n",
    "# hash 모드\n",
    "start = time.time()\n",
    "df.writeTo(f\"{DIST_TABLE}_hash\").append()\n",
    "hash_time = time.time() - start\n",
    "hash_files = count_files(f\"{DIST_PATH}_hash\")\n",
    "\n",
    "print(f\"{'모드':<8} {'쓰기 시간':>10} {'파일 수':>8}\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"{'none':<8} {none_time:>9.3f}s {none_files:>7}개\")\n",
    "print(f\"{'hash':<8} {hash_time:>9.3f}s {hash_files:>7}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== none 모드 파일 구조 ===\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   ├── 00000-14-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00001.parquet  (2.5 KB)\n",
      "│   │   ├── 00001-15-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00001.parquet  (2.4 KB)\n",
      "│   │   ├── 00002-16-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00001.parquet  (2.5 KB)\n",
      "│   │   ├── 00003-17-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00001.parquet  (2.3 KB)\n",
      "│   │   └── 00004-18-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00001.parquet  (2.4 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   ├── 00000-14-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00002.parquet  (2.5 KB)\n",
      "│   │   ├── 00001-15-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00002.parquet  (2.4 KB)\n",
      "│   │   ├── 00002-16-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00002.parquet  (2.5 KB)\n",
      "│   │   ├── 00003-17-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00002.parquet  (2.5 KB)\n",
      "│   │   └── 00004-18-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00002.parquet  (2.4 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       ├── 00000-14-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00003.parquet  (2.4 KB)\n",
      "│       ├── 00001-15-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00003.parquet  (2.5 KB)\n",
      "│       ├── 00002-16-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00003.parquet  (2.5 KB)\n",
      "│       ├── 00003-17-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00003.parquet  (2.5 KB)\n",
      "│       └── 00004-18-c71b1ec1-4b58-46a9-8946-a73a5b0f6b75-00003.parquet  (2.6 KB)\n",
      "└── metadata/\n",
      "    ├── 86f222c5-dafd-4df3-b40c-9ce36c754526-m0.avro  (8.0 KB)\n",
      "    ├── snap-1559464342689933748-1-86f222c5-dafd-4df3-b40c-9ce36c754526.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.6 KB)\n",
      "    ├── v2.metadata.json  (2.6 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "=== hash 모드 파일 구조 ===\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   │   └── 00000-24-36a10d37-8adb-4eda-a3d4-312dc18177fc-00001.parquet  (3.5 KB)\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   │   └── 00000-24-36a10d37-8adb-4eda-a3d4-312dc18177fc-00002.parquet  (3.5 KB)\n",
      "│   └── order_date_month=2024-03/\n",
      "│       └── 00000-24-36a10d37-8adb-4eda-a3d4-312dc18177fc-00003.parquet  (3.7 KB)\n",
      "└── metadata/\n",
      "    ├── 3af2aaa3-65b7-4eb3-b2af-d25712f2b91c-m0.avro  (7.3 KB)\n",
      "    ├── snap-6740342067303639565-1-3af2aaa3-65b7-4eb3-b2af-d25712f2b91c.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.6 KB)\n",
      "    ├── v2.metadata.json  (2.6 KB)\n",
      "    └── version-hint.text  (1 B)\n"
     ]
    }
   ],
   "source": [
    "# 파일 구조 시각적 비교\n",
    "print(\"=== none 모드 파일 구조 ===\")\n",
    "show_tree(f\"{DIST_PATH}_none\", max_depth=3)\n",
    "\n",
    "print(\"\\n=== hash 모드 파일 구조 ===\")\n",
    "show_tree(f\"{DIST_PATH}_hash\", max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000015",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Write Distribution Mode\n",
    "\n",
    "- **none**: 셔플 없이 바로 작성하므로 빠르지만, Spark 태스크 수만큼 파일이 생길 수 있습니다\n",
    "- **hash**: 파티션 키 기준으로 셔플하여 같은 파티션의 데이터를 모은 후 작성하므로 파일 수가 줄어듭니다\n",
    "- **range**: hash에 더해 정렬까지 보장하지만 셔플 비용이 가장 높습니다\n",
    "\n",
    "### 선택 가이드\n",
    "\n",
    "| 상황 | 권장 모드 |\n",
    "|------|----------|\n",
    "| 스트리밍 수집 (SLA 빡빡) | `none` + 주기적 Compaction |\n",
    "| 배치 적재 (파일 수 관리) | `hash` |\n",
    "| 배치 적재 + 정렬 필요 | `range` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000016",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 3: Bloom Filters — Point Lookup 성능 개선\n",
    "\n",
    "### Bloom Filter 개념\n",
    "\n",
    "Bloom Filter는 **\"이 파일에 찾는 값이 있을 수 있는가?\"**를 빠르게 판단하는 확률적 자료구조입니다.\n",
    "\n",
    "```\n",
    "Bloom Filter 응답:\n",
    "  \"없다\" → 확실히 없음 (True Negative) → 파일 스킵!\n",
    "  \"있다\" → 있을 수도 있음 (True/False Positive) → 파일 읽기\n",
    "```\n",
    "\n",
    "### 동작 원리\n",
    "\n",
    "1. 비트 배열(예: 1024비트)을 0으로 초기화\n",
    "2. 각 값을 K개의 해시 함수로 해시하여 비트 위치를 계산\n",
    "3. 해당 위치의 비트를 1로 설정\n",
    "4. 검색 시: 값을 해시하여 모든 비트가 1인지 확인\n",
    "\n",
    "### 적합한 경우\n",
    "\n",
    "- `WHERE user_id = 'abc123'` 같은 **Point Lookup** 쿼리가 빈번할 때\n",
    "- **고카디널리티 컬럼** (값의 종류가 매우 많은 컬럼)에 효과적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1000017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테이블 생성 + 1000건 삽입 완료\n",
      "파일 수: 15\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {BLOOM_TABLE}\")\n",
    "\n",
    "# Bloom Filter 없는 기본 테이블\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {BLOOM_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "# 대량 데이터 삽입 (여러 배치로 파일 분산)\n",
    "for i in range(5):\n",
    "    orders = generate_orders(num_records=200, seed=i, id_offset=i*200+1)\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(BLOOM_TABLE).append()\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {BLOOM_TABLE}\").collect()[0][0]\n",
    "print(f\"테이블 생성 + {total}건 삽입 완료\")\n",
    "print(f\"파일 수: {count_files(BLOOM_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1000018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom Filter 없이 Point Lookup 평균: 0.138초\n"
     ]
    }
   ],
   "source": [
    "# Bloom Filter 없이 Point Lookup 성능 측정\n",
    "times_without_bloom = []\n",
    "for _ in range(3):\n",
    "    start = time.time()\n",
    "    spark.sql(f\"SELECT * FROM {BLOOM_TABLE} WHERE order_id = 42\").collect()\n",
    "    times_without_bloom.append(time.time() - start)\n",
    "\n",
    "avg_without = sum(times_without_bloom) / len(times_without_bloom)\n",
    "print(f\"Bloom Filter 없이 Point Lookup 평균: {avg_without:.3f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1000019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id 컬럼에 Bloom Filter 활성화\n",
      "(기존 파일에는 적용되지 않음 — Compaction이나 새 쓰기 시 적용)\n"
     ]
    }
   ],
   "source": [
    "# Bloom Filter 활성화\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {BLOOM_TABLE}\n",
    "SET TBLPROPERTIES (\n",
    "    'write.parquet.bloom-filter-enabled.column.order_id' = 'true',\n",
    "    'write.parquet.bloom-filter-max-bytes' = '1048576'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"order_id 컬럼에 Bloom Filter 활성화\")\n",
    "print(\"(기존 파일에는 적용되지 않음 — Compaction이나 새 쓰기 시 적용)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1000020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|15                        |3                     |41841                |0                      |\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n",
      "Compaction 후 파일 수: 18\n"
     ]
    }
   ],
   "source": [
    "# Compaction으로 Bloom Filter가 포함된 새 파일 생성\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_data_files(\n",
    "    table => '{BLOOM_TABLE}',\n",
    "    strategy => 'binpack'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(f\"Compaction 후 파일 수: {count_files(BLOOM_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1000021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom Filter 없이: 0.138초\n",
      "Bloom Filter 있음: 0.072초\n",
      "개선율: 47.9%\n"
     ]
    }
   ],
   "source": [
    "# Bloom Filter 적용 후 Point Lookup 성능 측정\n",
    "times_with_bloom = []\n",
    "for _ in range(3):\n",
    "    start = time.time()\n",
    "    spark.sql(f\"SELECT * FROM {BLOOM_TABLE} WHERE order_id = 42\").collect()\n",
    "    times_with_bloom.append(time.time() - start)\n",
    "\n",
    "avg_with = sum(times_with_bloom) / len(times_with_bloom)\n",
    "print(f\"Bloom Filter 없이: {avg_without:.3f}초\")\n",
    "print(f\"Bloom Filter 있음: {avg_with:.3f}초\")\n",
    "\n",
    "if avg_without > 0:\n",
    "    improvement = (avg_without - avg_with) / avg_without * 100\n",
    "    print(f\"개선율: {improvement:.1f}%\")\n",
    "    if improvement < 0:\n",
    "        print(\"(작은 데이터셋에서는 Bloom Filter 오버헤드로 오히려 느릴 수 있음)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000022",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Bloom Filters\n",
    "\n",
    "- Bloom Filter는 **Point Lookup** (`WHERE col = value`) 쿼리에서 불필요한 파일을 빠르게 건너뛸 수 있게 합니다\n",
    "- 소규모 데이터셋에서는 효과가 미미하거나 오히려 오버헤드가 될 수 있습니다\n",
    "- **대규모 데이터 + 고카디널리티 컬럼 + Point Lookup**이 빈번한 환경에서 가장 효과적입니다\n",
    "- Bloom Filter는 **Parquet 파일 레벨**에 저장되므로, 활성화 후 **Compaction이나 새 쓰기가 필요**합니다\n",
    "\n",
    "> 주의: Bloom Filter는 파일 크기를 약간 증가시킵니다. `bloom-filter-max-bytes`로 크기를 제한하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55194e8",
   "metadata": {},
   "source": [
    "---\n",
    "## 보강: Puffin Files와 Theta Sketch (개념)\n",
    "\n",
    "Puffin 파일은 metadata/manifest 통계를 보완하는 고급 통계 저장 포맷입니다.\n",
    "\n",
    "- Puffin은 blob + 메타데이터 구조를 사용합니다.\n",
    "- 대표 예: **Apache DataSketches Theta sketch** (근사 distinct 계산)\n",
    "- 대규모 cardinality 추정/탐색 최적화에서 메타데이터 비용을 줄이는 데 유리합니다.\n",
    "\n",
    "> 이 실습 환경에서는 Puffin을 직접 생성/조회하지 않을 수 있습니다.\n",
    "> 핵심은 \"고급 통계를 manifest 밖에 분리 저장해 쿼리 최적화에 활용\"한다는 개념 이해입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000023",
   "metadata": {},
   "source": [
    "---\n",
    "## Object Storage 최적화\n",
    "\n",
    "S3 같은 오브젝트 스토리지에서는 **prefix throttling** 문제가 발생할 수 있습니다.\n",
    "\n",
    "### 문제\n",
    "\n",
    "S3는 prefix(경로 접두사)별로 요청 속도를 제한합니다. 같은 파티션 디렉토리에 대량의 파일이 쓰이면 스로틀링이 발생할 수 있습니다.\n",
    "\n",
    "### 해결: Object Storage Path\n",
    "\n",
    "```sql\n",
    "ALTER TABLE my_table SET TBLPROPERTIES (\n",
    "    'write.object-storage.enabled' = 'true',\n",
    "    'write.data.path' = 's3://bucket/data'\n",
    ");\n",
    "```\n",
    "\n",
    "활성화하면 Iceberg가 **파일 경로에 해시를 추가**하여 다양한 prefix에 분산 저장합니다:\n",
    "\n",
    "```\n",
    "# Before (기본)\n",
    "s3://bucket/warehouse/my_table/data/order_date_month=2024-01/file1.parquet\n",
    "s3://bucket/warehouse/my_table/data/order_date_month=2024-01/file2.parquet\n",
    "\n",
    "# After (Object Storage 활성화)\n",
    "s3://bucket/data/a1b2c3d4/order_date_month=2024-01/file1.parquet\n",
    "s3://bucket/data/e5f6a7b8/order_date_month=2024-01/file2.parquet\n",
    "```\n",
    "\n",
    "파일 위치가 분산되지만, Iceberg 메타데이터가 파일 경로를 추적하므로 쿼리에는 영향이 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000024",
   "metadata": {},
   "source": [
    "---\n",
    "## 튜닝 옵션 요약\n",
    "\n",
    "| 옵션 | 기본값 | 튜닝 포인트 | 효과 |\n",
    "|------|--------|------------|------|\n",
    "| **Metrics** | `truncate(16)` | 넓은 테이블 → 핵심만 `full`, 나머지 `none` | 메타데이터 크기 절약 |\n",
    "| **Distribution** | `none` | 배치 → `hash`, SLA 빡빡 → `none` | 파일 수 제어 |\n",
    "| **Bloom Filter** | 비활성 | 고카디널리티 + Point Lookup → 활성화 | 파일 스킵 |\n",
    "| **Puffin/Theta** | 엔진/설정 의존 | 근사 통계가 필요한 대규모 분석 | 고급 통계 활용 |\n",
    "| **Object Storage** | 비활성 | S3 대량 쓰기 → 활성화 | prefix 스로틀링 방지 |\n",
    "\n",
    "### 언제 튜닝이 필요한가?\n",
    "\n",
    "0. **먼저 01~03 최적화(Compaction/Partitioning/Maintenance)를 완료했는지 확인**\n",
    "1. **넓은 테이블**(100+ 컬럼)에서 메타데이터 크기가 문제일 때 → Metrics 조정\n",
    "2. **스트리밍 수집 후 파일이 너무 많을 때** → Distribution Mode 조정\n",
    "3. **특정 값 검색이 느릴 때** → Bloom Filter 활성화\n",
    "4. **근사 distinct/고급 통계가 필요할 때** → Puffin/Theta 활용 검토\n",
    "5. **S3 쓰기 성능이 떨어질 때** → Object Storage 경로 활성화\n",
    "\n",
    "> 대부분의 경우 Compaction + Partitioning이면 충분합니다. 이 튜닝들은 **문제가 확인된 후** 적용하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1000025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
