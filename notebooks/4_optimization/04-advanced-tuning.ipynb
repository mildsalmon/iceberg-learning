{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1000001",
   "metadata": {},
   "source": [
    "# Advanced Tuning — Metrics, Write Distribution, Bloom Filters\n",
    "\n",
    "이 노트북에서는 Iceberg의 **세밀한 성능 튜닝** 옵션을 실습합니다.\n",
    "\n",
    "기본적인 Compaction과 Partitioning으로 충분하지 않을 때, 메트릭 수집 레벨, 쓰기 분배 모드, Bloom Filter 등을 조정하여 추가 성능 개선을 달성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000002",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "import json\n",
    "import glob as glob_mod\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "METRICS_TABLE = \"demo.lab.tuning_metrics\"\n",
    "METRICS_PATH = \"/home/jovyan/data/warehouse/lab/tuning_metrics\"\n",
    "\n",
    "DIST_TABLE = \"demo.lab.tuning_distribution\"\n",
    "DIST_PATH = \"/home/jovyan/data/warehouse/lab/tuning_distribution\"\n",
    "\n",
    "BLOOM_TABLE = \"demo.lab.tuning_bloom\"\n",
    "BLOOM_PATH = \"/home/jovyan/data/warehouse/lab/tuning_bloom\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000005",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 1: Metrics Collection — 컬럼별 통계 수집 레벨\n",
    "\n",
    "Iceberg는 각 데이터 파일에 대해 **컬럼별 통계**를 manifest 파일에 기록합니다. 이 통계는 쿼리 시 파일을 건너뛸 수 있게 해주는 핵심 정보입니다.\n",
    "\n",
    "### 4가지 메트릭 모드\n",
    "\n",
    "| 모드 | 수집 정보 | 용도 |\n",
    "|------|----------|------|\n",
    "| `none` | 통계 없음 | 넓은 테이블에서 메타데이터 크기 절약 |\n",
    "| `counts` | null 수, 값 수 | 기본 정보만 필요할 때 |\n",
    "| `truncate(N)` | counts + min/max (N바이트로 잘림) | **기본값** (N=16), 대부분의 경우 충분 |\n",
    "| `full` | counts + 전체 min/max | 가장 정밀, 메타데이터 크기 증가 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {METRICS_TABLE}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {METRICS_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "# 데이터 삽입\n",
    "orders = generate_orders(num_records=500, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(METRICS_TABLE).append()\n",
    "\n",
    "print(f\"테이블 생성 + 500건 삽입 완료: {METRICS_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000007",
   "metadata": {},
   "outputs": [],
   "source": "# 기본 메트릭 (truncate(16)) 확인\nprint(\"=== 기본 메트릭 (truncate(16)) — Manifest 파일 정보 ===\")\nspark.sql(f\"\"\"\nSELECT \n    regexp_replace(file_path, '^file:.*?/(data/)', '$1') as file_path,\n    record_count,\n    lower_bounds,\n    upper_bounds,\n    null_value_counts,\n    value_counts\nFROM {METRICS_TABLE}.files\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 컬럼의 메트릭 모드 변경: status는 필터링에 안 쓰므로 none으로\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {METRICS_TABLE}\n",
    "SET TBLPROPERTIES (\n",
    "    'write.metadata.metrics.column.status' = 'none'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"status 컬럼 메트릭을 'none'으로 변경\")\n",
    "print(\"(새로 쓰는 파일부터 적용됨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000009",
   "metadata": {},
   "outputs": [],
   "source": "# 메트릭 변경 후 새 데이터 삽입\norders_new = generate_orders(num_records=100, seed=99, id_offset=501)\ndf_new = to_spark_df(spark, orders_new)\ndf_new.writeTo(METRICS_TABLE).append()\n\n# 새 파일의 메트릭 확인\nprint(\"=== 메트릭 변경 후 새 파일 정보 ===\")\nspark.sql(f\"\"\"\nSELECT \n    regexp_replace(file_path, '^file:.*?/(data/)', '$1') as file_path,\n    record_count,\n    lower_bounds,\n    upper_bounds\nFROM {METRICS_TABLE}.files\nORDER BY file_path DESC\n\"\"\").show(truncate=False)"
  },
  {
   "cell_type": "markdown",
   "id": "c1000010",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Metrics Collection\n",
    "\n",
    "- 기본적으로 모든 컬럼에 `truncate(16)` 통계가 수집됩니다\n",
    "- **넓은 테이블**(컬럼 100개 이상)에서는 모든 컬럼의 통계를 수집하면 메타데이터가 매우 커집니다\n",
    "- 필터링에 사용하지 않는 컬럼은 `none`으로 설정하여 메타데이터 크기를 줄일 수 있습니다\n",
    "- 자주 필터링하는 핵심 컬럼은 `full`로 설정하여 프루닝 정밀도를 높일 수 있습니다\n",
    "\n",
    "```sql\n",
    "-- 넓은 테이블 전략 예시\n",
    "ALTER TABLE my_table SET TBLPROPERTIES (\n",
    "    'write.metadata.metrics.default' = 'none',              -- 기본: 통계 없음\n",
    "    'write.metadata.metrics.column.user_id' = 'full',       -- 핵심 필터 컬럼만 full\n",
    "    'write.metadata.metrics.column.created_at' = 'truncate(16)'\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000011",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 2: Write Distribution Mode — 쓰기 분배 전략\n",
    "\n",
    "Write Distribution Mode는 Spark가 데이터를 파일로 쓸 때 **레코드를 어떻게 분배할지** 결정합니다.\n",
    "\n",
    "### 3가지 분배 모드\n",
    "\n",
    "| 모드 | 동작 | 장점 | 단점 |\n",
    "|------|------|------|------|\n",
    "| `none` | 분배 없음, 각 태스크가 자기 데이터를 바로 작성 | 쓰기 속도 최고 | 파티션당 파일 폭증 가능 |\n",
    "| `hash` | 파티션 키 기준 해시 셔플 | 파티션당 파일 수 제어 | 셔플 비용 발생 |\n",
    "| `range` | 파티션 키 + sort 키 기준 범위 셔플 | 파일 내 정렬 보장 | 셔플 비용 가장 높음 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# none 모드 테이블\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DIST_TABLE}_none\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {DIST_TABLE}_none (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES ('write.distribution-mode' = 'none')\n",
    "\"\"\")\n",
    "\n",
    "# hash 모드 테이블\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DIST_TABLE}_hash\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {DIST_TABLE}_hash (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES ('write.distribution-mode' = 'hash')\n",
    "\"\"\")\n",
    "\n",
    "print(\"none / hash 모드 테이블 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일 데이터를 양쪽에 삽입하고 파일 구조 비교\n",
    "orders = generate_orders(num_records=500, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "\n",
    "# none 모드\n",
    "start = time.time()\n",
    "df.writeTo(f\"{DIST_TABLE}_none\").append()\n",
    "none_time = time.time() - start\n",
    "none_files = count_files(f\"{DIST_PATH}_none\")\n",
    "\n",
    "# hash 모드\n",
    "start = time.time()\n",
    "df.writeTo(f\"{DIST_TABLE}_hash\").append()\n",
    "hash_time = time.time() - start\n",
    "hash_files = count_files(f\"{DIST_PATH}_hash\")\n",
    "\n",
    "print(f\"{'모드':<8} {'쓰기 시간':>10} {'파일 수':>8}\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"{'none':<8} {none_time:>9.3f}s {none_files:>7}개\")\n",
    "print(f\"{'hash':<8} {hash_time:>9.3f}s {hash_files:>7}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 구조 시각적 비교\n",
    "print(\"=== none 모드 파일 구조 ===\")\n",
    "show_tree(f\"{DIST_PATH}_none\", max_depth=2)\n",
    "\n",
    "print(\"\\n=== hash 모드 파일 구조 ===\")\n",
    "show_tree(f\"{DIST_PATH}_hash\", max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000015",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Write Distribution Mode\n",
    "\n",
    "- **none**: 셔플 없이 바로 작성하므로 빠르지만, Spark 태스크 수만큼 파일이 생길 수 있습니다\n",
    "- **hash**: 파티션 키 기준으로 셔플하여 같은 파티션의 데이터를 모은 후 작성하므로 파일 수가 줄어듭니다\n",
    "- **range**: hash에 더해 정렬까지 보장하지만 셔플 비용이 가장 높습니다\n",
    "\n",
    "### 선택 가이드\n",
    "\n",
    "| 상황 | 권장 모드 |\n",
    "|------|----------|\n",
    "| 스트리밍 수집 (SLA 빡빡) | `none` + 주기적 Compaction |\n",
    "| 배치 적재 (파일 수 관리) | `hash` |\n",
    "| 배치 적재 + 정렬 필요 | `range` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000016",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 3: Bloom Filters — Point Lookup 성능 개선\n",
    "\n",
    "### Bloom Filter 개념\n",
    "\n",
    "Bloom Filter는 **\"이 파일에 찾는 값이 있을 수 있는가?\"**를 빠르게 판단하는 확률적 자료구조입니다.\n",
    "\n",
    "```\n",
    "Bloom Filter 응답:\n",
    "  \"없다\" → 확실히 없음 (True Negative) → 파일 스킵!\n",
    "  \"있다\" → 있을 수도 있음 (True/False Positive) → 파일 읽기\n",
    "```\n",
    "\n",
    "### 동작 원리\n",
    "\n",
    "1. 비트 배열(예: 1024비트)을 0으로 초기화\n",
    "2. 각 값을 K개의 해시 함수로 해시하여 비트 위치를 계산\n",
    "3. 해당 위치의 비트를 1로 설정\n",
    "4. 검색 시: 값을 해시하여 모든 비트가 1인지 확인\n",
    "\n",
    "### 적합한 경우\n",
    "\n",
    "- `WHERE user_id = 'abc123'` 같은 **Point Lookup** 쿼리가 빈번할 때\n",
    "- **고카디널리티 컬럼** (값의 종류가 매우 많은 컬럼)에 효과적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {BLOOM_TABLE}\")\n",
    "\n",
    "# Bloom Filter 없는 기본 테이블\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {BLOOM_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "# 대량 데이터 삽입 (여러 배치로 파일 분산)\n",
    "for i in range(5):\n",
    "    orders = generate_orders(num_records=200, seed=i, id_offset=i*200+1)\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(BLOOM_TABLE).append()\n",
    "\n",
    "total = spark.sql(f\"SELECT COUNT(*) FROM {BLOOM_TABLE}\").collect()[0][0]\n",
    "print(f\"테이블 생성 + {total}건 삽입 완료\")\n",
    "print(f\"파일 수: {count_files(BLOOM_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloom Filter 없이 Point Lookup 성능 측정\n",
    "times_without_bloom = []\n",
    "for _ in range(3):\n",
    "    start = time.time()\n",
    "    spark.sql(f\"SELECT * FROM {BLOOM_TABLE} WHERE order_id = 42\").collect()\n",
    "    times_without_bloom.append(time.time() - start)\n",
    "\n",
    "avg_without = sum(times_without_bloom) / len(times_without_bloom)\n",
    "print(f\"Bloom Filter 없이 Point Lookup 평균: {avg_without:.3f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloom Filter 활성화\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {BLOOM_TABLE}\n",
    "SET TBLPROPERTIES (\n",
    "    'write.parquet.bloom-filter-enabled.column.order_id' = 'true',\n",
    "    'write.parquet.bloom-filter-max-bytes' = '1048576'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"order_id 컬럼에 Bloom Filter 활성화\")\n",
    "print(\"(기존 파일에는 적용되지 않음 — Compaction이나 새 쓰기 시 적용)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compaction으로 Bloom Filter가 포함된 새 파일 생성\n",
    "spark.sql(f\"\"\"\n",
    "CALL demo.system.rewrite_data_files(\n",
    "    table => '{BLOOM_TABLE}',\n",
    "    strategy => 'binpack'\n",
    ")\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(f\"Compaction 후 파일 수: {count_files(BLOOM_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloom Filter 적용 후 Point Lookup 성능 측정\n",
    "times_with_bloom = []\n",
    "for _ in range(3):\n",
    "    start = time.time()\n",
    "    spark.sql(f\"SELECT * FROM {BLOOM_TABLE} WHERE order_id = 42\").collect()\n",
    "    times_with_bloom.append(time.time() - start)\n",
    "\n",
    "avg_with = sum(times_with_bloom) / len(times_with_bloom)\n",
    "print(f\"Bloom Filter 없이: {avg_without:.3f}초\")\n",
    "print(f\"Bloom Filter 있음: {avg_with:.3f}초\")\n",
    "\n",
    "if avg_without > 0:\n",
    "    improvement = (avg_without - avg_with) / avg_without * 100\n",
    "    print(f\"개선율: {improvement:.1f}%\")\n",
    "    if improvement < 0:\n",
    "        print(\"(작은 데이터셋에서는 Bloom Filter 오버헤드로 오히려 느릴 수 있음)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000022",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Bloom Filters\n",
    "\n",
    "- Bloom Filter는 **Point Lookup** (`WHERE col = value`) 쿼리에서 불필요한 파일을 빠르게 건너뛸 수 있게 합니다\n",
    "- 소규모 데이터셋에서는 효과가 미미하거나 오히려 오버헤드가 될 수 있습니다\n",
    "- **대규모 데이터 + 고카디널리티 컬럼 + Point Lookup**이 빈번한 환경에서 가장 효과적입니다\n",
    "- Bloom Filter는 **Parquet 파일 레벨**에 저장되므로, 활성화 후 **Compaction이나 새 쓰기가 필요**합니다\n",
    "\n",
    "> 주의: Bloom Filter는 파일 크기를 약간 증가시킵니다. `bloom-filter-max-bytes`로 크기를 제한하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000023",
   "metadata": {},
   "source": [
    "---\n",
    "## Object Storage 최적화\n",
    "\n",
    "S3 같은 오브젝트 스토리지에서는 **prefix throttling** 문제가 발생할 수 있습니다.\n",
    "\n",
    "### 문제\n",
    "\n",
    "S3는 prefix(경로 접두사)별로 요청 속도를 제한합니다. 같은 파티션 디렉토리에 대량의 파일이 쓰이면 스로틀링이 발생할 수 있습니다.\n",
    "\n",
    "### 해결: Object Storage Path\n",
    "\n",
    "```sql\n",
    "ALTER TABLE my_table SET TBLPROPERTIES (\n",
    "    'write.object-storage.enabled' = 'true',\n",
    "    'write.data.path' = 's3://bucket/data'\n",
    ");\n",
    "```\n",
    "\n",
    "활성화하면 Iceberg가 **파일 경로에 해시를 추가**하여 다양한 prefix에 분산 저장합니다:\n",
    "\n",
    "```\n",
    "# Before (기본)\n",
    "s3://bucket/warehouse/my_table/data/order_date_month=2024-01/file1.parquet\n",
    "s3://bucket/warehouse/my_table/data/order_date_month=2024-01/file2.parquet\n",
    "\n",
    "# After (Object Storage 활성화)\n",
    "s3://bucket/data/a1b2c3d4/order_date_month=2024-01/file1.parquet\n",
    "s3://bucket/data/e5f6a7b8/order_date_month=2024-01/file2.parquet\n",
    "```\n",
    "\n",
    "파일 위치가 분산되지만, Iceberg 메타데이터가 파일 경로를 추적하므로 쿼리에는 영향이 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1000024",
   "metadata": {},
   "source": [
    "---\n",
    "## 튜닝 옵션 요약\n",
    "\n",
    "| 옵션 | 기본값 | 튜닝 포인트 | 효과 |\n",
    "|------|--------|------------|------|\n",
    "| **Metrics** | `truncate(16)` | 넓은 테이블 → 핵심만 `full`, 나머지 `none` | 메타데이터 크기 절약 |\n",
    "| **Distribution** | `none` | 배치 → `hash`, SLA 빡빡 → `none` | 파일 수 제어 |\n",
    "| **Bloom Filter** | 비활성 | 고카디널리티 + Point Lookup → 활성화 | 파일 스킵 |\n",
    "| **Object Storage** | 비활성 | S3 대량 쓰기 → 활성화 | prefix 스로틀링 방지 |\n",
    "\n",
    "### 언제 튜닝이 필요한가?\n",
    "\n",
    "1. **Compaction + Partitioning만으로 충분하지 않을 때**\n",
    "2. **넓은 테이블**(100+ 컬럼)에서 메타데이터 크기가 문제일 때 → Metrics 조정\n",
    "3. **스트리밍 수집 후 파일이 너무 많을 때** → Distribution Mode 조정\n",
    "4. **특정 값 검색이 느릴 때** → Bloom Filter 활성화\n",
    "5. **S3 쓰기 성능이 떨어질 때** → Object Storage 경로 활성화\n",
    "\n",
    "> 대부분의 경우 Compaction + Partitioning이면 충분합니다. 이 튜닝들은 **문제가 확인된 후** 적용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}