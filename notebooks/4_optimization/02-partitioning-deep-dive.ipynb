{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# Partitioning Deep Dive — Hidden Partitioning & Partition Evolution\n",
    "\n",
    "이 노트북에서는 Iceberg의 **파티셔닝** 전략을 실습합니다.\n",
    "\n",
    "전통적 파티셔닝의 한계를 확인하고, Iceberg의 Hidden Partitioning과 Partition Evolution이 이를 어떻게 해결하는지 관찰합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000002",
   "metadata": {},
   "source": [
    "## 파티셔닝이란?\n",
    "\n",
    "파티셔닝은 테이블의 데이터를 **논리적 그룹으로 나누어 저장**하는 것입니다.\n",
    "\n",
    "쿼리 시 WHERE 조건에 해당하는 파티션만 읽으면 되므로, 불필요한 데이터를 건너뛸 수 있습니다 (**Partition Pruning**).\n",
    "\n",
    "### Hive 스타일 파티셔닝의 문제점\n",
    "\n",
    "| 문제 | 설명 |\n",
    "|------|------|\n",
    "| **파티션 폭증** | `PARTITIONED BY (order_date)` → 날짜가 60일이면 파티션 60개 |\n",
    "| **파생 컬럼 필요** | 월별 파티션을 원하면 `order_month` 같은 컬럼을 직접 만들어야 함 |\n",
    "| **사용자가 변환을 알아야 함** | 쿼리 시 파티션 컬럼으로 필터링해야 프루닝 동작 |\n",
    "| **파티션 변경 불가** | 파티션 스키마를 바꾸면 전체 테이블 재작성 필요 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000003",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.spark_setup import create_spark_session\n",
    "from utils.data_generator import generate_orders, to_spark_df\n",
    "from utils.file_explorer import show_tree, snapshot_tree, diff_tree, count_files, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg 세션 준비 완료 (warehouse: file:///home/jovyan/data/warehouse)\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "# 테이블 설정\n",
    "IDENTITY_TABLE = \"demo.lab.partition_identity\"\n",
    "IDENTITY_PATH = \"/home/jovyan/data/warehouse/lab/partition_identity\"\n",
    "\n",
    "HIDDEN_TABLE = \"demo.lab.partition_hidden\"\n",
    "HIDDEN_PATH = \"/home/jovyan/data/warehouse/lab/partition_hidden\"\n",
    "\n",
    "EVOLUTION_TABLE = \"demo.lab.partition_evolution\"\n",
    "EVOLUTION_PATH = \"/home/jovyan/data/warehouse/lab/partition_evolution\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000006",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 1: Identity Partition — 전통적 파티셔닝의 한계\n",
    "\n",
    "`order_date` 컬럼으로 identity 파티셔닝하면, 날짜 값 하나마다 별도 파티션이 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity 파티션 테이블 생성 완료: demo.lab.partition_identity\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {IDENTITY_TABLE}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {IDENTITY_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (order_date)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Identity 파티션 테이블 생성 완료: {IDENTITY_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1000008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 레코드 수: 500\n"
     ]
    }
   ],
   "source": [
    "# 3개월(90일) 분량 데이터 삽입\n",
    "orders = generate_orders(num_records=500, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(IDENTITY_TABLE).append()\n",
    "\n",
    "print(f\"총 레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {IDENTITY_TABLE}').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1000009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity 파티션 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date=2024-01-01/\n",
      "│   ├── order_date=2024-01-02/\n",
      "│   ├── order_date=2024-01-03/\n",
      "│   ├── order_date=2024-01-04/\n",
      "│   ├── order_date=2024-01-05/\n",
      "│   ├── order_date=2024-01-06/\n",
      "│   ├── order_date=2024-01-07/\n",
      "│   ├── order_date=2024-01-08/\n",
      "│   ├── order_date=2024-01-09/\n",
      "│   ├── order_date=2024-01-10/\n",
      "│   ├── order_date=2024-01-11/\n",
      "│   ├── order_date=2024-01-12/\n",
      "│   ├── order_date=2024-01-13/\n",
      "│   ├── order_date=2024-01-14/\n",
      "│   ├── order_date=2024-01-15/\n",
      "│   ├── order_date=2024-01-16/\n",
      "│   ├── order_date=2024-01-17/\n",
      "│   ├── order_date=2024-01-18/\n",
      "│   ├── order_date=2024-01-19/\n",
      "│   ├── order_date=2024-01-20/\n",
      "│   ├── order_date=2024-01-21/\n",
      "│   ├── order_date=2024-01-22/\n",
      "│   ├── order_date=2024-01-23/\n",
      "│   ├── order_date=2024-01-24/\n",
      "│   ├── order_date=2024-01-25/\n",
      "│   ├── order_date=2024-01-27/\n",
      "│   ├── order_date=2024-01-28/\n",
      "│   ├── order_date=2024-01-29/\n",
      "│   ├── order_date=2024-01-30/\n",
      "│   ├── order_date=2024-01-31/\n",
      "│   ├── order_date=2024-02-01/\n",
      "│   ├── order_date=2024-02-02/\n",
      "│   ├── order_date=2024-02-03/\n",
      "│   ├── order_date=2024-02-04/\n",
      "│   ├── order_date=2024-02-05/\n",
      "│   ├── order_date=2024-02-06/\n",
      "│   ├── order_date=2024-02-07/\n",
      "│   ├── order_date=2024-02-08/\n",
      "│   ├── order_date=2024-02-09/\n",
      "│   ├── order_date=2024-02-10/\n",
      "│   ├── order_date=2024-02-11/\n",
      "│   ├── order_date=2024-02-12/\n",
      "│   ├── order_date=2024-02-13/\n",
      "│   ├── order_date=2024-02-14/\n",
      "│   ├── order_date=2024-02-15/\n",
      "│   ├── order_date=2024-02-16/\n",
      "│   ├── order_date=2024-02-17/\n",
      "│   ├── order_date=2024-02-18/\n",
      "│   ├── order_date=2024-02-19/\n",
      "│   ├── order_date=2024-02-20/\n",
      "│   ├── order_date=2024-02-21/\n",
      "│   ├── order_date=2024-02-22/\n",
      "│   ├── order_date=2024-02-23/\n",
      "│   ├── order_date=2024-02-24/\n",
      "│   ├── order_date=2024-02-25/\n",
      "│   ├── order_date=2024-02-26/\n",
      "│   ├── order_date=2024-02-27/\n",
      "│   ├── order_date=2024-02-28/\n",
      "│   ├── order_date=2024-02-29/\n",
      "│   ├── order_date=2024-03-01/\n",
      "│   ├── order_date=2024-03-02/\n",
      "│   ├── order_date=2024-03-03/\n",
      "│   ├── order_date=2024-03-04/\n",
      "│   ├── order_date=2024-03-05/\n",
      "│   ├── order_date=2024-03-06/\n",
      "│   ├── order_date=2024-03-07/\n",
      "│   ├── order_date=2024-03-08/\n",
      "│   ├── order_date=2024-03-09/\n",
      "│   ├── order_date=2024-03-10/\n",
      "│   ├── order_date=2024-03-11/\n",
      "│   ├── order_date=2024-03-12/\n",
      "│   ├── order_date=2024-03-13/\n",
      "│   ├── order_date=2024-03-14/\n",
      "│   ├── order_date=2024-03-15/\n",
      "│   ├── order_date=2024-03-16/\n",
      "│   ├── order_date=2024-03-17/\n",
      "│   ├── order_date=2024-03-18/\n",
      "│   ├── order_date=2024-03-19/\n",
      "│   ├── order_date=2024-03-20/\n",
      "│   ├── order_date=2024-03-21/\n",
      "│   ├── order_date=2024-03-22/\n",
      "│   ├── order_date=2024-03-23/\n",
      "│   ├── order_date=2024-03-24/\n",
      "│   ├── order_date=2024-03-25/\n",
      "│   ├── order_date=2024-03-26/\n",
      "│   ├── order_date=2024-03-27/\n",
      "│   ├── order_date=2024-03-28/\n",
      "│   ├── order_date=2024-03-29/\n",
      "│   ├── order_date=2024-03-30/\n",
      "│   └── order_date=2024-03-31/\n",
      "└── metadata/\n",
      "    ├── 37f5a4f1-4275-4337-be46-56fd660b3b42-m0.avro  (11.9 KB)\n",
      "    ├── snap-6631091067753903216-1-37f5a4f1-4275-4337-be46-56fd660b3b42.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v2.metadata.json  (2.6 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "파티션 수: 90\n",
      "Parquet 파일 수: 90\n"
     ]
    }
   ],
   "source": [
    "# Identity 파티션 구조 확인\n",
    "print(\"Identity 파티션 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(IDENTITY_PATH, max_depth=2)\n",
    "\n",
    "# 파티션 수 확인\n",
    "partitions = spark.sql(f\"SELECT * FROM {IDENTITY_TABLE}.partitions\").collect()\n",
    "print(f\"\\n파티션 수: {len(partitions)}\")\n",
    "print(f\"Parquet 파일 수: {count_files(IDENTITY_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000010",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Identity Partition\n",
    "\n",
    "- 90일 범위의 데이터로 **최대 90개의 파티션 디렉토리**가 생성되었습니다\n",
    "- 각 파티션에 소량의 데이터만 들어가 **Small File Problem** 발생\n",
    "- 이는 Hive 스타일 파티셔닝에서 흔히 겪는 문제입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000011",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 2: Hidden Partitioning — 변환 함수로 파티션 축소\n",
    "\n",
    "Iceberg는 **Hidden Partitioning**을 지원합니다. 파티션 컬럼을 별도로 만들지 않아도, `months()`, `days()`, `hours()`, `truncate()`, `bucket()` 등의 **변환 함수**를 사용하여 파티셔닝할 수 있습니다.\n",
    "\n",
    "### 사용 가능한 변환 함수\n",
    "\n",
    "| 변환 | 설명 | 예시 |\n",
    "|------|------|------|\n",
    "| `year(col)` | 연도 단위 | 2024 |\n",
    "| `month(col)` | 연-월 단위 | 2024-01 |\n",
    "| `day(col)` | 연-월-일 단위 | 2024-01-15 |\n",
    "| `hour(col)` | 시간 단위 | 2024-01-15-08 |\n",
    "| `truncate(col, N)` | 문자열/숫자를 N 단위로 자름 | truncate(id, 100) → 0, 100, 200... |\n",
    "| `bucket(col, N)` | N개 버킷으로 해시 분배 | bucket(id, 16) → 0~15 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1000012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Partition 테이블 생성 완료: demo.lab.partition_hidden\n",
      "파티션 변환: months(order_date) → 월 단위로 파티셔닝\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {HIDDEN_TABLE}\")\n",
    "\n",
    "# months() 변환을 사용한 Hidden Partitioning\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {HIDDEN_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (months(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Hidden Partition 테이블 생성 완료: {HIDDEN_TABLE}\")\n",
    "print(\"파티션 변환: months(order_date) → 월 단위로 파티셔닝\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1000013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 레코드 수: 500\n"
     ]
    }
   ],
   "source": [
    "# 동일한 데이터 삽입 (3개월 분량)\n",
    "orders = generate_orders(num_records=500, seed=42)\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(HIDDEN_TABLE).append()\n",
    "\n",
    "print(f\"총 레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {HIDDEN_TABLE}').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Partition 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-01/\n",
      "│   ├── order_date_month=2024-02/\n",
      "│   └── order_date_month=2024-03/\n",
      "└── metadata/\n",
      "    ├── 191ddb06-80e6-4e59-90a3-376d6eec267c-m0.avro  (7.3 KB)\n",
      "    ├── snap-827375320685005734-1-191ddb06-80e6-4e59-90a3-376d6eec267c.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v2.metadata.json  (2.6 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "Identity 파티션 수: 90\n",
      "Hidden 파티션 수:   3\n",
      "Parquet 파일 수:    3\n"
     ]
    }
   ],
   "source": [
    "# Hidden Partition 구조 확인\n",
    "print(\"Hidden Partition 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(HIDDEN_PATH, max_depth=2)\n",
    "\n",
    "# 파티션 수 비교\n",
    "hidden_partitions = spark.sql(f\"SELECT * FROM {HIDDEN_TABLE}.partitions\").collect()\n",
    "print(f\"\\nIdentity 파티션 수: {len(partitions)}\")\n",
    "print(f\"Hidden 파티션 수:   {len(hidden_partitions)}\")\n",
    "print(f\"Parquet 파일 수:    {count_files(HIDDEN_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPLAIN: order_date 필터 쿼리 (Hidden Partition 자동 프루닝)\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Parsed Logical Plan ==\\n'Project [*]\\n+- 'Filter (('order_date >= 2024-02-01) AND ('order_date < 2024-03-01))\\n   +- 'UnresolvedRelation [demo, lab, partition_hidden], [], false\\n\\n== Analyzed Logical Plan ==\\norder_id: bigint, customer_id: bigint, product_name: string, order_date: date, amount: decimal(10,2), status: string\\nProject [order_id#174L, customer_id#175L, product_name#176, order_date#177, amount#178, status#179]\\n+- Filter ((order_date#177 >= cast(2024-02-01 as date)) AND (order_date#177 < cast(2024-03-01 as date)))\\n   +- SubqueryAlias demo.lab.partition_hidden\\n      +- RelationV2[order_id#174L, customer_id#175L, product_name#176, order_date#177, amount#178, status#179] demo.lab.partition_hidden demo.lab.partition_hidden\\n\\n== Optimized Logical Plan ==\\nRelationV2[order_id#174L, customer_id#175L, product_name#176, order_date#177, amount#178, status#179] demo.lab.partition_hidden\\n\\n== Physical Plan ==\\n*(1) ColumnarToRow\\n+- BatchScan demo.lab.partition_hidden[order_id#174L, customer_id#175L, product_name#176, order_date#177, amount#178, status#179] demo.lab.partition_hidden (branch=null) [filters=order_date IS NOT NULL, order_date >= 19754, order_date < 19783, groupedBy=] RuntimeFilters: []\\n|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Partition Pruning 확인 — order_date로 필터링하면 자동으로 월 파티션 프루닝\n",
    "print(\"EXPLAIN: order_date 필터 쿼리 (Hidden Partition 자동 프루닝)\")\n",
    "spark.sql(f\"\"\"\n",
    "EXPLAIN EXTENDED\n",
    "SELECT * FROM {HIDDEN_TABLE}\n",
    "WHERE order_date >= '2024-02-01' AND order_date < '2024-03-01'\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000016",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Hidden Partitioning\n",
    "\n",
    "- `months(order_date)` 변환으로 **3개월 → 3개 파티션**만 생성되었습니다 (vs Identity의 ~90개)\n",
    "- 사용자는 `order_date`로 필터링하면 Iceberg가 **자동으로 해당 월 파티션만 스캔**합니다\n",
    "- 별도의 파티션 컬럼(`order_month`)을 만들 필요가 없습니다\n",
    "- 쿼리 작성자가 파티셔닝 방식을 몰라도 프루닝이 동작합니다 — 이것이 **\"Hidden\"**의 의미입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000017",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 3: 변환 함수 비교 — year vs months vs days\n",
    "\n",
    "같은 데이터에 서로 다른 변환을 적용했을 때 파티션 수가 어떻게 달라지는지 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1000018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year     (years(order_date)        ) → 파티션 1개, 파일 1개\n",
      "months   (months(order_date)       ) → 파티션 3개, 파일 3개\n",
      "days     (days(order_date)         ) → 파티션 90개, 파일 90개\n"
     ]
    }
   ],
   "source": [
    "# year, months, days 각각으로 파티셔닝한 테이블 생성\n",
    "transforms = {\n",
    "    \"year\":   \"years(order_date)\",\n",
    "    \"months\": \"months(order_date)\",\n",
    "    \"days\":   \"days(order_date)\",\n",
    "}\n",
    "\n",
    "for name, transform in transforms.items():\n",
    "    table = f\"demo.lab.partition_transform_{name}\"\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE {table} (\n",
    "        order_id BIGINT,\n",
    "        customer_id BIGINT,\n",
    "        product_name STRING,\n",
    "        order_date DATE,\n",
    "        amount DECIMAL(10,2),\n",
    "        status STRING\n",
    "    ) USING ICEBERG\n",
    "    PARTITIONED BY ({transform})\n",
    "    \"\"\")\n",
    "    orders = generate_orders(num_records=500, seed=42)\n",
    "    df = to_spark_df(spark, orders)\n",
    "    df.writeTo(table).append()\n",
    "    \n",
    "    partition_count = len(spark.sql(f\"SELECT * FROM {table}.partitions\").collect())\n",
    "    file_count = count_files(f\"/home/jovyan/data/warehouse/lab/partition_transform_{name}\")\n",
    "    print(f\"{name:8s} ({transform:25s}) → 파티션 {partition_count}개, 파일 {file_count}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000019",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 변환 함수 비교\n",
    "\n",
    "| 변환 | 예상 파티션 수 | 적합 사례 |\n",
    "|------|---------------|----------|\n",
    "| `years()` | ~1개 | 수년 단위 데이터, 매우 큰 테이블 |\n",
    "| `months()` | ~3개 | 월 단위 분석이 빈번한 경우 (가장 일반적) |\n",
    "| `days()` | ~90개 | 일 단위 정밀 프루닝이 필요한 경우 |\n",
    "\n",
    "> **파티션 수의 황금률**: 파티션당 최소 수십~수백 MB의 데이터가 있도록 설계하세요.\n",
    "> 파티션이 너무 많으면 Small File Problem, 너무 적으면 프루닝 효과 저하."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000020",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 4: Partition Evolution — 파티션 변경\n",
    "\n",
    "Iceberg의 **Partition Evolution**은 기존 데이터를 재작성하지 않고 파티션 방식을 변경할 수 있게 해줍니다.\n",
    "\n",
    "- 기존 데이터: 이전 파티션 스펙 유지\n",
    "- 새 데이터: 새 파티션 스펙 적용\n",
    "- 메타데이터에 여러 파티션 스펙이 공존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1000021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase 1] year 파티셔닝으로 200건 삽입\n",
      "파티션 수: 1\n",
      "├── data/\n",
      "│   └── order_date_year=2024/\n",
      "└── metadata/\n",
      "    ├── 648703bc-ed83-4c5a-b6d4-d12c478ac4c6-m0.avro  (7.1 KB)\n",
      "    ├── snap-6663851353061963763-1-648703bc-ed83-4c5a-b6d4-d12c478ac4c6.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v2.metadata.json  (2.6 KB)\n",
      "    └── version-hint.text  (1 B)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {EVOLUTION_TABLE}\")\n",
    "\n",
    "# 처음에는 year 파티셔닝으로 시작\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {EVOLUTION_TABLE} (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_name STRING,\n",
    "    order_date DATE,\n",
    "    amount DECIMAL(10,2),\n",
    "    status STRING\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (years(order_date))\n",
    "\"\"\")\n",
    "\n",
    "# 초기 데이터 삽입 (year 파티셔닝 적용)\n",
    "orders = generate_orders(num_records=200, seed=10, start_date=\"2024-01-01\", end_date=\"2024-03-31\")\n",
    "df = to_spark_df(spark, orders)\n",
    "df.writeTo(EVOLUTION_TABLE).append()\n",
    "\n",
    "print(\"[Phase 1] year 파티셔닝으로 200건 삽입\")\n",
    "print(f\"파티션 수: {len(spark.sql(f'SELECT * FROM {EVOLUTION_TABLE}.partitions').collect())}\")\n",
    "show_tree(EVOLUTION_PATH, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1000022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파티션 스펙 변경 완료: years(order_date) → months(order_date)\n",
      "(기존 데이터는 재작성되지 않음 — 새 데이터만 새 스펙 적용)\n"
     ]
    }
   ],
   "source": [
    "# Partition Evolution: year → months로 변경\n",
    "spark.sql(f\"ALTER TABLE {EVOLUTION_TABLE} REPLACE PARTITION FIELD years(order_date) WITH months(order_date)\")\n",
    "\n",
    "print(\"파티션 스펙 변경 완료: years(order_date) → months(order_date)\")\n",
    "print(\"(기존 데이터는 재작성되지 않음 — 새 데이터만 새 스펙 적용)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1000023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase 2] months 파티셔닝으로 200건 추가 삽입\n",
      "총 레코드 수: 400\n"
     ]
    }
   ],
   "source": [
    "# 새 데이터 삽입 (months 파티셔닝 적용)\n",
    "orders_new = generate_orders(num_records=200, seed=20, start_date=\"2024-04-01\", end_date=\"2024-06-30\", id_offset=201)\n",
    "df_new = to_spark_df(spark, orders_new)\n",
    "df_new.writeTo(EVOLUTION_TABLE).append()\n",
    "\n",
    "print(\"[Phase 2] months 파티셔닝으로 200건 추가 삽입\")\n",
    "print(f\"총 레코드 수: {spark.sql(f'SELECT COUNT(*) FROM {EVOLUTION_TABLE}').collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1000024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Evolution 후 파일 구조:\n",
      "============================================================\n",
      "├── data/\n",
      "│   ├── order_date_month=2024-04/\n",
      "│   ├── order_date_month=2024-05/\n",
      "│   ├── order_date_month=2024-06/\n",
      "│   └── order_date_year=2024/\n",
      "└── metadata/\n",
      "    ├── 3548eeae-b0b2-421a-a5d6-be146bbdb0ea-m0.avro  (7.3 KB)\n",
      "    ├── 648703bc-ed83-4c5a-b6d4-d12c478ac4c6-m0.avro  (7.1 KB)\n",
      "    ├── snap-6057118507659139080-1-3548eeae-b0b2-421a-a5d6-be146bbdb0ea.avro  (4.3 KB)\n",
      "    ├── snap-6663851353061963763-1-648703bc-ed83-4c5a-b6d4-d12c478ac4c6.avro  (4.2 KB)\n",
      "    ├── v1.metadata.json  (1.5 KB)\n",
      "    ├── v2.metadata.json  (2.6 KB)\n",
      "    ├── v3.metadata.json  (2.9 KB)\n",
      "    ├── v4.metadata.json  (3.9 KB)\n",
      "    └── version-hint.text  (1 B)\n",
      "\n",
      "파티션 정보:\n",
      ".partitions 조회 실패: An error occurred while calling o274.showString.\n",
      "대체 출력: .files 기반 파티션 통계\n",
      "+-----------+----------+-------+---------------------+\n",
      "|partition  |data_files|records|total_file_size_bytes|\n",
      "+-----------+----------+-------+---------------------+\n",
      "|{54, null} |1         |200    |4174                 |\n",
      "|{null, 651}|1         |80     |2894                 |\n",
      "|{null, 652}|1         |61     |2841                 |\n",
      "|{null, 653}|1         |59     |2830                 |\n",
      "+-----------+----------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evolution 후 파일 구조 확인 — 두 스펙이 공존\n",
    "print(\"Partition Evolution 후 파일 구조:\")\n",
    "print(\"=\" * 60)\n",
    "show_tree(EVOLUTION_PATH, max_depth=2)\n",
    "\n",
    "print(\"\\n파티션 정보:\")\n",
    "try:\n",
    "    spark.sql(f\"SELECT * FROM {EVOLUTION_TABLE}.partitions\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\".partitions 조회 실패: {str(e).splitlines()[0]}\")\n",
    "    print(\"대체 출력: .files 기반 파티션 통계\")\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        CAST(partition AS STRING) AS partition,\n",
    "        COUNT(*) AS data_files,\n",
    "        SUM(record_count) AS records,\n",
    "        SUM(file_size_in_bytes) AS total_file_size_bytes\n",
    "    FROM {EVOLUTION_TABLE}.files\n",
    "    WHERE content = 0\n",
    "    GROUP BY CAST(partition AS STRING)\n",
    "    ORDER BY partition\n",
    "    \"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1000025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타데이터 파일: v4.metadata.json\n",
      "\n",
      "파티션 스펙 수: 2\n",
      "  spec-id 0: [{'name': 'order_date_year', 'transform': 'year', 'source-id': 4, 'field-id': 1000}]\n",
      "  spec-id 1: [{'name': 'order_date_month', 'transform': 'month', 'source-id': 4, 'field-id': 1001}]\n",
      "\n",
      "현재 기본 스펙 ID: 1\n"
     ]
    }
   ],
   "source": [
    "# 파티션 스펙 히스토리 확인 — 메타데이터에 여러 스펙 공존\n",
    "import json\n",
    "import glob\n",
    "\n",
    "metadata_files = sorted(glob.glob(f\"{EVOLUTION_PATH}/metadata/*.metadata.json\"))\n",
    "if metadata_files:\n",
    "    latest = metadata_files[-1]\n",
    "    with open(latest) as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    print(f\"메타데이터 파일: {latest.split('/')[-1]}\")\n",
    "    print(f\"\\n파티션 스펙 수: {len(meta.get('partition-specs', []))}\")\n",
    "    for spec in meta.get('partition-specs', []):\n",
    "        print(f\"  spec-id {spec['spec-id']}: {spec['fields']}\")\n",
    "    print(f\"\\n현재 기본 스펙 ID: {meta.get('default-spec-id')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1000025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec-id별 파티션 매핑 (.files 메타테이블):\n",
      "+-------+---------------+----------+-------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|spec_id|partition_value|data_files|records|sample_file                                                                                                                     |\n",
      "+-------+---------------+----------+-------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0      |{54, null}     |1         |200    |data/warehouse/lab/partition_evolution/data/order_date_year=2024/00000-44-c04b5dfb-2703-408b-856d-da1e77f00ff7-00001.parquet    |\n",
      "|1      |{null, 651}    |1         |80     |data/warehouse/lab/partition_evolution/data/order_date_month=2024-04/00000-51-f63df151-298e-4fd9-b062-256201552390-00001.parquet|\n",
      "|1      |{null, 652}    |1         |61     |data/warehouse/lab/partition_evolution/data/order_date_month=2024-05/00000-51-f63df151-298e-4fd9-b062-256201552390-00002.parquet|\n",
      "|1      |{null, 653}    |1         |59     |data/warehouse/lab/partition_evolution/data/order_date_month=2024-06/00000-51-f63df151-298e-4fd9-b062-256201552390-00003.parquet|\n",
      "+-------+---------------+----------+-------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "해석:\n",
      "- spec_id = 0: year(order_date) 스펙으로 작성된 기존 파일\n",
      "- spec_id = 1: month(order_date) 스펙으로 작성된 신규 파일\n",
      "- default-spec-id는 앞으로 쓰일 기본 스펙이며, 기존 파일의 spec_id는 바뀌지 않음\n"
     ]
    }
   ],
   "source": [
    "# spec-id별 파티션/파일 매핑 확인 — 어떤 파일이 어떤 파티션 스펙으로 쓰였는지\n",
    "print(\"spec-id별 파티션 매핑 (.files 메타테이블):\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    spec_id,\n",
    "    CAST(partition AS STRING) AS partition_value,\n",
    "    COUNT(*) AS data_files,\n",
    "    SUM(record_count) AS records,\n",
    "    MIN(regexp_replace(file_path, '^file:.*?/(data/)', '$1')) AS sample_file\n",
    "FROM {EVOLUTION_TABLE}.files\n",
    "WHERE content = 0\n",
    "GROUP BY spec_id, CAST(partition AS STRING)\n",
    "ORDER BY spec_id, partition_value\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n해석:\")\n",
    "print(\"- spec_id = 0: year(order_date) 스펙으로 작성된 기존 파일\")\n",
    "print(\"- spec_id = 1: month(order_date) 스펙으로 작성된 신규 파일\")\n",
    "print(\"- default-spec-id는 앞으로 쓰일 기본 스펙이며, 기존 파일의 spec_id는 바뀌지 않음\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000026",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — Partition Evolution\n",
    "\n",
    "- 파티션 스펙을 `years` → `months`로 변경했지만, **기존 데이터는 재작성되지 않았습니다**\n",
    "- 메타데이터에 **두 개의 파티션 스펙이 공존**합니다 (spec-id 0, 1)\n",
    "- 어떤 파일/파티션이 어떤 스펙을 썼는지는 `table.files`의 `spec_id`로 확인합니다 (`GROUP BY spec_id, partition`)\n",
    "- 기존 파일은 year 스펙, 새 파일은 months 스펙으로 기록됩니다\n",
    "- `default-spec-id`는 **신규 write 기본값**이며, 기존 파일의 `spec_id`는 유지됩니다\n",
    "- 일부 런타임 조합에서는 `table.partitions` 조회가 실패할 수 있어, 이 경우 `table.files` 집계로 파티션 상태를 확인합니다\n",
    "- Iceberg 쿼리 엔진은 두 스펙을 모두 이해하고 올바르게 프루닝합니다\n",
    "- Hive에서는 파티션 변경 시 전체 테이블을 재작성해야 했지만, Iceberg에서는 **메타데이터만 업데이트**하면 됩니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000027",
   "metadata": {},
   "source": [
    "---\n",
    "## 실험 5: 메타데이터 테이블로 파티션 모니터링\n",
    "\n",
    "Iceberg는 파티션 상태를 모니터링할 수 있는 **메타데이터 테이블**을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1000028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== .partitions — 파티션별 통계 ===\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at        |last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|{648}    |0      |159         |1         |3576                         |0                           |0                         |0                           |0                         |2026-02-16 00:11:33.251|827375320685005734      |\n",
      "|{649}    |0      |161         |1         |3605                         |0                           |0                         |0                           |0                         |2026-02-16 00:11:33.251|827375320685005734      |\n",
      "|{650}    |0      |180         |1         |3756                         |0                           |0                         |0                           |0                         |2026-02-16 00:11:33.251|827375320685005734      |\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .partitions 메타데이터 테이블\n",
    "print(\"=== .partitions — 파티션별 통계 ===\")\n",
    "spark.sql(f\"SELECT * FROM {HIDDEN_TABLE}.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1000029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== .files — 파일별 상세 정보 ===\n",
      "+---------+-----------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+\n",
      "|partition|file_path                                                                                                                    |file_format|record_count|file_size_in_bytes|\n",
      "+---------+-----------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+\n",
      "|{648}    |data/warehouse/lab/partition_hidden/data/order_date_month=2024-01/00000-14-480a11db-2e22-40fa-87a9-c326bb4d352b-00001.parquet|PARQUET    |159         |3576              |\n",
      "|{649}    |data/warehouse/lab/partition_hidden/data/order_date_month=2024-02/00000-14-480a11db-2e22-40fa-87a9-c326bb4d352b-00002.parquet|PARQUET    |161         |3605              |\n",
      "|{650}    |data/warehouse/lab/partition_hidden/data/order_date_month=2024-03/00000-14-480a11db-2e22-40fa-87a9-c326bb4d352b-00003.parquet|PARQUET    |180         |3756              |\n",
      "+---------+-----------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .files 메타데이터 테이블 — 파일별 상세 정보\n",
    "print(\"=== .files — 파일별 상세 정보 ===\")\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    partition,\n",
    "    regexp_replace(file_path, '^file:.*?/(data/)', '$1') as file_path,\n",
    "    file_format,\n",
    "    record_count,\n",
    "    file_size_in_bytes\n",
    "FROM {HIDDEN_TABLE}.files\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000030",
   "metadata": {},
   "source": [
    "### 관찰 포인트 — 메타데이터 테이블\n",
    "\n",
    "- `.partitions` 테이블로 **파티션별 레코드 수, 파일 수**를 확인할 수 있습니다\n",
    "- `.files` 테이블로 **개별 파일의 크기, 레코드 수, 파티션 소속**을 확인할 수 있습니다\n",
    "- 이를 통해 **불균형 파티션**(한 파티션에 데이터 편중)이나 **Small File Problem**을 사전에 감지할 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000031",
   "metadata": {},
   "source": [
    "---\n",
    "## Hive vs Iceberg 파티셔닝 비교\n",
    "\n",
    "| 항목 | Hive 스타일 | Iceberg |\n",
    "|------|-----------|--------|\n",
    "| **파티션 컬럼** | 별도 컬럼 필요 (`order_month`) | 원본 컬럼 그대로 사용 |\n",
    "| **변환** | 사용자가 직접 계산 | 변환 함수 자동 적용 (`months()`) |\n",
    "| **쿼리** | 파티션 컬럼으로 필터링해야 프루닝 | 원본 컬럼 필터링 시 자동 프루닝 |\n",
    "| **파티션 변경** | 전체 테이블 재작성 | 메타데이터만 업데이트 (Partition Evolution) |\n",
    "| **파티션 스펙 공존** | 불가 | 가능 (여러 스펙 동시 존재) |\n",
    "| **파티션 디스커버리** | 별도 `MSCK REPAIR TABLE` 필요 | 불필요 (메타데이터에 파일 목록 포함) |\n",
    "\n",
    "### 핵심 요약\n",
    "\n",
    "1. **Hidden Partitioning**: 파티션 컬럼 없이 변환 함수로 자동 파티셔닝 → 사용자 투명\n",
    "2. **Partition Evolution**: 기존 데이터 재작성 없이 파티션 방식 변경 → 운영 유연성\n",
    "3. **메타데이터 테이블**: `.partitions`, `.files`로 파티션 상태 모니터링 → 사전 진단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1000032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 세션 종료\n"
     ]
    }
   ],
   "source": [
    "# 정리\n",
    "for name in transforms:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS demo.lab.partition_transform_{name}\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"Spark 세션 종료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
